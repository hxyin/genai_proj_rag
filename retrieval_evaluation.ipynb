{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础库\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Google Cloud 认证\n",
    "from google.auth import load_credentials_from_file\n",
    "\n",
    "# Langchain 相关\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Ragas 相关\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas import EvaluationDataset # type: ignore\n",
    "\n",
    "# 类型提示\n",
    "from typing import cast as t\n",
    "from langchain_core.outputs import LLMResult, ChatGeneration\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# PDF处理\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载认证信息（换成自己的）\n",
    "credentials, project_id = load_credentials_from_file(\n",
    "    \"./unified-sensor-437622-t3-1c0bfcf1fd30.json\"\n",
    ")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./unified-sensor-437622-t3-1c0bfcf1fd30.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # 换成自己的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHybridRetriever:\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "    \n",
    "    def invoke(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using both retrievers and combine results\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            \n",
    "        Returns:\n",
    "            List of unique documents, limited to top 4 results\n",
    "        \"\"\"\n",
    "        if not query or pd.isna(query):\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            # 使用新的 invoke 方法\n",
    "            vector_docs = self.vector_retriever.invoke(query)\n",
    "            bm25_docs = self.bm25_retriever.invoke(query)\n",
    "            \n",
    "            # 合并结果并去重\n",
    "            seen = set()\n",
    "            unique_docs = []\n",
    "            for doc in vector_docs + bm25_docs:\n",
    "                if doc.page_content not in seen:\n",
    "                    seen.add(doc.page_content)\n",
    "                    unique_docs.append(doc)\n",
    "            \n",
    "            return unique_docs[:4]  # 返回前4个文档\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid retrieval: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"从PDF文件中提取文本\"\"\"\n",
    "    \"\"\"\n",
    "    Extract text content from PDF files\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects containing page content and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text = page.get_text(\"text\")\n",
    "            if text.strip():\n",
    "                metadata = {\n",
    "                    \"source\": pdf_path,\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"total_pages\": len(pdf_document)\n",
    "                }\n",
    "                documents.append(Document(page_content=text, metadata=metadata))\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_directory(directory_path: str) -> List[Document]:\n",
    "    \"\"\"从目录加载PDF文档\"\"\"\n",
    "    \"\"\"\n",
    "    Load all PDF documents from a specified directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing PDF files\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects from all PDFs in directory\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with tqdm(os.listdir(directory_path), desc=\"Loading PDFs\") as pbar:\n",
    "        for filename in pbar:\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                pbar.set_postfix(file=filename)\n",
    "                documents.extend(extract_text_from_pdf(file_path))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_is_finished_parser(response: LLMResult):\n",
    "    \"\"\"自定义完成状态解析器\"\"\"\n",
    "    is_finished_list = []\n",
    "    for g in response.flatten():\n",
    "        resp = g.generations[0][0]\n",
    "        if resp.generation_info is not None:\n",
    "            if resp.generation_info.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp.generation_info.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "        elif (\n",
    "            isinstance(resp, ChatGeneration)\n",
    "            and t.cast(ChatGeneration, resp).message is not None\n",
    "        ):\n",
    "            resp_message: BaseMessage = t.cast(ChatGeneration, resp).message\n",
    "            if resp_message.response_metadata.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp_message.response_metadata.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "        else:\n",
    "            is_finished_list.append(True)\n",
    "    return all(is_finished_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    def __init__(self, data_dir: str, persist_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.persist_dir = persist_dir\n",
    "        \n",
    "        # 初始化 embeddings\n",
    "        self.embeddings = VertexAIEmbeddings(\n",
    "            model_name=\"textembedding-gecko\",\n",
    "            project=project_id,\n",
    "            credentials=credentials,\n",
    "            location=\"us-central1\"\n",
    "        )\n",
    "        \n",
    "        # 初始化LLMs\n",
    "        self.eval_llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        self.exp_llm = VertexAI(\n",
    "            model_name=\"gemini-1.0-pro\",\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=8192,\n",
    "            project=project_id,\n",
    "            credentials=credentials\n",
    "        )\n",
    "        \n",
    "        print(\"Loading documents...\")\n",
    "        self.documents = load_pdfs_from_directory(data_dir)\n",
    "        print(f\"Loaded {len(self.documents)} documents\")\n",
    "        \n",
    "        print(\"Initializing retrievers...\")\n",
    "        self.retrievers = {\n",
    "            \"vector\": self._init_vector_retriever(),\n",
    "            \"bm25\": self._init_bm25_retriever(),\n",
    "            \"hybrid\": self._init_hybrid_retriever(),\n",
    "            \"parent_doc\": self._init_parent_doc_retriever()\n",
    "        }\n",
    "        print(\"Initialization complete\")\n",
    "\n",
    "    def _init_vector_retriever(self):\n",
    "        \"\"\"初始化向量检索器\"\"\"\n",
    "        print(\"Initializing vector retriever...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=f\"{self.persist_dir}/vector\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "\n",
    "    def _init_bm25_retriever(self):\n",
    "        \"\"\"初始化BM25检索器\"\"\"\n",
    "        print(\"Initializing BM25 retriever...\")\n",
    "        return BM25Retriever.from_documents(\n",
    "            self.documents,\n",
    "            k=4\n",
    "        )\n",
    "\n",
    "    def _init_hybrid_retriever(self):\n",
    "        \"\"\"初始化混合检索器\"\"\"\n",
    "        print(\"Initializing hybrid retriever...\")\n",
    "        vector_retriever = self._init_vector_retriever()\n",
    "        bm25_retriever = self._init_bm25_retriever()\n",
    "        return CustomHybridRetriever(vector_retriever, bm25_retriever)\n",
    "\n",
    "    def _init_parent_doc_retriever(self):\n",
    "        \"\"\"初始化父文档检索器\"\"\"\n",
    "        print(\"Initializing parent document retriever...\")\n",
    "        \n",
    "        child_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        parent_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=400\n",
    "        )\n",
    "        \n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=f\"{self.persist_dir}/parent_doc\"\n",
    "        )\n",
    "        \n",
    "        store = InMemoryStore()\n",
    "        \n",
    "        retriever = ParentDocumentRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            docstore=store,\n",
    "            child_splitter=child_splitter,\n",
    "            parent_splitter=parent_splitter,\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        retriever.add_documents(self.documents)\n",
    "        return retriever\n",
    "\n",
    "    def _generate_response(self, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"生成回答\"\"\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        prompt = f\"Based on the following context, please answer the question:\\n\\nContext: {context}\\n\\nQuestion: {query}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.exp_llm.invoke(prompt)\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _single_evaluation_run(self, retriever, test_dataset: pd.DataFrame):\n",
    "        \"\"\"单次评估运行\"\"\"\n",
    "        try:\n",
    "            # 创建评估数据集\n",
    "            valid_results = pd.DataFrame()\n",
    "            valid_results['user_input'] = test_dataset['question']\n",
    "            valid_results['reference'] = test_dataset['ground_truth']\n",
    "            valid_results['response'] = ''\n",
    "            valid_results['retrieved_contexts'] = [[]] * len(test_dataset)\n",
    "\n",
    "            print(f\"Processing {len(valid_results)} valid queries\")\n",
    "            \n",
    "            # 处理每个查询\n",
    "            for idx, row in valid_results.iterrows():\n",
    "                try:\n",
    "                    query = row['user_input']\n",
    "                    if pd.isna(query):\n",
    "                        continue\n",
    "                        \n",
    "                    # 检索文档\n",
    "                    retrieved_docs = retriever.invoke(query)\n",
    "                    if not retrieved_docs:\n",
    "                        print(f\"Warning: No documents retrieved for query: {query[:50]}...\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 生成回答\n",
    "                    response = self._generate_response(query, retrieved_docs)\n",
    "                    \n",
    "                    # 更新结果\n",
    "                    valid_results.at[idx, 'response'] = response\n",
    "                    valid_results.at[idx, 'retrieved_contexts'] = [doc.page_content for doc in retrieved_docs]\n",
    "                    \n",
    "                    time.sleep(1)  # 避免速率限制\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing query: {query[:50]}...\")\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Successfully processed {len(valid_results)} queries\")\n",
    "\n",
    "            # 创建评估数据集\n",
    "            eval_dataset = EvaluationDataset.from_pandas(valid_results)\n",
    "\n",
    "            # 设置评估器\n",
    "            evaluator_llm = LangchainLLMWrapper(self.eval_llm)\n",
    "            evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "            # 定义评估指标\n",
    "            metrics = [\n",
    "                LLMContextRecall(llm=evaluator_llm),\n",
    "                FactualCorrectness(llm=evaluator_llm),\n",
    "                Faithfulness(llm=evaluator_llm),\n",
    "                SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "            ]\n",
    "\n",
    "            # 运行评估\n",
    "            results = evaluate(\n",
    "                dataset=eval_dataset,\n",
    "                metrics=metrics\n",
    "            )\n",
    "\n",
    "            # 直接返回评估结果\n",
    "            return {\n",
    "                \"scores\": results.scores,\n",
    "                \"results\": results,\n",
    "                \"metadata\": results.metadata if hasattr(results, 'metadata') else {}\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def evaluate_retriever(self, retriever_name: str, test_dataset: pd.DataFrame, n_runs: int = 1):\n",
    "        \"\"\"评估特定检索器的性能\"\"\"\n",
    "        results = []\n",
    "        retriever = self.retrievers[retriever_name]\n",
    "        \n",
    "        try:\n",
    "            for run_idx in tqdm(range(n_runs), desc=f\"Evaluating {retriever_name}\", leave=True):\n",
    "                run_result = self._single_evaluation_run(retriever, test_dataset.copy())\n",
    "                if run_result:\n",
    "                    results.append(run_result)\n",
    "            \n",
    "            if not results:\n",
    "                return {\n",
    "                    \"mean_scores\": None,\n",
    "                    \"confidence_intervals\": None,\n",
    "                    \"raw_results\": []\n",
    "                }\n",
    "                \n",
    "            # 直接返回第一次运行的结果\n",
    "            return results[0]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {retriever_name}: {str(e)}\")\n",
    "            return {\n",
    "                \"mean_scores\": None,\n",
    "                \"confidence_intervals\": None,\n",
    "                \"raw_results\": [],\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(data_dir: str = \"./data\", persist_dir: str = \"./chroma_db\", n_samples: int = 4):\n",
    "    \"\"\"运行评估流程\"\"\"\n",
    "    print(\"Starting evaluation process...\")\n",
    "    \n",
    "    # 初始化评估器\n",
    "    evaluator = RetrievalEvaluator(data_dir, persist_dir)\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    print(\"Loading test dataset...\")\n",
    "    test_dataset = pd.read_json('./data/eval_dataset_1.json')\n",
    "    if n_samples:\n",
    "        test_dataset = test_dataset.head(n_samples)\n",
    "    \n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(test_dataset.info())\n",
    "    print(test_dataset)\n",
    "    print(f\"\\nLoaded {len(test_dataset)} valid test cases\")\n",
    "    \n",
    "    # 评估所有检索器\n",
    "    results = {}\n",
    "    retrievers = [\"vector\", \"bm25\", \"hybrid\", \"parent_doc\"]\n",
    "    \n",
    "    for retriever in tqdm(retrievers, desc=\"Evaluating retrievers\"):\n",
    "        print(f\"\\nEvaluating {retriever}...\")\n",
    "        result = evaluator.evaluate_retriever(retriever, test_dataset)\n",
    "        results[retriever] = result\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for retriever in retrievers:\n",
    "        print(f\"\\nResults for {retriever}:\")\n",
    "        if results[retriever] and results[retriever].get(\"scores\"):\n",
    "            print(f\"Mean scores: {results[retriever]['scores']}\")\n",
    "        else:\n",
    "            print(\"Mean scores: None\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation process...\n",
      "Loading documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b44c14dd8e4e9b8266eaa931512978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading PDFs:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 290 documents\n",
      "Initializing retrievers...\n",
      "Initializing vector retriever...\n",
      "Initializing BM25 retriever...\n",
      "Initializing hybrid retriever...\n",
      "Initializing vector retriever...\n",
      "Initializing BM25 retriever...\n",
      "Initializing parent document retriever...\n",
      "Initialization complete\n",
      "Loading test dataset...\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, 0 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      1 non-null      object\n",
      " 1   ground_truth  1 non-null      object\n",
      " 2   answer        1 non-null      object\n",
      " 3   contexts      1 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 40.0+ bytes\n",
      "None\n",
      "                                            question  \\\n",
      "0  How does instruction tuning affect the zero-sh...   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  For larger models on the order of 100B paramet...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  For larger models with around 100B parameters,...   \n",
      "\n",
      "                                            contexts  \n",
      "0  [Published as a conference paper at ICLR 2022\\...  \n",
      "\n",
      "Loaded 1 valid test cases\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa6edf4c7f4a34a77b84ea70cc71de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating retrievers:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vector...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f0513567ec4ac292543590f7332365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating vector:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 valid queries\n",
      "Successfully processed 1 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaef59b4411d4cc9834281fb94a60e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating bm25...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aab5b1de8974f47a0a489954ccce2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating bm25:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 valid queries\n",
      "Successfully processed 1 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b0924d490d44168b0436fa5b9079f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating hybrid...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a63302a3e2b4501afa29aad2cac2b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating hybrid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 valid queries\n",
      "Successfully processed 1 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59937242b9047e2b847e0f075147e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating parent_doc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94744a6752d94b899328a51bcdc36263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating parent_doc:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 valid queries\n",
      "Successfully processed 1 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c76f2c1def8421485be98cb867c38cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "==================================================\n",
      "\n",
      "Results for vector:\n",
      "Mean scores: [{'context_recall': 0.0, 'factual_correctness': 0.0, 'faithfulness': 0.5384615384615384, 'semantic_similarity': 0.8600790061438462}]\n",
      "------------------------------\n",
      "\n",
      "Results for bm25:\n",
      "Mean scores: [{'context_recall': 1.0, 'factual_correctness': 0.22, 'faithfulness': 0.8947368421052632, 'semantic_similarity': 0.8423422425167596}]\n",
      "------------------------------\n",
      "\n",
      "Results for hybrid:\n",
      "Mean scores: [{'context_recall': 1.0, 'factual_correctness': 0.0, 'faithfulness': 0.8666666666666667, 'semantic_similarity': 0.8634133691045071}]\n",
      "------------------------------\n",
      "\n",
      "Results for parent_doc:\n",
      "Mean scores: [{'context_recall': 0.5, 'factual_correctness': 0.17, 'faithfulness': 0.5, 'semantic_similarity': 0.8379020210931905}]\n",
      "------------------------------\n",
      "\n",
      "Results for vector:\n",
      "                                          user_input  \\\n",
      "0  How does instruction tuning affect the zero-sh...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [Published as a conference paper at ICLR 2022\\...   \n",
      "\n",
      "                                            response  \\\n",
      "0  ## How Instruction Tuning Affects Zero-Shot Pe...   \n",
      "\n",
      "                                           reference  context_recall  \\\n",
      "0  For larger models on the order of 100B paramet...             0.0   \n",
      "\n",
      "   factual_correctness  faithfulness  semantic_similarity  \n",
      "0                  0.0      0.538462             0.860079  \n",
      "\n",
      "Results for bm25:\n",
      "                                          user_input  \\\n",
      "0  How does instruction tuning affect the zero-sh...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [Published as a conference paper at ICLR 2022\\...   \n",
      "\n",
      "                                            response  \\\n",
      "0  ## How Instruction Tuning Affects Zero-Shot Pe...   \n",
      "\n",
      "                                           reference  context_recall  \\\n",
      "0  For larger models on the order of 100B paramet...             1.0   \n",
      "\n",
      "   factual_correctness  faithfulness  semantic_similarity  \n",
      "0                 0.22      0.894737             0.842342  \n",
      "\n",
      "Results for hybrid:\n",
      "                                          user_input  \\\n",
      "0  How does instruction tuning affect the zero-sh...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [Published as a conference paper at ICLR 2022\\...   \n",
      "\n",
      "                                            response  \\\n",
      "0  ## Instruction Tuning and Zero-Shot Performanc...   \n",
      "\n",
      "                                           reference  context_recall  \\\n",
      "0  For larger models on the order of 100B paramet...             1.0   \n",
      "\n",
      "   factual_correctness  faithfulness  semantic_similarity  \n",
      "0                  0.0      0.866667             0.863413  \n",
      "\n",
      "Results for parent_doc:\n",
      "                                          user_input  \\\n",
      "0  How does instruction tuning affect the zero-sh...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [Published as a conference paper at ICLR 2022\\...   \n",
      "\n",
      "                                            response  \\\n",
      "0  ## How does instruction tuning affect the zero...   \n",
      "\n",
      "                                           reference  context_recall  \\\n",
      "0  For larger models on the order of 100B paramet...             0.5   \n",
      "\n",
      "   factual_correctness  faithfulness  semantic_similarity  \n",
      "0                 0.17           0.5             0.837902  \n"
     ]
    }
   ],
   "source": [
    "# 运行评估\n",
    "results = run_evaluation(n_samples=1)\n",
    "\n",
    "# 查看结果\n",
    "for retriever_name, result in results.items():\n",
    "    if result and result.get(\"results\"):\n",
    "        print(f\"\\nResults for {retriever_name}:\")\n",
    "        df = result[\"results\"].to_pandas()\n",
    "        print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag6998",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
