{"question":"How does instruction tuning affect the zero-shot performance on unseen tasks?","answer":"For larger models with around 100B parameters, instruction tuning substantially improves performance on unseen tasks. However, for smaller models such as 8B and below, instruction tuning actually hurts performance on unseen tasks. This could be due to the smaller models' capacity being filled by learning the tasks during instruction tuning, leading to worse performance on new tasks. In contrast, larger models benefit from instruction tuning as it helps them learn how to follow instructions while still having capacity to generalize to new tasks.","contexts":["Published as a conference paper at ICLR 2022\n4.2 S CALING LAWS\n0.4B2B8B68B137BModel Size (# parameters)3040506070Instruction tuningUntuned modelAverage zero-shot accuracy  on 13 held-out tasks (%) Performance on held-out tasks\nFigure 7: Whereas instruction tuning helps large\nmodels generalize to new tasks, for small models it\nactually hurts generalization to unseen tasks, poten-\ntially because all model capacity is used to learn the\nmixture of instruction tuning tasks.As Brown et al. (2020) shows that zero and\nfew-shot capabilities of language models sub-\nstantially improve for larger models, we next\nexplore how the bene\ufb01ts of instruction tuning\nare affected by model scale. Using the same\ncluster split as in the previous ablation study,\nwe evaluate the effect of instruction tuning\non models of size 422M, 2B, 8B, 68B, and\n137B parameters.\nFigure 7 shows these results. We see that\nfor the two models on the order of 100B pa-\nrameters, instruction tuning substantially im-\nproves performance on held-out tasks, as is\nexpected given the prior results in our pa-\nper. The behavior on held-out tasks for the\n8B and smaller models, however, is thought-\nprovoking\u2014instruction tuning actually hurts performance on held-out tasks. One potential explana-\ntion for this result could be that for small-scale models, learning the \u223c40 tasks used during instruction\ntuning \ufb01lls the entire model capacity, causing these models to perform worse on new tasks. Under\nthis potential explanation, for the larger scale models, instruction tuning \ufb01lls up some model capacity\nbut also teaches these models how to follow instructions, allowing them to generalize to new tasks\nwith the remaining capacity.\n4.3 R OLE OF INSTRUCTIONS\n2030405060FT: no instruction\nEval: instructionFT: dataset name\nEval: instructionFT: dataset name\nEval: dataset nameFT: instruction\nEval: instruction\n(FLAN)37.346.647.055.2Zero-shot performance \n(4 task cluster avg.)","Sanh et al.\n(2021) \ufb01netune T5 in a setup similar to ours, \ufb01nding that zero-shot learning can be improved in a\nmodel of 11B parameters. At a model scale similar to ours, OpenAI\u2019s InstructGPT models are trained\nvia both \ufb01netuning and reinforcement learning to produce outputs that are more preferred by human\nraters (Ouyang et al., 2022).\n6 D ISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does \ufb01netuning a model on a\ncollection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\nthis question via instruction tuning, a simple method that combines appealing aspects of both\nthe pretrain\u2013\ufb01netune and prompting paradigms. Our instruction-tuned model, FLAN, improves\nperformance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\nwe evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\nof instruction tuning task clusters, and, interestingly, that performance improvements from instruction\ntuning emerge only with suf\ufb01cient model scale. Moreover, instruction tuning can be combined with\nother prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between\nspecialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan\net al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might\nexpect labeled data to have the most natural role in improving specialist models, instruction tuning\ndemonstrates how labeled data can be used to help large language models perform many, unseen\ntasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\ntask-speci\ufb01c training is complementary to general language modeling and motivates further research\non generalist models.\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\nwe try to use accepted categorizations in the literature), and we only explore the use of relatively\nshort instructions of typically a single sentence (c.f.","Published as a conference paper at ICLR 2022\net al., 2021). Compared with this prior work that \ufb01netunes and evaluates on the same downstream\ntask, our setup studies the effect of instruction tuning on ability to perform unseen tasks.\nD.5 M ULTI -TASK QUESTION ANSWERING\nThe instructions we use for instruction tuning are similar to QA-based task formulation research,\nwhich aims to unify NLP tasks by casting them as question-answering over a context. For instance,\nMcCann et al. (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated\nwith natural language prompts; they report transfer learning gains on \ufb01netuning tasks as well as\nzero-shot domain adaptation results on SNLI (Bowman et al., 2015) and Amazon\/Yelp Reviews\n(Kotzias et al., 2015). While McCann et al. (2018) does not leverage unsupervised pre-training and\nonly reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on\nzero-shot performance on unseen task clusters. Uni\ufb01edQA (Khashabi et al., 2020) shows similar\ntransfer learning gains as McCann et al. (2018) across 20 datasets and reports good generalization\nto unseen tasks across four types of QA. Focusing on binary text classi\ufb01cation, Zhong et al. (2021)\n\ufb01netune T5-770M on 43 tasks phrased as yes\/no questions and study the zero-shot performance on\nunseen tasks. In comparison, our paper is much larger in scope, empirically demonstrating the idea\non a wide range of tasks with a much larger model. Other work has used QA-based task formulation\nfor more-targeted applications including semantic role labeling (He et al., 2015), relation extraction\n(Levy et al., 2017), coreference resolution (Wu et al., 2020) and named entity recognition (Li et al.,\n2020) as question answering."],"ground_truth":"For larger models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks. However, for smaller models (8B and smaller), instruction tuning actually hurts performance on held-out tasks."}
{"question":"What is the Zero-shot-CoT method and how does it elicit chain of thought from large language models?","answer":"The Zero-shot-CoT method is a zero-shot template-based prompting technique for chain of thought reasoning. It differs from the original chain of thought prompting by not requiring step-by-step few-shot examples and from prior template prompting by being task-agnostic, enabling multi-hop reasoning across various tasks with a single template. The method involves adding a specific text like \"Let\u2019s think step by step\" to prompt large language models to extract step-by-step reasoning without the need for task-specific examples.","contexts":["Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-speci\ufb01c few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%).","This result aligns with the few-shot experiment results in Wei et al. [2022].\nAppendix E shows extensive experiment results using wider variety of language models, including\nGPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain\nof thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\nsampled outputs for each model).\nError Analysis To better understand the behavior of Zero-shot-CoT, we manually investigated\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Ap-\npendix C for examples, where some of the observations include: (1) In commonsense reasoning\n(CommonsenseQA), Zero-shot-CoT often produces \ufb02exible and reasonable chain of thought even\nwhen the \ufb01nal prediction is not correct. Zero-shot-CoT often output multiple answer choices when\nthe model \ufb01nd it is dif\ufb01cult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\n7","In work, we call such prompts with explicit conditioning on few task examples as\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signi\ufb01cant\nboosts in performance across these dif\ufb01cult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such dif\ufb01cult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nasFew-shot-CoT in this work.\n3 Zero-shot Chain of Thought\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let\u2019s\nthink step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\n3.1 Two-stage prompting\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\nalready uses prompting in the form of \u201cThe answer is\u201d, to extract the answers in correct formats."],"ground_truth":"Zero-shot-CoT is a zero-shot template-based prompting method that elicits chain of thought reasoning from large language models. It does not require step-by-step few-shot examples and instead uses a single fixed prompt to prompt the models. This method encourages the discovery of broad cognitive abilities in LLMs rather than narrow task-specific skills."}
{"question":"How does prompt tuning affect model performance in improving NLP tasks?","answer":"Prompt tuning has been shown to enhance model performance in NLP tasks, particularly when using FLAN compared to LaMDA-PT. In various scenarios, including low-resource settings with limited training examples, prompt tuning with FLAN has demonstrated over 10% improvement compared to prompt tuning with LaMDA-PT. This indicates that prompt tuning, especially with FLAN, can significantly boost model performance in NLP tasks, showcasing the impact of instruction tuning on achieving more desirable outcomes for NLP applications.","contexts":["The orange bars indicate standard deviation among\ntemplates, averaged at the dataset level for each task cluster.\n4.5 I NSTRUCTION TUNING FACILITATES PROMPT TUNING\n32 training examplesFull training set1000507525Performance after prompt tuningInstruction-tuned modelUntuned model63.878.179.187.4\nFigure 10: Instruction-tuned\nmodels respond better to contin-\nuous inputs from prompt tuning.\nWhen prompt tuning on a given\ndataset, no tasks from the same\ncluster as that dataset were seen\nduring instruction tuning. Perfor-\nmance shown is the average on\nthe SuperGLUE dev set.As we\u2019ve seen that instruction tuning improves the ability of\na model to respond to instructions, it follows that, if FLAN is\nindeed more amenable to performing NLP tasks, then it should\nalso achieve better performance when performing inference using\nsoft prompts, represented by prepended continuous variables\noptimized via prompt tuning (Li & Liang, 2021; Lester et al.,\n2021). As further analysis, we train continuous prompts for each\nof the SuperGLUE (Wang et al., 2019a) tasks in accordance with\nthe cluster splits from \u00a72.2 such that when prompt-tuning on task\nT, no tasks in the same cluster as Twere seen during instruction\ntuning. Our prompt tuning setup follows the procedure of Lester\net al. (2021) except that we use a prompt length of 10, weight\ndecay of 1e-4, and did not use dropout on the attention scores; we\nfound in preliminary experiments that these changes improved\nthe performance of LaMDA-PT.\nFigure 10 shows the results of these prompt tuning experiments\nfor both using a fully-supervised training set and in a low-resource\nsetting with only 32 training examples. We see that in all sce-\nnarios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the\nlow-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over\nprompt tuning on the LaMDA-PT. This result exempli\ufb01es in another way how instruction tuning can\nresult in a checkpoint that is more desirable for performing NLP tasks.","Our prompt tuning setup follows the procedure of Lester\net al. (2021) except that we use a prompt length of 10, weight\ndecay of 1e-4, and did not use dropout on the attention scores; we\nfound in preliminary experiments that these changes improved\nthe performance of LaMDA-PT.\nFigure 10 shows the results of these prompt tuning experiments\nfor both using a fully-supervised training set and in a low-resource\nsetting with only 32 training examples. We see that in all sce-\nnarios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the\nlow-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over\nprompt tuning on the LaMDA-PT. This result exempli\ufb01es in another way how instruction tuning can\nresult in a checkpoint that is more desirable for performing NLP tasks.\n5 R ELATED WORK\nOur work relates to several broad research areas including zero-shot learning, prompting, multi-task\nlearning, and language models for NLP applications (Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Efrat & Levy, 2020; Aghajanyan et al., 2021; Li & Liang, 2021, inter alia ). We describe\nprior work for these broad areas in an extended related work section (Appendix D), and here we\ndescribe two subareas narrower in scope that perhaps relate most closely to our work.\n8","(2021)). Another line of work that shares this goal prompts models with continuous inputs optimized\nvia backpropagation to substantially improve performance (Li & Liang, 2021; Lester et al., 2021;\nQin & Eisner, 2021), as well as work that prompts models to produce specialized outputs (Wei et al.,\n2022). Although the success of these approaches depends heavily on model scale (Lester et al., 2021),\nfor which large models can be costly to serve, the ability of a single large model to perform many\ntasks slightly eases this burden. As shown by our experiments in \u00a74.5, prompt tuning is an orthogonal\nmethod for which instruction tuning can additionally improve performance. Reif et al. (2021) is\nsimilar to our work in that they also use related tasks to improve zero-shot learning, though they\ndiffer by only using related tasks in the context (and not \ufb01netuning), and focus on the application of\ntext style transfer.\nOur work shares similar motivations with prompting in that we use inference-time text interactions\nto prompt a single model, without creating separate checkpoints for each task. Whereas prompting\nwork such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is\nlikely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with\nan answer key), we hope that \ufb01netuning models to respond to natural language instructions instead of\ncompleting a sentence will make such large models more accessible to non-technical users.\nD.4 F INETUNING LARGE LANGUAGE MODELS\nFinetuning pretrained language models is a well-established method in NLP, with much of the work\nso far occurring on models in the range of 100M to 10B parameters (Dai & Le, 2015; Devlin et al.,\n2019; Raffel et al., 2020; Lewis et al., 2020, inter alia )."],"ground_truth":"Prompt tuning improves model performance in improving NLP tasks, especially when using FLAN. In many cases, prompt tuning on FLAN achieves more than 10% improvement over prompt tuning on LaMDA-PT. This result exemplifies how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks."}
{"question":"What is the purpose of instruction tuning in language models and how does it enhance response to NLP instructions for unseen tasks?","answer":"The purpose of instruction tuning in language models is to improve their ability to respond to NLP instructions. By using supervision to teach a language model to perform tasks described via instructions, instruction tuning enables the model to learn how to follow instructions and execute tasks even for unseen scenarios. This enhancement allows the language model to effectively understand and act upon natural language instructions for a wide range of tasks, ultimately improving its performance on tasks that it has not been explicitly trained on.","contexts":["Although one might\nexpect labeled data to have the most natural role in improving specialist models, instruction tuning\ndemonstrates how labeled data can be used to help large language models perform many, unseen\ntasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\ntask-speci\ufb01c training is complementary to general language modeling and motivates further research\non generalist models.\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\nwe try to use accepted categorizations in the literature), and we only explore the use of relatively\nshort instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers).\nA limitation for our evaluation is that individual examples might have appeared in the models\u2019\npretraining data, which includes web documents, though in post-hoc analysis (Appendix C) we do\nnot \ufb01nd any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN\n137B makes it costly to serve. Future work on instruction tuning could include gathering\/generating\neven more task clusters for \ufb01netuning, cross-lingual experiments, using FLAN to generate data for\ntraining downstream classi\ufb01ers, and using \ufb01netuning to improve model behavior with respect to bias\nand fairness (Solaiman & Dennison, 2021).\n7 C ONCLUSIONS\nThis paper has explored a simple method for improving the ability of language models at scale to\nperform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares\nfavorably against GPT-3 and signals the potential ability for language models at scale to follow\ninstructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot\nlearning, and using labeled data to improve large language models.\n9","In ablation studies, we \ufb01nd that increasing\nthe number of task clusters in instruction tuning improves performance on unseen tasks and that the\nbene\ufb01ts of instruction tuning emerge only with suf\ufb01cient model scale.\nInstruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects\nof both the pretrain\u2013\ufb01netune and prompting paradigms by using supervision via \ufb01netuning to\nimprove language model\u2019s responses to inference-time text interactions. Our empirical results\ndemonstrate promising abilities of language models to perform tasks described purely via instructions.\nSource code for loading the instruction tuning dataset used for FLAN is publicly available at\nhttps:\/\/github.com\/google-research\/flan .\n(A) Pretrain\u2013finetune (BERT, T5)Finetune on task AInference on task APretrained LM\u2022Typically requires many task-specific examples \u2022One specialized model for each task(B) Prompting (GPT-3)Inference on task APretrained LMImprove performance via few-shot prompting or prompt engineeringPretrained  LM(C) Instruction tuning (FLAN)Instruction-tune on many tasks:  B, C, D, \u2026Inference on task AInference on unseen taskModel learns to perform many tasks via natural language instructions\nFigure 2: Comparing instruction tuning with pretrain\u2013\ufb01netune and prompting.\n2 FLAN: I NSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING\nThe motivation of instruction tuning is to improve the ability of language models to respond to NLP\ninstructions. The idea is that by using supervision to teach an LM to perform tasks described via\ninstructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate\nperformance on unseen tasks, we group datasets into clusters by task type and hold out each task\ncluster for evaluation while instruction tuning on all remaining clusters.\n2","Sanh et al.\n(2021) \ufb01netune T5 in a setup similar to ours, \ufb01nding that zero-shot learning can be improved in a\nmodel of 11B parameters. At a model scale similar to ours, OpenAI\u2019s InstructGPT models are trained\nvia both \ufb01netuning and reinforcement learning to produce outputs that are more preferred by human\nraters (Ouyang et al., 2022).\n6 D ISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does \ufb01netuning a model on a\ncollection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\nthis question via instruction tuning, a simple method that combines appealing aspects of both\nthe pretrain\u2013\ufb01netune and prompting paradigms. Our instruction-tuned model, FLAN, improves\nperformance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\nwe evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\nof instruction tuning task clusters, and, interestingly, that performance improvements from instruction\ntuning emerge only with suf\ufb01cient model scale. Moreover, instruction tuning can be combined with\nother prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between\nspecialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan\net al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might\nexpect labeled data to have the most natural role in improving specialist models, instruction tuning\ndemonstrates how labeled data can be used to help large language models perform many, unseen\ntasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\ntask-speci\ufb01c training is complementary to general language modeling and motivates further research\non generalist models.\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\nwe try to use accepted categorizations in the literature), and we only explore the use of relatively\nshort instructions of typically a single sentence (c.f."],"ground_truth":"The purpose of instruction tuning in language models is to improve their ability to respond to NLP instructions. It involves using supervision to teach the language model to perform tasks described via instructions. By instruction tuning, the language model learns to follow instructions and can do so even for unseen tasks. This enhances the model's response to NLP instructions for tasks it has not encountered before."}
{"question":"What distinguishes Zero-shot-CoT from Few-shot-CoT in terms of prompting methods for large language models?","answer":"Zero-shot-CoT requires prompting LLMs twice but involves less engineering effort, while Few-shot-CoT necessitates careful human engineering of a few prompt examples with specific answer formats per task.","contexts":["Baselines We compare our Zero-shot-CoT mainly to standard Zero-shot prompting to verify the\neffectiveness of its chain of thought reasoning. For Zero-shot experiments, similar answer prompts\nas Zero-shot-CoT are used as default. See Appendix A.5 for detail. To better evaluate the zero-shot\nability of LLMs on reasoning tasks, we also compare our method to Few-shot and Few-shot-CoT\nbaselines from [Wei et al., 2022], using the same in-context examples. Throughout the experiments,\nwe use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore\ndeterministic. For the few-shot approaches, since the order of in-context examples could affect the\nresults [Lu et al., 2022], we run each experiment only once with a \ufb01xed seed across all methods and\ndatasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of\nexamples did not cause large variance in CoT experiments.\nAnswer cleansing After the model outputs a text by answer extraction (see \u00a7 3 and Figure 2), our\nmethod picks up only the part of the answer text that \ufb01rst satis\ufb01es the answer format. For example,\nif the answer prompting outputs \u201cprobably 375 and 376\u201d on arithmetic tasks, we extract the \ufb01rst\nnumber \u201c375\u201d and set it as the model prediction. In the case of multiple-choice, the \ufb01rst large letter\nwe encounter is set as the prediction. See Appendix A.6 for more detail. Standard Zero-shot method\nfollows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and\n\ufb01rst extract the answer text after \"The answer is \" from the model output, and apply the same answer\ncleansing to parse the answer text. If \u201cThe answer is\u201d is not found in the model output, we search\nfrom the back of the text and set the \ufb01rst text that satis\ufb01es the answer format as the prediction.\n4.1 Results\nZero-shot-CoT vs.","First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting\nthe correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also\nsometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT\ntend to fail when generated chain of thought include ternary operation, e.g. (3 + 2)\u22174.\nHow does prompt selection affect Zero-shot-CoT? We validate the robustness of Zero-shot-CoT\nagainst input prompts. Table 4 summarizes performance using 16 different templates with three\ncategories. Speci\ufb01cally, following Webson and Pavlick [2022], the categories include instructive\n(encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong\nway), and irrelevant (nothing to do with reasoning). The results indicate that the performance is\nimproved if the text is written in a way that encourages chain of thought reasoning, i.e., the templates\nare within \"instructive\" category. However, the difference in accuracy is signi\ufb01cant depending on\nthe sentence. In this experiment, \"Let\u2019s think step by step.\" achieves the best results. Interestingly,\nit is found that different templates encourage the model to express reasoning quite differently (see\nAppendix B for sample outputs by each template). In contrast, when we use misleading or irrelevant\ntemplates, the performance does not improve. It remains an open question how to automatically\ncreate better templates for Zero-shot-CoT.\nHow does prompt selection affect Few-shot-CoT? Table 5 shows the performance of Few-\nshot-CoT when using examples from different datasets: CommonsenseQA to AQUA-RAT and\nCommonsenseQA to MultiArith. The domains are different in both cases, but the answer format\n8","in Figure 1). In summary, Few-shot-CoT [Wei et al., 2022] requires careful human engineering of\na few prompt examples with speci\ufb01c answer formats per task, while Zero-shot-CoT requires less\nengineering but requires prompting LLMs twice.\n1st prompt: reasoning extraction In this step we \ufb01rst modify the input question xinto a prompt\nx\u2032using a simple template \u201cQ: [X]. A:[T]\u201d, where [X]is an input slot for xand[T]is an slot\nfor hand-crafted trigger sentence tthat would extract chain of though to answer the question x. For\nexample, if we use \u201cLet\u2019s think step by step\u201d as a trigger sentence, the prompt x\u2032would be \u201cQ: [X].\nA: Let\u2019s think step by step.\u201d. See Table 4 for more trigger examples. Prompted text x\u2032is then fed into\na language model and generate subsequent sentence z. We can use any decoding strategy, but we\nused greedy decoding throughout the paper for the simplicity.\n2nd prompt: answer extraction In the second step, we use generated sentence zalong with\nprompted sentence x\u2032to extract the \ufb01nal answer from the language model. To be concrete, we simply\nconcatenate three elements as with \u201c [X\u2032] [Z] [A] \u201d:[X\u2032]for 1st prompt x\u2032,[Z] for sentence z\ngenerated at the \ufb01rst step, and [A]for a trigger sentence to extract answer. The prompt for this step\nisself-augmented , since the prompt contains the sentence zgenerated by the same language model.\nIn experiment, we use slightly different answer trigger depending on the answer format. For example,\nwe use \u201cTherefore, among A through E, the answer is\u201d for multi-choice QA, and \u201cTherefore, the\nanswer (arabic numerals) is\u201d for math problem requiring numerical answer. See Appendix A.5 for\nthe lists of answer trigger sentences. Finally, the language model is fed the prompted text as input to\ngenerate sentences \u02c6 yand parse the \ufb01nal answer. See \u201cAnswer Cleansing\u201d at \u00a74 for the parser details."],"ground_truth":"Zero-shot-CoT differs from Few-shot-CoT in that it does not require step-by-step few-shot examples for prompting. It is a zero-shot template-based prompting method for chain of thought reasoning."}
{"question":"Which language models were used in the experiment 'Exploring Zero-Shot Learning in Neural Networks', and what were their parameters, libraries\/API names, and licenses?","answer":"The experiment 'Exploring Zero-Shot Learning in Neural Networks' used pretrained language models as mentioned in the context. The specific language models used were not explicitly stated in the provided information. The parameters, libraries\/API names, and licenses associated with these language models were not detailed in the context either.","contexts":["Published as a conference paper at ICLR 2022\nETHICAL CONSIDERATIONS\nThis work uses language models, for which the risks and potential harms are discussed in Bender &\nKoller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our\ncontribution in this paper is not a pretrained language model itself but rather an empirical study of\nhow instruction tuning affects the zero-shot performance of a language model on unseen tasks, we\nadditionally highlight two relevant ethical considerations. First, labeled datasets such as those we\nuse for \ufb01netuning can contain undesirable biases, and these biases can be propagated into zero-shot\napplications of the model on downstream tasks. And second, instruction-tuned models can potentially\nrequire less data and expertise to use; such lower barriers to access could increase both the bene\ufb01ts\nand associated risks of such models.\nENVIRONMENTAL CONSIDERATIONS\nWe use the same pretrained language models as Austin et al. (2021). The energy cost and carbon\nfootprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional\ninstruction tuning gradient-steps for \ufb01netuning FLAN is less than 2% of the number of pretraining\nsteps, and so the estimated additional energy cost is comparatively smaller.\nAUTHOR CONTRIBUTIONS\nMaarten Bosma conceived the original idea and implemented the \ufb01rst version of FLAN. Vincent Zhao\nprototyped the training and evaluation pipelines, as well as rank classi\ufb01cation. Kelvin Guu proposed\nand implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten\nBosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao,\nand Adams Wei Yu conducted and managed most of the experiments. Jason Wei designed and ran the\nablation studies. Jason Wei, Maarten Bosma, and Quoc V . Le wrote most of the paper. Jason Wei,\nMaarten Bosma, and Nan Du obtained the zero and few-shot baselines. Vincent Zhao and Kelvin Guu\ndesigned, implemented, and conducted the few-shot FLAN experiments.","A Details of Experimental Setup\nA.1 Code\nCode is available at https:\/\/github.com\/kojima-takeshi188\/zero_shot_cot .\nA.2 Datasets\nA.2.1 Dataset Description\nTable 7 summarizes the description of each dataset used in our experiment.\nTable 7: Dataset Description. Our experiments used publicly available datasets except for \u201cLast\nLetters\u201d and \u201cCoin Flip\u201d datasets. We created these two datasets. See Appendix A.2.2 for the details.\n(*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format.\n(*2) Average number of words in questions texts.\nDataset Answer\nFormat\n(*1)# of\nsamplesAvg #\nwords\n(*2)Data split (\ufb01lename)\nused for our experimentLicense\nSingleEq N 508 27.4 questions.json No License\nAddSub N 395 31.5 AddSub.json Unspeci\ufb01ed\nMultiArith N 600 31.8 MultiArith.json Unspeci\ufb01ed\nGSM8K N 1319 46.9 test.jsonl MIT License\nAQUA-RAT M 254 51.9 test.jsonl Apache-2.0\nSV AMP N 1000 31.8 SV AMP.json MIT License\nCommonsenseQA M 1221 27.8 dev_rand_split.jsonl Unspeci\ufb01ed\nStrategyQA Y 2290 9.6 task.json Apache-2.0\nDate Understanding M 369 35.0 task.json Apache-2.0\nShuf\ufb02ed Objects M 750 91.1 three_objects\/task.json Apache-2.0\nLast Letters F 500 15.0 - -\nCoin Flip Y 500 37.0 - -\nA.2.2 Dataset creation\nRegarding \u201cLast Letter Concatenation\u201d and \u201cCoin Flip\u201d, datasets are not publicly available so we\ncreated the datasets following Wei et al. [2022] with a minor rephrasing of the question template.\nSpeci\ufb01cally, as for Last Letter Concatenation, we use the following template.","951\u2013958. IEEE, 2009. URL https:\/\/ieeexplore.ieee.org\/\ndocument\/5206594 .\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli \u00b4c, and Goran Glava\u0161. From zero to hero: On the\nlimitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4483\u20134499,\n2020. URL https:\/\/aclanthology.org\/2020.emnlp-main.363 .\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pp. 6086\u20136096, 2019. URL https:\/\/aclanthology.org\/\nP19-1612 .\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. In International Conference on Learning Representations ,\n2020. URL https:\/\/openreview.net\/forum?id=qrwe7XHTmYb .\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-ef\ufb01cient prompt\ntuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing ,\n2021. URL https:\/\/arxiv.org\/abs\/2104.08691 .\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In\nThirteenth International Conference on the Principles of Knowledge Representation and Reasoning ,\n2012. URL https:\/\/dl.acm.org\/doi\/10.5555\/3031843.3031909 .\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\nvia reading comprehension."],"ground_truth":"The language models used in the experiment 'Exploring Zero-Shot Learning in Neural Networks' were PaLM with 540B parameters, Original GPT3 with 175B parameters (library\/API name: OpenAI API, model name: davinci), Original GPT3 with 6.7B parameters (library\/API name: OpenAI API, model name: curie), Original GPT3 with 1.3B parameters (library\/API name: OpenAI API, model name: babbage), Original GPT3 with 0.3B parameters (library\/API name: OpenAI API, model name: ada), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-davinci-002), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-davinci-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-curie-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-babbage-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-ada-001), OPT with 13B parameters (library\/API name: Hugging Face Library, model name: opt-13b), T0 with 11B parameters (library\/API name: Hugging Face Library, model name: T0pp), GPT-J with 6B parameters (library\/API name: Hugging Face Library, model name: gptj), GPT-Neo with 2.7B parameters (library\/API name: Hugging Face Library, model name: gpt-neo), and GPT-2 with 1.5B parameters (library\/API name: Hugging Face Library, model name: gpt2-xl). The licenses for the models are unspecified for most of them, except for Original GPT3 and GPT-2 which have an Apache-2.0 license."}
{"question":"How does Zero-shot-CoT differ from previous few-shot approaches in eliciting chain of thought from large language models?","answer":"Zero-shot-CoT differs from previous few-shot approaches by not requiring step-by-step few-shot examples and instead using a single template for eliciting multi-hop reasoning across various tasks.","contexts":["Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-speci\ufb01c few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%).","In work, we call such prompts with explicit conditioning on few task examples as\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signi\ufb01cant\nboosts in performance across these dif\ufb01cult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such dif\ufb01cult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nasFew-shot-CoT in this work.\n3 Zero-shot Chain of Thought\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let\u2019s\nthink step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\n3.1 Two-stage prompting\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\nalready uses prompting in the form of \u201cThe answer is\u201d, to extract the answers in correct formats.","This result aligns with the few-shot experiment results in Wei et al. [2022].\nAppendix E shows extensive experiment results using wider variety of language models, including\nGPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain\nof thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\nsampled outputs for each model).\nError Analysis To better understand the behavior of Zero-shot-CoT, we manually investigated\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Ap-\npendix C for examples, where some of the observations include: (1) In commonsense reasoning\n(CommonsenseQA), Zero-shot-CoT often produces \ufb02exible and reasonable chain of thought even\nwhen the \ufb01nal prediction is not correct. Zero-shot-CoT often output multiple answer choices when\nthe model \ufb01nd it is dif\ufb01cult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\n7"],"ground_truth":"Zero-shot-CoT differs from previous few-shot approaches in eliciting chain of thought from large language models by being a single zero-shot prompt that elicits chain of thought across a variety of reasoning tasks, whereas previous few-shot approaches require hand-crafting few-shot examples per task."}
{"question":"What are the stages in the Zero-shot-CoT method for reasoning and answer extraction, and how are they implemented?","answer":"The Zero-shot-CoT method for reasoning and answer extraction involves two main stages. Firstly, after the model outputs a text by answer extraction, the method selects only the part of the answer text that first satisfies the answer format. For example, in arithmetic tasks, if the answer prompting outputs multiple numbers, the method extracts the first number as the model prediction. In multiple-choice tasks, the first large letter encountered is set as the prediction. Secondly, for Few-shot and Few-shot-CoT methods, the answer text is extracted after \"The answer is\" from the model output, and the same answer cleansing process is applied to parse the answer text. If \"The answer is\" is not found in the model output, the method searches from the back of the text and sets the first text that satisfies the answer format as the prediction.","contexts":["Baselines We compare our Zero-shot-CoT mainly to standard Zero-shot prompting to verify the\neffectiveness of its chain of thought reasoning. For Zero-shot experiments, similar answer prompts\nas Zero-shot-CoT are used as default. See Appendix A.5 for detail. To better evaluate the zero-shot\nability of LLMs on reasoning tasks, we also compare our method to Few-shot and Few-shot-CoT\nbaselines from [Wei et al., 2022], using the same in-context examples. Throughout the experiments,\nwe use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore\ndeterministic. For the few-shot approaches, since the order of in-context examples could affect the\nresults [Lu et al., 2022], we run each experiment only once with a \ufb01xed seed across all methods and\ndatasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of\nexamples did not cause large variance in CoT experiments.\nAnswer cleansing After the model outputs a text by answer extraction (see \u00a7 3 and Figure 2), our\nmethod picks up only the part of the answer text that \ufb01rst satis\ufb01es the answer format. For example,\nif the answer prompting outputs \u201cprobably 375 and 376\u201d on arithmetic tasks, we extract the \ufb01rst\nnumber \u201c375\u201d and set it as the model prediction. In the case of multiple-choice, the \ufb01rst large letter\nwe encounter is set as the prediction. See Appendix A.6 for more detail. Standard Zero-shot method\nfollows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and\n\ufb01rst extract the answer text after \"The answer is \" from the model output, and apply the same answer\ncleansing to parse the answer text. If \u201cThe answer is\u201d is not found in the model output, we search\nfrom the back of the text and set the \ufb01rst text that satis\ufb01es the answer format as the prediction.\n4.1 Results\nZero-shot-CoT vs.","Our method gives on-par performances for the remaining two arithmetic reasoning\ntasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning.\nIn commonsense reasoning tasks, Zero-shot-CoT does not provide performance gains. It is expected\nas Wei et al. [2022] also reports that even Few-shot-CoT does not provide performance gains on\nLambda (135B), but does improve StrategyQA when combined with substantially larger PaLM\n(540B) model, which may also apply for ours. More importantly, we observe that many generated\nchain of thought themselves are surprisingly logically correct or only contains human-understandable\nmistakes (See Table 3), suggesting that Zero-shot-CoT does elicit for better commonsense reasoning\neven when the task metrics do not directly re\ufb02ect it. We provide samples generated by Zero-shot-CoT\nfor each dataset in Appendix B.\n6","Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-speci\ufb01c few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%)."],"ground_truth":"The Zero-shot-CoT method for reasoning and answer extraction involves two stages: reasoning extraction and answer extraction. In the reasoning extraction stage, the input question is modified into a prompt using a template. The prompt is fed into a language model to generate a subsequent sentence. In the answer extraction stage, the generated sentence and the prompted sentence are used to extract the final answer from the language model. The prompt for this stage is self-augmented, as it contains the sentence generated by the same language model. The language model is then fed the prompted text to generate sentences and parse the final answer."}
{"question":"What are the main approaches for inducing LLMs to perform chain-of-thought reasoning, according to Wei et al. in 2022?","answer":"The main approaches for inducing LLMs to perform chain-of-thought reasoning, according to Wei et al. in 2022, are through the use of chain of thought prompting (CoT). This can be achieved by either leveraging a simple prompt like \"Let\u2019s think step by step\" to facilitate step-by-step thinking before answering a question, or by providing manual demonstrations composed of a question and a reasoning chain that leads to an answer.","contexts":["To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%).\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-speci\ufb01c\nprompting work [Gao et al., 2021, Schick and Sch\u00fctze, 2021, Liu et al., 2021b], are often attributed\nto LLMs\u2019 ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\nreasoners by adding a simple prompt, Let\u2019s think step by step , to facilitate step-by-step thinking before\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\nproblem where the standard zero-shot approach fails.","1 I NTRODUCTION\nAlthough language models have demonstrated remarkable success across a range of NLP tasks, their\nability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by\nincreasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia ). In an effort\nto address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting , where\na language model is prompted to generate a series of short sentences that mimic the reasoning\nprocess a person might employ in solving a task. For example, given the question \u201cIf there are 3\ncars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\u201d , instead\nof directly responding with \u201c5\u201d, a language model would be prompted to respond with the entire\nchain-of-thought: \u201cThere are 3 cars in the parking lot already. 2 more arrive. Now there are 3 +\n2 = 5 cars. The answer is 5. \u201d . It has been observed that chain-of-thought prompting signi\ufb01cantly\nimproves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022).\nIn this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy\ndecoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves\nlanguage models\u2019 reasoning performance by a signi\ufb01cant margin. Self-consistency leverages the\nintuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct\nanswer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a\nproblem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\nFigure 1 illustrates the self-consistency method with an example.","AUTOMATIC CHAIN OF THOUGHT PROMPTING\nINLARGE LANGUAGE MODELS\nZhuosheng Zhang\u2020\u2217, Aston Zhang\u2021, Mu Li\u2021, Alex Smola\u2021\n\u2020Shanghai Jiao Tong University,\u2021Amazon Web Services\nABSTRACT\nLarge language models (LLMs) can perform complex reasoning by generating intermediate reasoning\nsteps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting.\nCoT prompting has two major paradigms. One leverages a simple prompt like \u201cLet\u2019s think step by\nstep\u201d to facilitate step-by-step thinking before answering a question. The other uses a few manual\ndemonstrations one by one, each composed of a question and a reasoning chain that leads to an\nanswer. The superior performance of the second paradigm hinges on the hand-crafting of task-speci\ufb01c\ndemonstrations one by one. We show that such manual efforts may be eliminated by leveraging\nLLMs with the \u201cLet\u2019s think step by step\u201d prompt to generate reasoning chains for demonstrations one\nby one, i.e., let\u2019s think not just step by step, but also one by one . However, these generated chains\noften come with mistakes. To mitigate the effect of such mistakes, we \ufb01nd that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-\nCoT. It samples questions with diversity and generates reasoning chains to construct demonstrations.\nOn ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the\nperformance of the CoT paradigm that requires manual designs of demonstrations. Code is available\nathttps:\/\/github.com\/amazon-research\/auto-cot\n1 Introduction\nLarge language models (LLMs) [Brown et al., 2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]\nhave performed impressively on complex reasoning tasks by decomposing the multi-step problems into intermediate\nsteps before producing the answer. This reasoning process is elicited by a very recent technique: chain-of-thought\n(CoT) prompting [Wei et al., 2022a].\nCoT prompting can be categorized into two major paradigms."],"ground_truth":"The main approaches for inducing LLMs to perform chain-of-thought reasoning, according to Wei et al. in 2022, are Zero-Shot-CoT and Manual-CoT. Zero-Shot-CoT adds a single prompt like \"Let's think step by step\" after the test question to facilitate reasoning chains in LLMs. Manual-CoT involves few-shot prompting with manual reasoning demonstrations one by one, where each demonstration consists of a question and a reasoning chain composed of intermediate steps and an expected answer."}
{"question":"Which sorting method has the most impact on Auto-CoT's demonstration accuracy in question clustering?","answer":"In-Cluster Min Dist sorting method has the most impact on Auto-CoT's demonstration accuracy in question clustering.","contexts":["[2022a], and the Auto-CoT results are averaged\nover three random runs. Overall, Auto-CoT consistently\nmatches or exceeds the performance of the CoT paradigm\nthat requires manual designs of demonstrations. Due to\nthe cost of manual designs, Manual-CoT may design the\nsame demonstrations for multiple datasets (e.g., 5\/6of the\narithmetic datasets). In contrast, Auto-CoT is more \ufb02exible and task-adaptive: every single dataset gets its own\ndemonstrations that are automatically constructed.\n5.3 Visualization of Question Clustering\nFigure 5 visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist\ngeneric patterns, where different patterns may be characterized by questions from different clusters. We present the\nconstructed demonstrations of Auto-CoT in Appendix D.\n#5\nAddSub SingleEq\nCoin Flip \nGSM8K\nLast Letter Concatenation\nMultiArith\nStrategyQA\nAQUA\nCSQA\n SVAMP\nFigure 5: Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.\n8","the effect of misleading by similarity (Section 3.1). On the other hand, if we took each demonstration as a kind of skill,\ndiverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists\na small portion (e.g., 1\/8) of mistakes in the demonstrations, the performance will not be negatively affected (to be\nshown in Figure 6).\nNevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such\nas from questions in the frequent-error cluster. As we will show later, some of these wrong demonstrations may be\neliminated with heuristics. For example, wrong demonstrations often come with long questions and long rationales.\nUsing simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps\nmitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix C.2).\n4 Auto-CoT: Automatic Chain-of-Thought Prompting\nBased on the observations and considerations in Section 3, we propose an Auto-CoT method to construct demonstrations\nwith questions and reasoning chains automatically. Auto-CoT consists of two main stages: (i) question clustering:\npartition questions of a given dataset into a few clusters; (ii) demonstration sampling: select a representative question\nfrom each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. The overall procedure\nis illustrated in Figure 4.","reasoning in LLMs [Wei et al., 2022a, Kojima et al., 2022]. The model is accessed via the OpenAI API.5Greedy\ndecoding is used to generate the output. We set max_tokens = 256 and temperature = 0. Following Wei et al. [2022a],\nthe number of demonstrations kused for in-context learning is 8 in most tasks, except for 4 in AQuA and Last Letter\nConcatenation, 7 in CSQA, and 6 in StrategyQA.\nC Analysis\nC.1 Comparisons of criteria for sorting questions\nWe compare different ways of sorting questions in each cluster, including: (i) minimal distance to the cluster center\n(In-Cluster Min Dist, as adopted in Auto-CoT), (ii) maximal distance to the cluster center (In-Cluster Max Dist), and\n(iii) random sampling inside the cluster (In-Cluster Random). To alleviate the in\ufb02uence of wrong demonstrations, we\nonly sample the demonstrations with correct answers for this analysis.\nTable 8: In\ufb02uence of demonstration sampling.\nMethod MultiArith\nAuto-CoT 93.7\nIn-Cluster Min Dist 93.7\nIn-Cluster Random 89.2\nIn-Cluster Max Dist 88.7\nComparing the results in Table 8, we see that the demonstrations are generally better if they are closer to the cluster\ncenter.\nC.2 Effectiveness of the simple heuristics\nIn Section 4, we apply simple heuristics to encourage the model to use simple and accurate demonstrations. Similar to\nthe criteria of the hand-crafting demonstrations in Wei et al."],"ground_truth":"The sorting method that has the most impact on Auto-CoT's demonstration accuracy in question clustering is the minimal distance to the cluster center (In-Cluster Min Dist)."}
{"question":"What are the pros and cons of prompting methods for large language models in terms of their cognitive abilities and task prompt sensitivity?","answer":"Prompting methods for large language models can help enhance cognitive abilities such as logical reasoning and system-2 thinking. They can facilitate the discovery of broad cognitive capabilities within these models. However, prompting methods can also amplify biases present in the training data, as language models tend to capture and reflect these biases. This could potentially lead to biased outcomes in tasks.","contexts":["On the other hand, our method is a multi-task prompt and elicits \u201cbroad generalization\u201d or broad\ncognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve\nas a reference for accelerating not just logical reasoning research with LLMs, but also discovery of\nother broad cognitive capabilities within LLMs.\nTraining Dataset Details A limitation of the work is the lack of public information on the details\nof training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs Instruct-\nGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-\narithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\ncon\ufb01rm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact Our work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.","Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to \ufb01netune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the \ufb01nal outputs of Python programs via\n\ufb01rst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the \ufb01nal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we \ufb01nd that chain-of-thought reasoning is an emergent property of model scale that allows\nsuf\ufb01ciently large language models to perform reasoning tasks that otherwise have \ufb02at scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9","Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\nleveraging language models for commonsense reasoning. In Proceedings of ACL , pages 4932\u20134942,\n2019. URL https:\/\/aclanthology.org\/P19-1487 .\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems , pages 1\u20137, 2021. URL https:\/\/arxiv.org\/pdf\/2102.07350.pdf .\nSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of EMNLP ,\npages 1743\u20131752, 2015. URL https:\/\/aclanthology.org\/D15-1202 .\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaf\ufb01n, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Man-\nica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala\nNeeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush.\nMultitask prompted training enables zero-shot task generalization. In ICLR , 2022. URL\nhttps:\/\/openreview.net\/forum?id=9Vrb9D0WI4 .\nTimo Schick and Hinrich Sch\u00fctze. It\u2019s not just size that matters: Small language models are also few-\nshot learners."],"ground_truth":"Our work is based on prompting methods for large language models. LLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset Details\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a method that looks to take advantage of the patterns captured by language models conducive to various tasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way to probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context learning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs."}
{"question":"What are the stages in Zero-shot-CoT for reasoning and answer extraction, and how is it different from Few-shot-CoT?","answer":"Zero-shot-CoT for reasoning involves using similar answer prompts as Zero-shot-CoT by default, and then extracting the part of the answer text that first satisfies the answer format. In the case of multiple-choice questions, the first large letter encountered is set as the prediction. On the other hand, Few-shot-CoT involves extracting the answer text after \"The answer is\" from the model output, and applying the same answer cleansing process to parse the answer text. If \"The answer is\" is not found in the model output, the process searches from the back of the text and sets the first text that satisfies the answer format as the prediction.","contexts":["Baselines We compare our Zero-shot-CoT mainly to standard Zero-shot prompting to verify the\neffectiveness of its chain of thought reasoning. For Zero-shot experiments, similar answer prompts\nas Zero-shot-CoT are used as default. See Appendix A.5 for detail. To better evaluate the zero-shot\nability of LLMs on reasoning tasks, we also compare our method to Few-shot and Few-shot-CoT\nbaselines from [Wei et al., 2022], using the same in-context examples. Throughout the experiments,\nwe use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore\ndeterministic. For the few-shot approaches, since the order of in-context examples could affect the\nresults [Lu et al., 2022], we run each experiment only once with a \ufb01xed seed across all methods and\ndatasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of\nexamples did not cause large variance in CoT experiments.\nAnswer cleansing After the model outputs a text by answer extraction (see \u00a7 3 and Figure 2), our\nmethod picks up only the part of the answer text that \ufb01rst satis\ufb01es the answer format. For example,\nif the answer prompting outputs \u201cprobably 375 and 376\u201d on arithmetic tasks, we extract the \ufb01rst\nnumber \u201c375\u201d and set it as the model prediction. In the case of multiple-choice, the \ufb01rst large letter\nwe encounter is set as the prediction. See Appendix A.6 for more detail. Standard Zero-shot method\nfollows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and\n\ufb01rst extract the answer text after \"The answer is \" from the model output, and apply the same answer\ncleansing to parse the answer text. If \u201cThe answer is\u201d is not found in the model output, we search\nfrom the back of the text and set the \ufb01rst text that satis\ufb01es the answer format as the prediction.\n4.1 Results\nZero-shot-CoT vs.","Our method gives on-par performances for the remaining two arithmetic reasoning\ntasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning.\nIn commonsense reasoning tasks, Zero-shot-CoT does not provide performance gains. It is expected\nas Wei et al. [2022] also reports that even Few-shot-CoT does not provide performance gains on\nLambda (135B), but does improve StrategyQA when combined with substantially larger PaLM\n(540B) model, which may also apply for ours. More importantly, we observe that many generated\nchain of thought themselves are surprisingly logically correct or only contains human-understandable\nmistakes (See Table 3), suggesting that Zero-shot-CoT does elicit for better commonsense reasoning\neven when the task metrics do not directly re\ufb02ect it. We provide samples generated by Zero-shot-CoT\nfor each dataset in Appendix B.\n6","Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-speci\ufb01c few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%)."],"ground_truth":"Zero-shot-CoT involves two stages: reasoning extraction and answer extraction. In the reasoning extraction stage, the input question is modified into a prompt using a template. The prompt is fed into a language model to generate a subsequent sentence. In the answer extraction stage, the generated sentence and the prompted sentence are used to extract the final answer from the language model. Zero-shot-CoT differs from Few-shot-CoT in that it does not require step-by-step few-shot examples."}
{"question":"How does the number of datasets and templates affect the performance of instruction tuning in the FLAN model?","answer":"Using more datasets per task cluster improves performance significantly, with an almost 10% improvement on average across three held-out clusters. On the other hand, increasing the number of templates per dataset has a minimal effect on performance, especially when there is only one task per cluster. This effect disappears when there are four tasks per cluster. The original expectation that composing ten templates per task would prevent overfitting to any specific template was not fully realized, highlighting the unpredictability of fine-tuning large language models.","contexts":["Published as a conference paper at ICLR 2022\nB F URTHER ABLATION STUDIES AND ANALYSIS\nB.1 D ATASETS PER TASK CLUSTER & T EMPLATES PER DATASET\nOur primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on\nunseen tasks. \u00a74.1 showed that adding more task clusters improves performance; here, we further\nexplore whether adding additional datasets improves performance when the number of task clusters\nis held constant. We use the same split as in \u00a74.1, where the NLI, commonsense reasoning, and\nclosed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning. For\nthese seven task clusters, we instruction tune models using just one dataset per task cluster and using\nfour datasets per task cluster (for task clusters that did not have four tasks, we just used all available\ntasks). In addition, we simultaneously explore the role of the number of instruction templates per\ndataset; as mentioned in \u00a72.1, for each dataset we manually composed ten instructional templates for\ninstruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\nFigure 11 shows these results. Using more datasets per cluster improved performance by almost\n10% on average across the three held-out clusters. Using more templates per dataset, however,\nhad a comparatively negligible effect on performance when there was one task per cluster, which\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\noriginal motivation that composing ten templates per task would mitigate over\ufb01tting to any particular\ntemplate. This results serves to underscore, however, the unpredictability of \ufb01netuning large language\nmodels, as one hypothesis is that models at such scale do not easily over\ufb01t to a \ufb01netuning single task.\nFigure 11: Effect of datasets per task cluster and templates per dataset on performance on three\nheld-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task\ncluster substantially improves performance. Using more templates per dataset, however, only had\na very small effect on performance, which disappeared when there were suf\ufb01cient dataset per task\ncluster.","Using more templates per dataset, however,\nhad a comparatively negligible effect on performance when there was one task per cluster, which\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\noriginal motivation that composing ten templates per task would mitigate over\ufb01tting to any particular\ntemplate. This results serves to underscore, however, the unpredictability of \ufb01netuning large language\nmodels, as one hypothesis is that models at such scale do not easily over\ufb01t to a \ufb01netuning single task.\nFigure 11: Effect of datasets per task cluster and templates per dataset on performance on three\nheld-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task\ncluster substantially improves performance. Using more templates per dataset, however, only had\na very small effect on performance, which disappeared when there were suf\ufb01cient dataset per task\ncluster.\nB.2 R OLE OF INSTRUCTIONS DURING FINETUNING\nThe per-cluster results for the ablation study from \u00a74.3 are shown in Table 3.\nB.3 F URTHER ANALYSIS : INSTRUCTION TUNING FACILITATES PROMPT TUNING\nThe per-dataset results for the analysis in \u00a74.5 are given in Table 4. As the above tasks are all\nclassi\ufb01cation, further work in this direction might include tasks such as summarization or question\nanswering, or try to \ufb01netune the model using the supervised datasets.\nC D ATA CONTAMINATION ANALYSIS\nOne reasonable concern is that since the pretraining corpus of FLAN has more than 2 trillion tokens,\nit is possible that examples from a given evaluation dataset may have already been seen verbatim\nby the model during pre-training, hence in\ufb02ating the performance of our purported zero-shot model.\nTo this end, like GPT-3 (Brown et al., 2020), we perform post-hoc data contamination analysis to\n24","The input and target sequence lengths\nused in \ufb01netuning are 1024 and 256, respectively. We use packing (Raffel et al., 2020) to combine\nmultiple training examples into a single sequence, separating inputs from targets using a special EOS\ntoken. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations,\nwe report results on the \ufb01nal checkpoint trained for 30k steps.\n3 R ESULTS\nWe evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transla-\ntion, commonsense reasoning, coreference resolution, and struct-to-text. As described in \u00a72.2, we\nevaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for\nevaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses\na different checkpoint). For each dataset, we evaluate the mean of performance on all templates,\nwhich proxies the expected performance given a typical natural language instruction. As a dev set is\nsometimes available for manual prompt engineering (Brown et al., 2020), for each dataset we also\nobtain the test set performance using the template with the best dev set performance.\nFor comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as\nGPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning). This\nbaseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning\nsigni\ufb01cantly improves LaMDA-PT on most datasets.\nWe also show the zero-shot performances of GPT-3 175B (Brown et al., 2020) and GLaM 64B\/64E\n(Du et al., 2021), as reported in their respective papers. With the best dev template, zero-shot FLAN\noutperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3\u2019s few-shot performance\non 10 datasets. With the best dev-template, zero-shot FLAN outperforms zero-shot GLaM on 13 of\n19 available datasets and one-shot GLaM on 11 of 19 datasets."],"ground_truth":"Using more datasets per task cluster improves performance, while using more templates per dataset has a negligible effect on performance."}
{"question":"What technique surpasses zero-shot large language models on diverse reasoning tasks without hand-crafted few-shot examples?","answer":"Zero-shot-CoT technique surpasses zero-shot large language models on diverse reasoning tasks without hand-crafted few-shot examples.","contexts":["Large Language Models are Zero-Shot Reasoners\nTakeshi Kojima\nThe University of Tokyo\nt.kojima@weblab.t.u-tokyo.ac.jpShixiang Shane Gu\nGoogle Research, Brain Team\nMachel Reid\nGoogle Research\u2217Yutaka Matsuo\nThe University of TokyoYusuke Iwasawa\nThe University of Tokyo\nAbstract\nPretrained large language models (LLMs) are widely used in many sub-\ufb01elds of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-speci\ufb01c exemplars. Notably, chain of thought (CoT) prompting,\na recent technique for eliciting complex multi-step reasoning through step-by-\nstep answer examples, achieved the state-of-the-art performances in arithmetics\nand symbolic reasoning, dif\ufb01cult system-2 tasks that do not follow the standard\nscaling laws for LLMs. While these successes are often attributed to LLMs\u2019\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\nby simply adding \u201cLet\u2019s think step by step\u201d before each answer. Experimental\nresults demonstrate that our Zero-shot-CoT, using the same single prompt template,\nsigni\ufb01cantly outperforms zero-shot LLM performances on diverse benchmark\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\nUnderstanding, Tracking Shuf\ufb02ed Objects), without any hand-crafted few-shot\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\n002), as well as similar magnitudes of improvements with another off-the-shelf\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\nvery diverse reasoning tasks hints at untapped and understudied fundamental\nzero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\ncapabilities may be extracted by simple prompting.","Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\nleveraging language models for commonsense reasoning. In Proceedings of ACL , pages 4932\u20134942,\n2019. URL https:\/\/aclanthology.org\/P19-1487 .\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems , pages 1\u20137, 2021. URL https:\/\/arxiv.org\/pdf\/2102.07350.pdf .\nSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of EMNLP ,\npages 1743\u20131752, 2015. URL https:\/\/aclanthology.org\/D15-1202 .\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaf\ufb01n, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Man-\nica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala\nNeeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush.\nMultitask prompted training enables zero-shot task generalization. In ICLR , 2022. URL\nhttps:\/\/openreview.net\/forum?id=9Vrb9D0WI4 .\nTimo Schick and Hinrich Sch\u00fctze. It\u2019s not just size that matters: Small language models are also few-\nshot learners.","Published as a conference paper at ICLR 2022\n1 I NTRODUCTION\nLanguage models (LMs) at scale, such as GPT-3 (Brown et al., 2020), have been shown to perform\nfew-shot learning remarkably well. They are less successful at zero-shot learning, however. For\nexample, GPT-3\u2019s zero-shot performance is much worse than few-shot performance on tasks such as\nreading comprehension, question answering, and natural language inference. One potential reason\nis that, without few-shot exemplars, it is harder for models to perform well on prompts that are not\nsimilar to the format of the pretraining data.\nIn this paper, we explore a simple method to improve the zero-shot performance of large language\nmodels, which would expand their reach to a broader audience. We leverage the intuition that NLP\ntasks can be described via natural language instructions, such as \u201c Is the sentiment of this movie review\npositive or negative? \u201d or \u201c Translate \u2018how are you\u2019 into Chinese. \u201d We take a pretrained language\nmodel of 137B parameters and perform instruction tuning \u2014\ufb01netuning the model on a mixture of\nmore than 60 NLP datasets expressed via natural language instructions. We refer to this resulting\nmodel as FLAN, for F inetuned La nguage N et.\nTo evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters\nbased on their task types and hold out each cluster for evaluation while instruction tuning FLAN\non all other clusters. For example, as shown in Figure 1, to evaluate FLAN\u2019s ability to perform\nnatural language inference, we instruction tune the model on a range of other NLP tasks such as\ncommonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has\nnot seen any natural language inference tasks in instruction tuning, we then evaluate its ability to\nperform zero-shot natural language inference.\nOur evaluations show that FLAN substantially improves the zero-shot performance of the base\n137B-parameter model."],"ground_truth":"Chain of thought (CoT) prompting"}
{"question":"How does language model scale impact instruction tuning on zero-shot performance?","answer":"For larger language models with hundreds of billions of parameters, instruction tuning through finetuning on a variety of datasets described via natural language instructions substantially improves zero-shot performance on unseen tasks. However, for smaller models with fewer parameters, instruction tuning can actually hurt generalization to new tasks. This could be due to the smaller models' capacity being filled by learning the tasks during instruction tuning, leading to worse performance on new tasks. In contrast, larger models benefit from instruction tuning as it helps them learn how to follow instructions while still having capacity to generalize to new tasks.","contexts":["Published as a conference paper at ICLR 2022\n4.2 S CALING LAWS\n0.4B2B8B68B137BModel Size (# parameters)3040506070Instruction tuningUntuned modelAverage zero-shot accuracy  on 13 held-out tasks (%) Performance on held-out tasks\nFigure 7: Whereas instruction tuning helps large\nmodels generalize to new tasks, for small models it\nactually hurts generalization to unseen tasks, poten-\ntially because all model capacity is used to learn the\nmixture of instruction tuning tasks.As Brown et al. (2020) shows that zero and\nfew-shot capabilities of language models sub-\nstantially improve for larger models, we next\nexplore how the bene\ufb01ts of instruction tuning\nare affected by model scale. Using the same\ncluster split as in the previous ablation study,\nwe evaluate the effect of instruction tuning\non models of size 422M, 2B, 8B, 68B, and\n137B parameters.\nFigure 7 shows these results. We see that\nfor the two models on the order of 100B pa-\nrameters, instruction tuning substantially im-\nproves performance on held-out tasks, as is\nexpected given the prior results in our pa-\nper. The behavior on held-out tasks for the\n8B and smaller models, however, is thought-\nprovoking\u2014instruction tuning actually hurts performance on held-out tasks. One potential explana-\ntion for this result could be that for small-scale models, learning the \u223c40 tasks used during instruction\ntuning \ufb01lls the entire model capacity, causing these models to perform worse on new tasks. Under\nthis potential explanation, for the larger scale models, instruction tuning \ufb01lls up some model capacity\nbut also teaches these models how to follow instructions, allowing them to generalize to new tasks\nwith the remaining capacity.\n4.3 R OLE OF INSTRUCTIONS\n2030405060FT: no instruction\nEval: instructionFT: dataset name\nEval: instructionFT: dataset name\nEval: dataset nameFT: instruction\nEval: instruction\n(FLAN)37.346.647.055.2Zero-shot performance \n(4 task cluster avg.)","Published as a conference paper at ICLR 2022\nFINETUNED LANGUAGE MODELS AREZERO-SHOT\nLEARNERS\nJason Wei\u2217, Maarten Bosma\u2217, Vincent Y. Zhao\u2217, Kelvin Guu\u2217, Adams Wei Yu,\nBrian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le\nGoogle Research\nABSTRACT\nThis paper explores a simple method for improving the zero-shot learning abilities\nof language models. We show that instruction tuning \u2014\ufb01netuning language models\non a collection of datasets described via instructions\u2014substantially improves zero-\nshot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction tune it on\nover 60 NLP datasets verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task types.\nFLAN substantially improves the performance of its unmodi\ufb01ed counterpart and\nsurpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even\noutperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC,\nOpenbookQA, and StoryCloze. Ablation studies reveal that number of \ufb01netuning\ndatasets, model scale, and natural language instructions are key to the success of\ninstruction tuning.\nTargetInput (Commonsense Reasoning)\nkeep stack of pillow cases in fridgeInference on unseen task typeFinetune on many tasks (\u201cinstruction-tuning\u201d)\n\u2026Translate this sentence to Spanish: The new office building was built in less than three months.Input (Translation)\nEl nuevo edificio de oficinas se construy\u00f3 en tres meses.TargetInput (Natural Language Inference)\nIt is not possible to tellFLAN ResponseCoreference resolution tasksSentiment analysis tasksGPT-3 175B zero shotGPT-3 175B few-shotFLAN 137B zero-shotPerformance on unseen task typesNatural language inference42.953.256.2Reading Comprehension63.772.677.4Closed-Book QA49.855.756.6Here is a goal: Get a cool sleep on summer days. How would you accomplish this goal?","Published as a conference paper at ICLR 2022\n1 I NTRODUCTION\nLanguage models (LMs) at scale, such as GPT-3 (Brown et al., 2020), have been shown to perform\nfew-shot learning remarkably well. They are less successful at zero-shot learning, however. For\nexample, GPT-3\u2019s zero-shot performance is much worse than few-shot performance on tasks such as\nreading comprehension, question answering, and natural language inference. One potential reason\nis that, without few-shot exemplars, it is harder for models to perform well on prompts that are not\nsimilar to the format of the pretraining data.\nIn this paper, we explore a simple method to improve the zero-shot performance of large language\nmodels, which would expand their reach to a broader audience. We leverage the intuition that NLP\ntasks can be described via natural language instructions, such as \u201c Is the sentiment of this movie review\npositive or negative? \u201d or \u201c Translate \u2018how are you\u2019 into Chinese. \u201d We take a pretrained language\nmodel of 137B parameters and perform instruction tuning \u2014\ufb01netuning the model on a mixture of\nmore than 60 NLP datasets expressed via natural language instructions. We refer to this resulting\nmodel as FLAN, for F inetuned La nguage N et.\nTo evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters\nbased on their task types and hold out each cluster for evaluation while instruction tuning FLAN\non all other clusters. For example, as shown in Figure 1, to evaluate FLAN\u2019s ability to perform\nnatural language inference, we instruction tune the model on a range of other NLP tasks such as\ncommonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has\nnot seen any natural language inference tasks in instruction tuning, we then evaluate its ability to\nperform zero-shot natural language inference.\nOur evaluations show that FLAN substantially improves the zero-shot performance of the base\n137B-parameter model."],"ground_truth":"For larger language models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks. However, for smaller models (8B and smaller), instruction tuning actually hurts performance on held-out tasks. One potential explanation for this result is that for small-scale models, learning the tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse."}
{"question":"What's the advantage of using Zero-shot-CoT prompts over Few-shot-CoT prompts for multi-step reasoning in large language models?","answer":"Zero-shot-CoT prompts offer the advantage of not requiring task-specific examples for each individual task, unlike Few-shot-CoT prompts which rely on carefully-crafted and task-specific step-by-step examples. This versatility allows Zero-shot-CoT to facilitate step-by-step reasoning across various tasks without the need to modify the prompt for each task, making it more adaptable and efficient for multi-step reasoning in large language models.","contexts":["First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting\nthe correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also\nsometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT\ntend to fail when generated chain of thought include ternary operation, e.g. (3 + 2)\u22174.\nHow does prompt selection affect Zero-shot-CoT? We validate the robustness of Zero-shot-CoT\nagainst input prompts. Table 4 summarizes performance using 16 different templates with three\ncategories. Speci\ufb01cally, following Webson and Pavlick [2022], the categories include instructive\n(encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong\nway), and irrelevant (nothing to do with reasoning). The results indicate that the performance is\nimproved if the text is written in a way that encourages chain of thought reasoning, i.e., the templates\nare within \"instructive\" category. However, the difference in accuracy is signi\ufb01cant depending on\nthe sentence. In this experiment, \"Let\u2019s think step by step.\" achieves the best results. Interestingly,\nit is found that different templates encourage the model to express reasoning quite differently (see\nAppendix B for sample outputs by each template). In contrast, when we use misleading or irrelevant\ntemplates, the performance does not improve. It remains an open question how to automatically\ncreate better templates for Zero-shot-CoT.\nHow does prompt selection affect Few-shot-CoT? Table 5 shows the performance of Few-\nshot-CoT when using examples from different datasets: CommonsenseQA to AQUA-RAT and\nCommonsenseQA to MultiArith. The domains are different in both cases, but the answer format\n8","Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-speci\ufb01c few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatis\ufb01es the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting signi\ufb01cantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%).","While the successes of CoT prompting [Wei et al., 2022], along those of many other task-speci\ufb01c\nprompting work [Gao et al., 2021, Schick and Sch\u00fctze, 2021, Liu et al., 2021b], are often attributed\nto LLMs\u2019 ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\nreasoners by adding a simple prompt, Let\u2019s think step by step , to facilitate step-by-step thinking before\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\nproblem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and\ntask-agnostic , unlike most prior task-speci\ufb01c prompt engineering in the forms of examples (few-shot)\nor templates (zero-shot) [Liu et al., 2021b]: it can facilitate step-by-step answers across various\nreasoning tasks, including arithmetic (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021],\nAQUA-RAT [Ling et al., 2017], and SV AMP [Patel et al., 2021]), symbolic reasoning (Last letter and\nCoin \ufb02ip), commonsense reasoning (CommonSenseQA [Talmor et al., 2019] and Strategy QA [Geva\net al., 2021]), and other logical reasoning tasks (Date understanding and Tracking Shuf\ufb02ed Objects\nfrom BIG-bench [Srivastava et al., 2022]) without modifying the prompt per task.\nWe empirically evaluate Zero-shot-CoT against other prompting baselines in Table 2. While our\nZero-shot-CoT underperforms Few-shot-CoT with carefully-crafted and task-speci\ufb01c step-by-step ex-\namples, Zero-shot-CoT achieves enormous score gains compared to the zero-shot baseline, e.g."],"ground_truth":"Zero-shot-CoT prompts offer the advantage of not requiring step-by-step few-shot examples, making them more versatile and scalable for multi-step reasoning in large language models. They provide a stronger zero-shot baseline and encourage the discovery of broad cognitive abilities in LLMs."}
{"question":"What's the difference in unresolving rate between Retrieval-Q-CoT and Random-Q-CoT with extra demonstrations?","answer":"The unresolving rate of Retrieval-Q-CoT is 46.9%, while the unresolving rate of Random-Q-CoT is 25.8%.","contexts":["Let us simply call demonstrations\nwith wrong answers as wrong demonstrations . Intuitively, after similar questions to a test question are retrieved, wrong\ndemonstrations caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g.,\nreplicating mistakes) for the test question. We refer to this phenomenon as misleading by similarity . We will investigate\nwhether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT.\nRetrieval-Q-CoT Random-Q-CoT20304050Rate (%)\nFigure 2: Unresolving Rate.To begin with, we invoke Zero-Shot-CoT on all the 600 questions\nfrom the MultiArith dataset. Among them, we collect those 128\nquestions (denoted as Q) where Zero-Shot-CoT generates wrong\nanswers (error rate: 21.3% = 128 \/600). As we mentioned, with\nextra demonstrations, Retrieval-Q-CoT and Random-Q-CoT are\nexpected to perform more competitively than Zero-Shot-CoT. Among\nQwhere Zero-Shot-CoT fails, we call those where Retrieval-Q-CoT\nor Random-Q-CoT still fail as their unresolved questions . We divide\nthe number of unresolved questions by 128 (number of questions in\nQ) to calculate the unresolving rate . A higher unresolving rate means\nthat a method more likely still makes mistakes like Zero-Shot-CoT.\nFigure 2 shows that the unresolving rate of Retrieval-Q-CoT (46.9%)\nis much higher than Random-Q-CoT (25.8%). It indicates that with similar questions being sampled for test questions,\nRetrieval-Q-CoT is negatively affected by misleading by similarity.\nTo show that unresolved questions of Retrieval-Q-CoT tend to be similar, we present a case study in Table 2. In the left\npart, the retrieved demonstration questions are similar to the test question and ask \u201c how long will it take him to cook the\nrest? \u201d The reasoning chains generated by Zero-Shot-CoT produce answers regarding \u201c the total of \u201d instead of \u201c the rest \u201d.\nFollowing the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of \u201c the rest \u201d.","A higher unresolving rate means\nthat a method more likely still makes mistakes like Zero-Shot-CoT.\nFigure 2 shows that the unresolving rate of Retrieval-Q-CoT (46.9%)\nis much higher than Random-Q-CoT (25.8%). It indicates that with similar questions being sampled for test questions,\nRetrieval-Q-CoT is negatively affected by misleading by similarity.\nTo show that unresolved questions of Retrieval-Q-CoT tend to be similar, we present a case study in Table 2. In the left\npart, the retrieved demonstration questions are similar to the test question and ask \u201c how long will it take him to cook the\nrest? \u201d The reasoning chains generated by Zero-Shot-CoT produce answers regarding \u201c the total of \u201d instead of \u201c the rest \u201d.\nFollowing the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of \u201c the rest \u201d. In contrast,\nRandom-Q-CoT correctly understands \u201c the rest \u201d better without making similar mistakes in the demonstrations, thanks\nto relatively more diverse (random) demonstrations.\n3.2 Errors Frequently Fall into the Same Cluster\nMotivated by the observations in Table 2, we use k-means to partition all the 600 test questions into k= 8clusters,\nwhere each cluster contains similar questions.2With these clusters and reasoning chains generated by Zero-Shot-CoT\n2We use Sentence-BERT [Reimers and Gurevych, 2019] to encode questions and apply k-means for clustering.\n4","On a high level, both Retrieval-Q-CoT and Random-Q-CoT take the concatenation of qdemo\ni, cdemo\nipairs ( i= 1, . . . , k )\nandqtestas input to predict the reasoning chain for qtest, which contains the answer in the end (like right of Figure 1).\nTable 1: Accuracy (%) of different sampling\nmethods. Symbol\u2020indicates using training sets\nwith annotated reasoning chains.\nMethod MultiArith GSM8K AQuA\nZero-Shot-CoT 78.7 40.7 33.5\nManual-CoT 91.7 46.9 35.8\u2020\nRandom-Q-CoT 86.2 47.6\u202036.2\u2020\nRetrieval-Q-CoT 82.8 48.0\u202039.7\u2020To our surprise, Retrieval-Q-CoT underperforms Random-Q-CoT\non the arithmetic dataset MultiArith [Roy and Roth, 2015] (Table\n1). Note that the retrieval methods were originally proposed in\ntasks with annotated labels [Rubin et al., 2022, Su et al., 2022],\nhowever, invoking Zero-Shot-CoT does not guarantee entirely correct\nreasoning chains. Thus, we hypothesize that the inferior performance\nof Retrieval-Q-CoT is caused by incorrect reasoning chains by Zero-\nShot-CoT. To test this hypothesis, we experiment with Retrieval-Q-\nCoT on two other datasets GSM8K [Cobbe et al., 2021] and AQuA\n[Ling et al., 2017] that have training sets with annotated reasoning\nchains. The results are shown with \u2020in Table 1. Under the setting\nwith annotated reasoning chains, Retrieval-Q-CoT even outperforms\nManual-CoT. The result indicates that Retrieval-Q-CoT is effective\nwhen human annotations are available.\nAlthough human annotations are useful, such manual efforts are nontrivial. However, automatically generating reasoning\nchains via Zero-Shot-CoT underperforms Manual-CoT, especially when the challenge of question sampling is not\naddressed. To design more effective Auto-CoT, we need to understand its challenge better."],"ground_truth":"The unresolving rate of Retrieval-Q-CoT is 46.9% and the unresolving rate of Random-Q-CoT is 25.8%. Therefore, the difference in unresolving rate between Retrieval-Q-CoT and Random-Q-CoT with extra demonstrations is 21.1%."}
{"question":"What are the pros and cons of prompting methods for large language models in terms of bias and unbiased study?","answer":"Prompting methods for large language models can be advantageous as they offer a more direct approach to probing complex reasoning inside pre-trained models. This method removes the confounding factor of in-context learning seen in prior few-shot approaches, allowing for a more unbiased study of biases present in these models. However, it is important to note that large language models trained on diverse datasets from the web can capture and amplify biases found in the training data. Prompting methods, by leveraging patterns captured by language models, may inherit these biases, leading to potential limitations in terms of bias mitigation and unbiased study of biases in language models.","contexts":["6.2.4 Future Bias and Fairness Challenges\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\nand to highlight the inherent dif\ufb01culties in characterizing biases in large-scale generative models; we expect this to be an\narea of continuous research for us and are excited to discuss different methodological approaches with the community.\nWe view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but\nwe recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18].\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this\nis also extensive [ QMZH19 ,HZJ+19], so we offer only a few brief comments on future directions speci\ufb01c to large\nlanguage models. In order to pave the way for effective bias prevention in general purpose models, there is a need for\nbuilding a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for\nthese models. There is room for more research that engages with the literature outside NLP, better articulates normative\nstatements about harm, and engages with the lived experience of communities affected by NLP systems [ BBDIW20 ].\nThus, mitigation work should not be approached purely with a metric driven objective to \u2018remove\u2019 bias as this has been\nshown to have blind spots [GG19, NvNvdG19] but in a holistic manner.\n6.3 Energy Usage\nPractical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3\n175B consumed several thousand peta\ufb02op\/s-days of compute during pre-training, compared to tens of peta\ufb02op\/s-days\nfor a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and ef\ufb01ciency of such\nmodels, as advocated by [SDSE19].","On the other hand, our method is a multi-task prompt and elicits \u201cbroad generalization\u201d or broad\ncognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve\nas a reference for accelerating not just logical reasoning research with LLMs, but also discovery of\nother broad cognitive capabilities within LLMs.\nTraining Dataset Details A limitation of the work is the lack of public information on the details\nof training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs Instruct-\nGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-\narithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\ncon\ufb01rm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact Our work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.","Overall, there are no guarantees that the reasoning processes generated by large language models\nare coherent or factually correct, as underscored by the recent work evaluating the factuality of\nlanguage model generations and explanations (Maynez et al., 2020; Rashkin et al., 2021; Ye and\nDurrett, 2022; Marasovi \u00b4c et al., 2022; Wiegreffe et al., 2022). Incorrect reasoning processes can lead\nto both incorrect \ufb01nal answers as well as accidentally correct \ufb01nal answers (with accidentally correct\n\ufb01nal answers being more likely for tasks such as binary classi\ufb01cation as opposed to free response).\nImproving the factuality of language model generations with respect to context and world knowledge\nis an important direction open problems in language model research and could also be expected to\npotentially improve multi-step reasoning abilities of language models. One potential method for\nimproving the quality of decoding could involve generating multiple reasoning paths and scoring\neach of them with a veri\ufb01er, though this requires training the veri\ufb01er (Cobbe et al., 2021; Shen et al.,\n2021; Thoppilan et al., 2022).\nD.3 Additional Robustness Analysis\nAs the experiments in the main paper use a \ufb01xed number of few-shot exemplars (8; as constrained by\nthe input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various\nnumbers of few-shot exemplars. We run experiments for LaMDA 137B, comparing chain-of-thought\nprompting with standard prompting for the \ufb01ve datasets where standard prompting had a mostly \ufb02at\nscaling curve (the largest model did not achieve high performance). As shown in Figure 11, the\nimprovement of chain-of-thought prompting over standard prompting remains robust to varying the\nnumber of few-shot exemplars in the prompt.\n28"],"ground_truth":"Prompting methods for large language models have both pros and cons in terms of bias and unbiased study. On one hand, these methods take advantage of the patterns captured by language models, which can amplify biases found in the training data. This means that the prompts may inherit and perpetuate biases present in the training data. On the other hand, prompting methods provide a more direct way to probe complex reasoning inside pre-trained language models, removing the confounding factor of in-context learning in prior few-shot approaches. This can lead to a more unbiased study of biases in language models. Overall, while prompting methods have the potential for bias, they also offer opportunities for unbiased exploration of language models."}
{"question":"What are the stages and processes in the Auto-CoT method for constructing demonstrations?","answer":"The stages and processes in the Auto-CoT method for constructing demonstrations involve the following steps:\n1. Encoding test questions using Sentence-BERT.\n2. Sampling demonstration questions based on similarity using Retrieval-Q-CoT method or randomly using Random-Q-CoT method.\n3. Generating reasoning chains (rationale and answer) for each sampled question using Zero-Shot-CoT with LLMs.\n4. Utilizing GPT-3 with 175B parameters for the LLM unless specified otherwise.","contexts":["[2022a], and the Auto-CoT results are averaged\nover three random runs. Overall, Auto-CoT consistently\nmatches or exceeds the performance of the CoT paradigm\nthat requires manual designs of demonstrations. Due to\nthe cost of manual designs, Manual-CoT may design the\nsame demonstrations for multiple datasets (e.g., 5\/6of the\narithmetic datasets). In contrast, Auto-CoT is more \ufb02exible and task-adaptive: every single dataset gets its own\ndemonstrations that are automatically constructed.\n5.3 Visualization of Question Clustering\nFigure 5 visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist\ngeneric patterns, where different patterns may be characterized by questions from different clusters. We present the\nconstructed demonstrations of Auto-CoT in Appendix D.\n#5\nAddSub SingleEq\nCoin Flip \nGSM8K\nLast Letter Concatenation\nMultiArith\nStrategyQA\nAQUA\nCSQA\n SVAMP\nFigure 5: Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.\n8","A Extended analysis for the challenge of Auto-CoT\nA.1 Impact of demonstration elements.\nA demonstration is a triple composed by <question, rationale, answer> as shown in Figure 1. We shuf\ufb02e either of\nthe demonstration components to see how the performance changes. According to the results in Table 5, shuf\ufb02ing\nquestions has the least performance reduction ( 91.7%\u219273.8%). A possible reason for the decent performance is that\nthe model may capture the rationale-answer mapping patterns. The pattern is expected to re\ufb02ect how the intermediate\nreasoning steps can lead to the \ufb01nal answer. This \ufb01nding shows that it is possible to leverage the generated rationales\nby Zero-Shot-CoT because those rationales are often logically correct even though they may lead to wrong answers\n[Kojima et al., 2022].\nTable 5: Impact of different components in the demonstrations.\nMethod Accuracy\nManual-CoT 91.7\nShuf\ufb02e Questions 73.8\nShuf\ufb02e Rationales 43.8\nShuf\ufb02e Answers 17.0\nIn contrast, shuf\ufb02ing either rationales or answers reduces the accuracy signi\ufb01cantly ( 91.7%\u219243.8%\/17.0%). The\nobservation indicates that the rationale-answer consistency is critical. This kind of mismatch actually happens in\nZero-Shot-CoT. An example is shown in Table 6. Using such demonstrations might teach the model illusion\u2014predicting\nanswers without basis.\nTable 6: Mistakes made by rationale-answer inconsistency in the demonstration. The \ufb01rst block shows the demonstration\nused for the test question in the second block.\nQ:For the school bake sale Wendy made pastries. She baked 4 cupcakes and 29 cookies. After the sale she had 24 to take back\nhome. How many pastries did she sell?\nA:Let\u2019s think step by step. Wendy made 4 cupcakes and 29 cookies. She sold some pastries. She had 24 to take back home.\nHow many pastries did she sell?We can set up an equation to solve this problem.","This suggests\nthat the key challenge of Auto-CoT lies in automatically constructing demonstrations with good questions and their\nreasoning chains.\nRecall that Manual-CoT hand-crafts a few (e.g., 8) questions in demonstrations. With similarity-based retrieval methods\nbeing widely adopted for prompting LLMs [Rubin et al., 2022, Su et al., 2022], a promising candidate solution is to\nsample demonstration questions using similarity-based retrieval. We follow the more challenging assumption in CoT\nstudies [Wei et al., 2022a, Kojima et al., 2022] that only a set of test questions are given (without a training dataset).\nFollowing Liu et al. [2022a], we use Sentence-BERT [Reimers and Gurevych, 2019] to encode questions. For each\nquestion qtestin a test dataset, we sample demonstration questions qdemo\ni (i= 1, . . . , k ) from the rest of the questions.\nWe design a Retrieval-Q-CoT method to retrieve the top- k(e.g.,k= 8) similar questions based on cosine similarity.\nTo compare with this similarity-based method, we also test a relatively more diversity-based method: Random-Q-CoT ,\nwhich randomly samples kother test questions for each test question.\nBoth Retrieval-Q-CoT and Random-Q-CoT invoke Zero-Shot-CoT [Kojima et al., 2022] to generate the reasoning chain\ncdemo\ni(rationale and answer) for each sampled question qdemo\ni, as LLMs are decent zero-shot reasoners [Kojima et al.,\n2022]. We use GPT-3 [Brown et al., 2020] with 175B parameters (text-davinci-002) for the LLM unless otherwise stated.\n3"],"ground_truth":"The Auto-CoT method for constructing demonstrations consists of two main stages: question clustering and demonstration sampling. In the question clustering stage, questions are partitioned into clusters using the k-means clustering algorithm. In the demonstration sampling stage, reasoning chains are generated for sampled questions and demonstrations are constructed. The overall procedure is illustrated in Figure 4."}
{"question":"How are genes passed from one generation to the next?","answer":"Genes are passed from one generation to the next through a process called inheritance, where genetic information is transmitted from parents to offspring. This transmission of genes occurs through the combination of genetic material from both parents during reproduction.","contexts":["Published as a conference paper at ICLR 2022\nG.2 R EADING COMPREHENSION\nINPUT\nThere are four ways an individual can acquire Canadian citizenship: by birth on Canadian\nsoil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption.\nAmong them, only citizenship by birth is granted automatically with limited exceptions, while\ncitizenship by descent or adoption is acquired automatically if the speci\ufb01ed conditions have been\nmet. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration,\nRefugees and Citizenship.\nCan we conclude that can i get canadian citizenship if my grandfather was canadian?\nOPTIONS:\n- no\n- yes\nTARGET\nno\nTable 9: Example input and target for Boolean Questions (BoolQ). BoolQ Clark et al. (2019a) asks a\nyes\/no question based on a passage and a question. Of the training set with 9,427 examples, we use\n9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set for\nreporting numbers.\nINPUT\nImagine you are standing in a farm \ufb01eld in central Illinois. The land is so \ufb02at you can see for\nmiles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to\nyourself, it sure is \ufb02at around here. If you drive one hundred miles to the south, the landscape\nchanges. In southern Illinois, there are rolling hills. Why do you think this is? What could have\ncaused these features? There are no big rivers that may have eroded and deposited this material.\nThe ground is capable of supporting grass and trees, so wind erosion would not explain it. To\nanswer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice\nsheet covered much of the Midwest United States. Spring\ufb01eld, Illinois, was covered by over a\nmile of ice. Its hard to imagine a mile thick sheet of ice.","We need to choose\n2 oranges from the basket.\nSo the number of ways are\n10C2. So the answer is (a).There are 10 oranges in the bas-\nket. If you choose 2 oranges, you\nhave 10C2 = 45 ways of choosing\n2 oranges. So the answer is (a).\n[CommonsenseQA] The man\nlaid on the soft moss and looked\nup at the trees, where was the\nman? (a) niagra falls (b) forest\n(c) waterfall (d) ground (e) treeThe answer must be something\nthat is soft and mossy. Of the\nabove choices, only ground is\nsoft and mossy. So the answer\nis (d).The answer must be\nsomewhere where moss\nis found. Of the above\nchoices, only forests are\nknown for having moss.\nSo the answer is (b).The answer must be a place where\nthe man is laying down and look-\ning up at trees. Of the above\nchoices, only forest is a place\nwhere people lay down and look\nat trees. So the answer is (b).\n[ARC] Which of these will most\nlikely harm a habitat? (a) plant-\ning trees. (b) water pollution. (c)\nrainfall. (d) sunlight.Planting trees will not harm a\nhabitat. So the answer is (a).The answer is (b), water\npollution, because it will\nmost likely harm the habi-\ntat. The other answers are\nnot likely to harm the habi-\ntat.The answer is (b), water pollution.\nTrees are good for a habitat, rain-\nfall is necessary for a habitat, and\nsunlight is necessary for a habitat.\nSo the answer is (b).\n[ARC-challenge] Which pro-\ncess in the carbon cycle takes the\nlongest to complete? (a) emis-\nsion of waste. (b) respiration in\nanimals. (c) photosynthesis in\nplants. (d) formation of fossil fu-\nels.The process of photosynthesis\nin plants takes the longest to\ncomplete. So the answer is (c).The formation of fossil fu-\nels takes the longest time\nto complete.","Therefore, among A through C, the answer is A.\n\u0017(GT : B)\nLast Letters Q: Take the last letters of\neach words in \"Vinny Landon\nMiguel Caitlyn\" and concate-\nnate them.A: Let\u2019s think step by step.\nThe last letter of \"Vinny\" is \"y\".\nThe last letter of \"Landon\" is \"n\".\nThe last letter of \"Miguel\" is \"l\".\nThe last letter of \"Caitlyn\" is \"n\".\nSo, the \ufb01nal answer would be \"ynln\".\nTherefore, the answer is \"ynln\". \u2713\nQ: Take the last letters of each\nwords in \"Lamont Frederick\nLance Esperanza\" and con-\ncatenate them.A: Let\u2019s think step by step.\nThe last letter of \"Lamont\" is \"t\".\nThe last letter of \"Frederick\" is \"k\".\nThe last letter of \"Lance\" is \"e\".\nThe last letter of \"Esperanza\" is \"a\".\nSo the \ufb01nal answer would be \"tkaea\".\nTherefore, the answer is \"tkaea\". \u0017(GT : tkea)\nCoin Flip Q: A coin is heads up. Jeff\ndoes not \ufb02ip the coin. Jen\n\ufb02ips the coin. Giselle \ufb02ips the\ncoin. Noel does not \ufb02ip the\ncoin. Is the coin still heads up?\nNote that \"\ufb02ip\" here means\n\"reverse\".A: Let\u2019s think step by step.\nJeff does not \ufb02ip the coin, so the coin remains\nheads up.\nJen \ufb02ips the coin, so the coin is now tails up.\nGiselle \ufb02ips the coin, so the coin is now heads up\nagain.\nNoel does not \ufb02ip the coin, so the coin remains\nheads up.\nTherefore, the \ufb01nal answer is that the coin is heads\nup.\nTherefore, the answer (Yes or No) is Yes. \u2713\nQ: A coin is heads up. Caro-\nline \ufb02ips the coin. Demetrius\n\ufb02ips the coin."],"ground_truth":"Genes are passed from parent to offspring."}
