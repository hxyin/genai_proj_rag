{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:08:29.240223Z",
     "start_time": "2024-11-17T03:08:29.232812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google.auth import load_credentials_from_file\n",
    "import pandas as pd\n",
    "# Replace this with your own GCP project IAM credential file\n",
    "credentials, project_id = load_credentials_from_file(\n",
    "    \"/Users/Apple/secrets/genai-441923-47c3e249f8b8.json\"\n",
    ")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/Apple/secrets/genai-441923-47c3e249f8b8.json\""
   ],
   "id": "3597dde478bb7b89",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-17T03:08:30.230723Z",
     "start_time": "2024-11-17T03:08:29.945041Z"
    }
   },
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "import fitz\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "import hashlib\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"gemini-1.0-pro\",\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=8192,\n",
    "    max_workers=2,\n",
    "    \n",
    ")\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:04:57.849132Z",
     "start_time": "2024-11-17T03:04:52.755906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 使用模型生成文本\n",
    "prompt = \"请简要介绍一下人工智能的发展历史。\"\n",
    "response = llm(prompt)\n",
    "print(response)\n"
   ],
   "id": "37a91bae339b3833",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 人工智能发展历史简述\n",
      "\n",
      "人工智能 (AI) 的发展历史可以追溯到 20 世纪 50 年代，经历了多个阶段，取得了显著的进步。\n",
      "\n",
      "**1. 萌芽阶段 (1950s-1960s):**\n",
      "\n",
      "* 1950 年，图灵测试提出，为人工智能研究奠定了基础。\n",
      "* 1956 年，达特茅斯会议上首次提出“人工智能”的概念。\n",
      "* 1960s，专家系统开始出现，用于解决特定领域的问题。\n",
      "\n",
      "**2. 发展阶段 (1970s-1980s):**\n",
      "\n",
      "* 1970s，人工智能研究陷入瓶颈，专家系统发展受限。\n",
      "* 1980s，机器学习开始兴起，为人工智能带来了新的突破。\n",
      "\n",
      "**3. 复兴阶段 (1990s-2010s):**\n",
      "\n",
      "* 1990s，深度学习技术取得突破，推动人工智能快速发展。\n",
      "* 2000s，人工智能应用开始普及，例如图像识别、语音识别等。\n",
      "* 2010s，人工智能在各个领域取得重大进展，例如自动驾驶、医疗诊断等。\n",
      "\n",
      "**4. 突破阶段 (2020s-至今):**\n",
      "\n",
      "* 2020s，人工智能模型规模不断扩大，性能大幅提升。\n",
      "* 人工智能应用范围不断扩展，例如生成式 AI、元宇宙等。\n",
      "* 人工智能伦理问题受到关注，引发广泛讨论。\n",
      "\n",
      "**总结:** 人工智能发展至今，取得了巨大的进步，并对各个领域产生了深刻的影响。未来，人工智能将继续发展，并为人类社会带来更多机遇和挑战。\n",
      "\n",
      "**参考资料:**\n",
      "\n",
      "* https://en.wikipedia.org/wiki/History_of_artificial_intelligence\n",
      "* https://www.britannica.com/technology/artificial-intelligence/History-of-artificial-intelligence\n",
      "* https://www.ibm.com/topics/artificial-intelligence\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T02:36:30.299879Z",
     "start_time": "2024-11-17T02:36:29.705230Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "494be5ef33bc5975",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T02:28:50.995636Z",
     "start_time": "2024-11-17T02:28:50.991072Z"
    }
   },
   "cell_type": "code",
   "source": "llm",
   "id": "8232ef8474afc23c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAI(client=<vertexai.generative_models.GenerativeModel object at 0x2b8e89ed0>, project='genai-441923', model_name='gemini-1.5-flash', client_options=ClientOptions: {'api_endpoint': 'us-central1-aiplatform.googleapis.com', 'client_cert_source': None, 'client_encrypted_cert_source': None, 'quota_project_id': None, 'credentials_file': None, 'scopes': None, 'api_key': None, 'api_audience': None, 'universe_domain': None}, default_metadata=(), client_preview=<vertexai.generative_models.GenerativeModel object at 0x2b8e89d80>, temperature=0.3, max_output_tokens=8192, model_family=<GoogleModelFamily.GEMINI_ADVANCED: '2'>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:08:38.541741Z",
     "start_time": "2024-11-17T03:08:38.526790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_directory_hash(directory_path: str) -> str:\n",
    "    \"\"\"Calculate a hash of the directory contents to detect changes.\"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text = page.get_text(\"text\", flags=fitz.TEXT_DEHYPHENATE | fitz.TEXT_PRESERVE_WHITESPACE)\n",
    "            if text.strip():\n",
    "                metadata = {\n",
    "                    \"source\": pdf_path,\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"total_pages\": len(pdf_document)\n",
    "                }\n",
    "                documents.append(Document(page_content=text, metadata=metadata))\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    return documents\n",
    "\n",
    "def load_pdfs_from_directory(directory_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            documents.extend(extract_text_from_pdf(file_path))\n",
    "    return documents\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, data_dir: str = \"./data\", persist_dir: str = \"./chroma_db\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.persist_dir = persist_dir\n",
    "        self.vectorstore = None\n",
    "        self.rag_chain = None\n",
    "        self.retrieve_docs = []  # 添加检索文档记录列表\n",
    "        self.initialize_pipeline()\n",
    "        \n",
    "    def initialize_pipeline(self):\n",
    "        # Create persist_dir if it doesn't exist\n",
    "        os.makedirs(self.persist_dir, exist_ok=True)\n",
    "\n",
    "        # Check if we need to update embeddings\n",
    "        should_update = self._should_update_embeddings()\n",
    "        print(f'Should update embeddings: {should_update}')\n",
    "        if not should_update and os.path.exists(self.persist_dir):\n",
    "            # Try to load existing vectorstore\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.persist_dir,\n",
    "                embedding_function=embedding_model\n",
    "            )\n",
    "        else:\n",
    "            # Create new embeddings\n",
    "            self._create_new_embeddings()\n",
    "\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            # 存储检索到的文档内容\n",
    "            doc_contents = [doc.page_content for doc in docs]\n",
    "            self.retrieve_docs.append(doc_contents)\n",
    "            return \"\\n\\n\".join(doc_contents)\n",
    "\n",
    "        self.rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def _should_update_embeddings(self) -> bool:\n",
    "        \"\"\"Check if the source documents have changed.\"\"\"\n",
    "        if not os.path.exists(self.persist_dir):\n",
    "            return True\n",
    "            \n",
    "        current_hash = get_directory_hash(self.data_dir)\n",
    "        hash_file = os.path.join(self.persist_dir, \"directory_hash.txt\")\n",
    "        \n",
    "        if not os.path.exists(hash_file):\n",
    "            return True\n",
    "        \n",
    "        with open(hash_file, 'r') as f:\n",
    "            stored_hash = f.read().strip()\n",
    "        \n",
    "        return current_hash != stored_hash\n",
    "\n",
    "    def _create_new_embeddings(self):\n",
    "        \"\"\"Create new embeddings from the documents.\"\"\"\n",
    "        docs = load_pdfs_from_directory(self.data_dir)\n",
    "        if not docs:\n",
    "            raise ValueError(f\"No documents were loaded from {self.data_dir}\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # Create new vectorstore with persistence enabled\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "\n",
    "        # Save directory hash\n",
    "        current_hash = get_directory_hash(self.data_dir)\n",
    "        with open(os.path.join(self.persist_dir, \"directory_hash.txt\"), 'w') as f:\n",
    "            f.write(current_hash)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents for a query.\"\"\"\n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        # 存储检索到的文档内容\n",
    "        self.retrieve_docs.append([doc.page_content for doc in docs])\n",
    "        return docs\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Query the RAG pipeline.\"\"\"\n",
    "        return self.rag_chain.invoke(question)\n",
    "\n",
    "    def get_retrieve_history(self) -> List[List[str]]:\n",
    "        \"\"\"获取所有检索历史记录\"\"\"\n",
    "        return self.retrieve_docs\n",
    "\n",
    "    def clear_retrieve_history(self):\n",
    "        \"\"\"清空检索历史记录\"\"\"\n",
    "        self.retrieve_docs = []\n",
    "    \n",
    "    def retrieve_and_query(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant documents and query the RAG pipeline.\"\"\"\n",
    "        self.retrieve(query)\n",
    "        query_res = self.query(query), \n",
    "        retrieve_res = self.get_retrieve_history()[0] if self.get_retrieve_history() else []\n",
    "        self.clear_retrieve_history()\n",
    "        return query_res, retrieve_res\n",
    "# Example usage:\n",
    "# rag = RAGPipeline(data_dir=\"./data\", persist_dir=\"./chroma_db\")\n",
    "# \n",
    "# # To retrieve documents:\n",
    "# relevant_docs = rag.retrieve(\"your query here\")\n",
    "# \n",
    "# # To get an answer:\n",
    "# answer = rag.query(\"your question here\")\n",
    "#\n",
    "# # To get retrieval history:\n",
    "# history = rag.get_retrieve_history()\n",
    "#\n",
    "# # To clear retrieval history:\n",
    "# rag.clear_retrieve_history()"
   ],
   "id": "f749aa0ee9fe9fbd",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:08:46.605714Z",
     "start_time": "2024-11-17T03:08:45.955864Z"
    }
   },
   "cell_type": "code",
   "source": "rag = RAGPipeline(data_dir=\"./data\", persist_dir=\"./chroma_db\")\n",
   "id": "ea4d206e9c4c95d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should update embeddings: False\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ans, retrieve_history = rag.retrieve_and_query(\"How does the number of datasets and templates affect the performance of instruction tuning in the FLAN model?\")",
   "id": "198d809548b82a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:48:16.531865Z",
     "start_time": "2024-11-17T01:48:16.528444Z"
    }
   },
   "cell_type": "code",
   "source": "ans",
   "id": "282f82c98629ce91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context does not directly address the impact of the number of datasets and templates on instruction tuning performance in the FLAN model. However, it does mention that using more datasets per cluster improved performance by almost 10% on average. Conversely, using more templates per dataset had a negligible effect on performance, suggesting that the model's performance is more sensitive to the number of datasets than the number of templates. \\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T01:48:21.541860Z",
     "start_time": "2024-11-17T01:48:21.538779Z"
    }
   },
   "cell_type": "code",
   "source": "retrieve_history",
   "id": "ea36fc351fe22c2c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'tasks). In addition, we simultaneously explore the role of the number of instruction templates per\\ndataset; as mentioned in §2.1, for each dataset we manually composed ten instructional templates for\\ninstruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\\nFigure 11 shows these results. Using more datasets per cluster improved performance by almost\\n10% on average across the three held-out clusters. Using more templates per dataset, however,\\nhad a comparatively negligible effect on performance when there was one task per cluster, which\\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\\noriginal motivation that composing ten templates per task would mitigate overfitting to any particular\\ntemplate. This results serves to underscore, however, the unpredictability of finetuning large language\\nmodels, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.'],\n",
       " ['better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'better understand the output format. In addition, for all task clusters, standard deviation among\\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\\nNLI\\nRead. Comp. Closed-Book QA Commonsense\\nCoreference\\nTranslation\\nZero-shot FLAN\\nFew-shot FLAN\\nPerformance\\n20\\n40\\n60\\n80\\n54.7 59.3\\n59.6 60.0\\n53.7\\nStruct to text\\n57.2\\n31.0 33.0\\n80.0 80.8\\n63.8 67.4\\n39.2\\n49.4\\nTask Cluster:\\n# datasets:\\n7\\n5\\n3\\n4\\n2\\n3\\n4\\nFigure 9:\\nAdding few-shot exemplars to FLAN is a complementary method for improving the\\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\\ntemplates, averaged at the dataset level for each task cluster.\\n4.5\\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\\n32 training \\nexamples\\nFull training \\nset\\n100\\n0\\n50\\n75\\n25\\nPerformance after \\nprompt tuning\\nInstruction-tuned model\\nUntuned model\\n63.8\\n78.1\\n79.1\\n87.4\\nFigure 10:\\nInstruction-tuned\\nmodels respond better to continuous inputs from prompt tuning.\\nWhen prompt tuning on a given',\n",
       "  'tasks). In addition, we simultaneously explore the role of the number of instruction templates per\\ndataset; as mentioned in §2.1, for each dataset we manually composed ten instructional templates for\\ninstruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\\nFigure 11 shows these results. Using more datasets per cluster improved performance by almost\\n10% on average across the three held-out clusters. Using more templates per dataset, however,\\nhad a comparatively negligible effect on performance when there was one task per cluster, which\\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\\noriginal motivation that composing ten templates per task would mitigate overfitting to any particular\\ntemplate. This results serves to underscore, however, the unpredictability of finetuning large language\\nmodels, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:06:58.331722Z",
     "start_time": "2024-11-17T03:06:58.254055Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset = pd.read_json('./data/eval_dataset_1.json')",
   "id": "b5a3090b31aa693",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:06:59.806802Z",
     "start_time": "2024-11-17T03:06:59.803618Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.columns = ['user_input', 'reference', 'response', 'retrieved_contexts']",
   "id": "931f990f61d123da",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:10:01.198637Z",
     "start_time": "2024-11-17T03:10:01.174815Z"
    }
   },
   "cell_type": "code",
   "source": "import time",
   "id": "73c42ae3e776e74f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:13:09.549563Z",
     "start_time": "2024-11-17T03:12:17.909888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(test_dataset)):\n",
    "    res, retrieve_data = rag.retrieve_and_query(test_dataset['user_input'][i])\n",
    "    # Use .loc to set values\n",
    "    test_dataset.loc[i, 'response'] = res[0]\n",
    "    test_dataset.loc[i, 'retrieved_contexts'] = retrieve_data\n",
    "    time.sleep(1)"
   ],
   "id": "6d2d7b7dbe9137d2",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:07.535790Z",
     "start_time": "2024-11-17T03:18:07.528095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas import EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_pandas(test_dataset[:4])"
   ],
   "id": "fc5ddb741b14301c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:08.566093Z",
     "start_time": "2024-11-17T03:18:08.560428Z"
    }
   },
   "cell_type": "code",
   "source": "eval_dataset.to_pandas()",
   "id": "246cb7f026654040",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How does instruction tuning affect the zero-sh...   \n",
       "1  What is the Zero-shot-CoT method and how does ...   \n",
       "2  How does prompt tuning affect model performanc...   \n",
       "3  What is the purpose of instruction tuning in l...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [models finetuned on dataset name only, we rep...   \n",
       "1  [Large Language Models are Zero-Shot Reasoners...   \n",
       "2  [prompt tuning\\nInstruction-tuned model\\nUntun...   \n",
       "3  [LM\\n(C) Instruction tuning (FLAN)\\nInstructio...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Instruction tuning significantly improves zero...   \n",
       "1  I'm sorry, but the provided context does not c...   \n",
       "2  Prompt tuning can improve the performance of N...   \n",
       "3  Instruction tuning aims to improve the ability...   \n",
       "\n",
       "                                           reference  \n",
       "0  For larger models on the order of 100B paramet...  \n",
       "1  Zero-shot-CoT is a zero-shot template-based pr...  \n",
       "2  Prompt tuning improves model performance in im...  \n",
       "3  The purpose of instruction tuning in language ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does instruction tuning affect the zero-sh...</td>\n",
       "      <td>[models finetuned on dataset name only, we rep...</td>\n",
       "      <td>Instruction tuning significantly improves zero...</td>\n",
       "      <td>For larger models on the order of 100B paramet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Zero-shot-CoT method and how does ...</td>\n",
       "      <td>[Large Language Models are Zero-Shot Reasoners...</td>\n",
       "      <td>I'm sorry, but the provided context does not c...</td>\n",
       "      <td>Zero-shot-CoT is a zero-shot template-based pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does prompt tuning affect model performanc...</td>\n",
       "      <td>[prompt tuning\\nInstruction-tuned model\\nUntun...</td>\n",
       "      <td>Prompt tuning can improve the performance of N...</td>\n",
       "      <td>Prompt tuning improves model performance in im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of instruction tuning in l...</td>\n",
       "      <td>[LM\\n(C) Instruction tuning (FLAN)\\nInstructio...</td>\n",
       "      <td>Instruction tuning aims to improve the ability...</td>\n",
       "      <td>The purpose of instruction tuning in language ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:13.965563Z",
     "start_time": "2024-11-17T03:18:13.962956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# increasing max_workers to 64 and timeout to 60 seconds\n",
    "\n",
    "my_run_config = RunConfig(max_workers=1, timeout=60)"
   ],
   "id": "92140cd937eb624c",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:15.227788Z",
     "start_time": "2024-11-17T03:18:15.224512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm, run_config=my_run_config)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embedding_model, run_config=my_run_config)"
   ],
   "id": "da8a24bec687e1d6",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:15.728690Z",
     "start_time": "2024-11-17T03:18:15.726617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ],
   "id": "10b4b9a6b1dffc02",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:17.235313Z",
     "start_time": "2024-11-17T03:18:17.175565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ],
   "id": "9bf2202080bcfd91",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:18.346944Z",
     "start_time": "2024-11-17T03:18:18.343776Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator_llm",
   "id": "12e30304bf3deb19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False, seed=42), multiple_completion_supported=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:18:19.185701Z",
     "start_time": "2024-11-17T03:18:19.183934Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d2cb751026d108a5",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:19:23.674974Z",
     "start_time": "2024-11-17T03:18:20.412585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics, run_config=my_run_config)"
   ],
   "id": "758d037a378abe6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "674e99826c0c46e59c3a53bb63e42e58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:19:53.886832Z",
     "start_time": "2024-11-17T03:19:53.869506Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b3e1033a6db9e165",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:19:54.045278Z",
     "start_time": "2024-11-17T03:19:54.042093Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "1ac7b31369ac8e1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.5833, 'factual_correctness': 0.2850, 'faithfulness': 0.2750, 'semantic_similarity': 0.9050}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T03:19:54.904191Z",
     "start_time": "2024-11-17T03:19:54.895734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = results.to_pandas()\n",
    "df.head()"
   ],
   "id": "fba3317f15f508a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How does instruction tuning affect the zero-sh...   \n",
       "1  What is the Zero-shot-CoT method and how does ...   \n",
       "2  How does prompt tuning affect model performanc...   \n",
       "3  What is the purpose of instruction tuning in l...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [models finetuned on dataset name only, we rep...   \n",
       "1  [Large Language Models are Zero-Shot Reasoners...   \n",
       "2  [prompt tuning\\nInstruction-tuned model\\nUntun...   \n",
       "3  [LM\\n(C) Instruction tuning (FLAN)\\nInstructio...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Instruction tuning significantly improves zero...   \n",
       "1  I'm sorry, but the provided context does not c...   \n",
       "2  Prompt tuning can improve the performance of N...   \n",
       "3  Instruction tuning aims to improve the ability...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  For larger models on the order of 100B paramet...        0.000000   \n",
       "1  Zero-shot-CoT is a zero-shot template-based pr...        1.000000   \n",
       "2  Prompt tuning improves model performance in im...        0.333333   \n",
       "3  The purpose of instruction tuning in language ...        1.000000   \n",
       "\n",
       "   factual_correctness  faithfulness  semantic_similarity  \n",
       "0                 0.00           0.5             0.887301  \n",
       "1                 0.22           0.0             0.848312  \n",
       "2                 0.00           0.0             0.917471  \n",
       "3                 0.92           0.6             0.966880  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does instruction tuning affect the zero-sh...</td>\n",
       "      <td>[models finetuned on dataset name only, we rep...</td>\n",
       "      <td>Instruction tuning significantly improves zero...</td>\n",
       "      <td>For larger models on the order of 100B paramet...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.887301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Zero-shot-CoT method and how does ...</td>\n",
       "      <td>[Large Language Models are Zero-Shot Reasoners...</td>\n",
       "      <td>I'm sorry, but the provided context does not c...</td>\n",
       "      <td>Zero-shot-CoT is a zero-shot template-based pr...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.848312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does prompt tuning affect model performanc...</td>\n",
       "      <td>[prompt tuning\\nInstruction-tuned model\\nUntun...</td>\n",
       "      <td>Prompt tuning can improve the performance of N...</td>\n",
       "      <td>Prompt tuning improves model performance in im...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.917471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of instruction tuning in l...</td>\n",
       "      <td>[LM\\n(C) Instruction tuning (FLAN)\\nInstructio...</td>\n",
       "      <td>Instruction tuning aims to improve the ability...</td>\n",
       "      <td>The purpose of instruction tuning in language ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.966880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3eeeacbd2db26ce8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
