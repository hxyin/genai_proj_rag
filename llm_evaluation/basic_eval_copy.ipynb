{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础库\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from os import stat\n",
    "import chromadb\n",
    "\n",
    "# Google Cloud 认证\n",
    "from google.auth import load_credentials_from_file\n",
    "\n",
    "# Langchain 相关\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Ragas 相关\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas import EvaluationDataset # type: ignore\n",
    "\n",
    "# 类型提示\n",
    "from typing import cast as t\n",
    "from langchain_core.outputs import LLMResult, ChatGeneration\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from llm_evaluation.evaluate_fact_beta import FactualCorrectnessReviseBeta\n",
    "\n",
    "# PDF处理\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载认证信息（换成自己的）\n",
    "credentials, project_id = load_credentials_from_file(\n",
    "    \"./unified-sensor-437622-t3-1c0bfcf1fd30.json\"\n",
    ")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./unified-sensor-437622-t3-1c0bfcf1fd30.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" # 换成自己的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHybridRetriever:\n",
    "    \"\"\"Custom hybrid retriever that combines vector and BM25 retrievers\"\"\"\n",
    "    def __init__(self, vector_retriever, bm25_retriever, weight_vector=0.5, k=4):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.weight_vector = weight_vector\n",
    "        self.k = k\n",
    "        \n",
    "    def invoke(self, query: str) -> List[Document]:\n",
    "        \"\"\"Get documents from both retrievers and combine results\"\"\"\n",
    "        try:\n",
    "            # Get more documents from each retriever for better reranking\n",
    "            vector_docs = self.vector_retriever.invoke(query)\n",
    "            bm25_docs = self.bm25_retriever.invoke(query)\n",
    "            \n",
    "            print(f\"Vector retriever returned {len(vector_docs)} docs\")\n",
    "            print(f\"BM25 retriever returned {len(bm25_docs)} docs\")\n",
    "            \n",
    "            # Create document map for scoring\n",
    "            doc_scores = {}\n",
    "            \n",
    "            # Normalize scores for vector docs\n",
    "            max_vector_score = max((doc.metadata.get('score', 0.0) for doc in vector_docs), default=1.0)\n",
    "            for doc in vector_docs:\n",
    "                content = doc.page_content\n",
    "                score = doc.metadata.get('score', 0.5)\n",
    "                # Normalize score\n",
    "                normalized_score = score / max_vector_score if max_vector_score > 0 else score\n",
    "                doc_scores[content] = {\n",
    "                    'vector_score': normalized_score,\n",
    "                    'bm25_score': 0.0,\n",
    "                    'doc': doc\n",
    "                }\n",
    "            \n",
    "            # Normalize scores for BM25 docs\n",
    "            max_bm25_score = max((doc.metadata.get('score', 0.0) for doc in bm25_docs), default=1.0)\n",
    "            for doc in bm25_docs:\n",
    "                content = doc.page_content\n",
    "                score = doc.metadata.get('score', 0.5)\n",
    "                # Normalize score\n",
    "                normalized_score = score / max_bm25_score if max_bm25_score > 0 else score\n",
    "                \n",
    "                if content in doc_scores:\n",
    "                    doc_scores[content]['bm25_score'] = normalized_score\n",
    "                else:\n",
    "                    doc_scores[content] = {\n",
    "                        'vector_score': 0.0,\n",
    "                        'bm25_score': normalized_score,\n",
    "                        'doc': doc\n",
    "                    }\n",
    "            \n",
    "            # Calculate combined scores\n",
    "            final_scores = []\n",
    "            for content, scores in doc_scores.items():\n",
    "                combined_score = (\n",
    "                    self.weight_vector * scores['vector_score'] +\n",
    "                    (1 - self.weight_vector) * scores['bm25_score']\n",
    "                )\n",
    "                final_scores.append((combined_score, scores['doc']))\n",
    "            \n",
    "            # Sort by combined score and get top k\n",
    "            final_docs = [\n",
    "                doc for _, doc in sorted(final_scores, key=lambda x: x[0], reverse=True)[:self.k]\n",
    "            ]\n",
    "            \n",
    "            # Add combined scores to metadata\n",
    "            for i, doc in enumerate(final_docs):\n",
    "                doc.metadata['hybrid_score'] = final_scores[i][0]\n",
    "            \n",
    "            print(f\"Hybrid retriever returning {len(final_docs)} docs\")\n",
    "            if final_docs:\n",
    "                print(f\"Top document scores: {[doc.metadata.get('hybrid_score', 0.0) for doc in final_docs[:3]]}\")\n",
    "            \n",
    "            return final_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid retriever: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"从PDF文件中提取文本\"\"\"\n",
    "    \"\"\"\n",
    "    Extract text content from PDF files\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects containing page content and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text = page.get_text(\"text\")\n",
    "            if text.strip():\n",
    "                metadata = {\n",
    "                    \"source\": pdf_path,\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"total_pages\": len(pdf_document)\n",
    "                }\n",
    "                documents.append(Document(page_content=text, metadata=metadata))\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_directory(directory_path: str) -> List[Document]:\n",
    "    \"\"\"从目录加载PDF文档\"\"\"\n",
    "    \"\"\"\n",
    "    Load all PDF documents from a specified directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing PDF files\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects from all PDFs in directory\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with tqdm(os.listdir(directory_path), desc=\"Loading PDFs\") as pbar:\n",
    "        for filename in pbar:\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                pbar.set_postfix(file=filename)\n",
    "                documents.extend(extract_text_from_pdf(file_path))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_is_finished_parser(response: LLMResult):\n",
    "    \"\"\"自定义完成状态解析器\"\"\"\n",
    "    is_finished_list = []\n",
    "    for g in response.flatten():\n",
    "        resp = g.generations[0][0]\n",
    "        if resp.generation_info is not None:\n",
    "            if resp.generation_info.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp.generation_info.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "        elif (\n",
    "            isinstance(resp, ChatGeneration)\n",
    "            and t.cast(ChatGeneration, resp).message is not None\n",
    "        ):\n",
    "            resp_message: BaseMessage = t.cast(ChatGeneration, resp).message\n",
    "            if resp_message.response_metadata.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp_message.response_metadata.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "        else:\n",
    "            is_finished_list.append(True)\n",
    "    return all(is_finished_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    def __init__(self, data_dir: str, persist_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.persist_dir = persist_dir\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = VertexAIEmbeddings(\n",
    "            model_name=\"textembedding-gecko\",\n",
    "            project=project_id,\n",
    "            credentials=credentials,\n",
    "            location=\"us-central1\"\n",
    "        )\n",
    "        \n",
    "        # Initialize LLMs with custom parser\n",
    "        self.eval_llm = VertexAI(\n",
    "            model_name=\"gemini-1.0-pro\",\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=8192,\n",
    "            project=project_id,\n",
    "            credentials=credentials,\n",
    "            is_finished_parser=custom_is_finished_parser  \n",
    "        )\n",
    "        \n",
    "        self.exp_llm = VertexAI(\n",
    "            model_name=\"gemini-1.0-pro\",\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=8192,\n",
    "            project=project_id,\n",
    "            credentials=credentials,\n",
    "            is_finished_parser=custom_is_finished_parser \n",
    "        )\n",
    "        \n",
    "        print(\"Loading documents...\")\n",
    "        self.documents = load_pdfs_from_directory(data_dir)\n",
    "        print(f\"Loaded {len(self.documents)} documents\")\n",
    "        \n",
    "        print(\"Initializing retrievers...\")\n",
    "        self.vector_retriever = self._init_vector_retriever()\n",
    "        self.bm25_retriever = self._init_bm25_retriever()\n",
    "        self.hybrid_retriever = self._init_hybrid_retriever()\n",
    "        print(\"Initialization complete\")\n",
    "\n",
    "    def _init_vector_retriever(self):\n",
    "        \"\"\"Initialize vector retriever with improved configuration\"\"\"\n",
    "        print(\"Initializing vector retriever...\")\n",
    "        try:\n",
    "            # 清除现有的向量存储\n",
    "            if os.path.exists(f\"{self.persist_dir}/vector\"):\n",
    "                import shutil\n",
    "                shutil.rmtree(f\"{self.persist_dir}/vector\")\n",
    "                print(\"Cleared existing vector store\")\n",
    "            \n",
    "            # 创建新的向量存储\n",
    "            print(\"Creating new vector store...\")\n",
    "            \n",
    "            # 确保文档内容不为空\n",
    "            valid_docs = [doc for doc in self.documents if doc.page_content.strip()]\n",
    "            print(f\"Found {len(valid_docs)} valid documents\")\n",
    "            \n",
    "            # 创建向量存储\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents=valid_docs,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=f\"{self.persist_dir}/vector\"\n",
    "            )\n",
    "            print(f\"Created vector store with {vectorstore._collection.count()} documents\")\n",
    "            \n",
    "            # 创建检索器\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_kwargs={\n",
    "                    \"k\": 4,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return retriever\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector retriever: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _init_bm25_retriever(self):\n",
    "        \"\"\"Initialize BM25 retriever with improved configuration\"\"\"\n",
    "        print(\"Initializing BM25 retriever...\")\n",
    "        try:\n",
    "            # 预处理文档\n",
    "            processed_docs = []\n",
    "            for doc in self.documents:\n",
    "                if doc.page_content.strip():  # 确保文档不为空\n",
    "                    # 保留原始元数据\n",
    "                    metadata = doc.metadata.copy() if hasattr(doc, 'metadata') else {}\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=doc.page_content,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "            \n",
    "            print(f\"Processing {len(processed_docs)} documents for BM25\")\n",
    "            \n",
    "            # 创建BM25检索器\n",
    "            retriever = BM25Retriever.from_documents(\n",
    "                documents=processed_docs,\n",
    "                preprocess_func=lambda text: text.split(),  # 简单的分词\n",
    "                k=4\n",
    "            )\n",
    "            \n",
    "            return retriever\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing BM25 retriever: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _init_hybrid_retriever(self):\n",
    "        \"\"\"Initialize hybrid retriever with optimized weights\"\"\"\n",
    "        print(\"Initializing hybrid retriever...\")\n",
    "        return CustomHybridRetriever(\n",
    "            self.vector_retriever,\n",
    "            self.bm25_retriever,\n",
    "            weight_vector=0.3,\n",
    "            k=4\n",
    "        )\n",
    "    \n",
    "    def get_vector_retriever(self):\n",
    "        \"\"\"Get vector retriever\"\"\"\n",
    "        return self.vector_retriever\n",
    "    \n",
    "    def get_bm25_retriever(self):\n",
    "        \"\"\"Get BM25 retriever\"\"\"\n",
    "        return self.bm25_retriever\n",
    "    \n",
    "    def get_hybrid_retriever(self):\n",
    "        \"\"\"Get hybrid retriever\"\"\"\n",
    "        return self.hybrid_retriever\n",
    "\n",
    "    def _generate_response(self, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"Generate answer based on retrieved documents\"\"\"\n",
    "        try:\n",
    "            # 验证和处理检索到的文档\n",
    "            valid_docs = []\n",
    "            for doc in retrieved_docs:\n",
    "                if isinstance(doc, Document) and doc.page_content.strip():\n",
    "                    valid_docs.append(doc)\n",
    "            \n",
    "            if not valid_docs:\n",
    "                print(\"Warning: No valid documents for response generation\")\n",
    "                return \"\"\n",
    "            \n",
    "            # 构建上下文\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in valid_docs])\n",
    "            prompt = f\"Based on the following context, please answer the question:\\n\\nContext: {context}\\n\\nQuestion: {query}\"\n",
    "            \n",
    "            # 生成响应\n",
    "            response = self.exp_llm.invoke(prompt)\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {str(e)}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def _process_query(self, retriever, query: str) -> List[Document]:\n",
    "        \"\"\"Process a single query with improved logging\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nProcessing query: {query[:100]}...\")\n",
    "            \n",
    "            # 获取检索结果\n",
    "            retrieved_docs = retriever.invoke(query)\n",
    "            \n",
    "            # 详细记录结果\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "            if retrieved_docs:\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"\\nDocument {i+1}:\")\n",
    "                    print(f\"Length: {len(doc.page_content)}\")\n",
    "                    print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "                    if hasattr(doc, 'metadata'):\n",
    "                        print(f\"Metadata: {doc.metadata}\")\n",
    "                    if hasattr(doc, 'similarity'):\n",
    "                        print(f\"Similarity score: {doc.similarity}\")\n",
    "            else:\n",
    "                print(\"Warning: No documents retrieved\")\n",
    "                \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def evaluate_retriever(self, retriever_name: str, test_dataset: pd.DataFrame, n_runs: int = 3):\n",
    "        \"\"\"\n",
    "        Evaluate retriever performance with multiple runs\n",
    "        \n",
    "        Args:\n",
    "            retriever_name: Name of the retriever to evaluate\n",
    "            test_dataset: Test dataset\n",
    "            n_runs: Number of evaluation runs (default: 3)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get retriever\n",
    "            if retriever_name == \"hybrid\":\n",
    "                retriever = self.get_hybrid_retriever()\n",
    "            elif retriever_name == \"vector\":\n",
    "                retriever = self.get_vector_retriever()\n",
    "            elif retriever_name == \"bm25\":\n",
    "                retriever = self.get_bm25_retriever()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown retriever type: {retriever_name}\")\n",
    "                \n",
    "            if retriever is None:\n",
    "                print(f\"Failed to initialize {retriever_name} retriever\")\n",
    "                return None\n",
    "            \n",
    "            # Store results from multiple runs\n",
    "            all_run_scores = []\n",
    "            \n",
    "            print(f\"\\nRunning {n_runs} evaluations for {retriever_name}...\")\n",
    "            \n",
    "            for run in range(n_runs):\n",
    "                print(f\"\\nStarting run {run + 1}/{n_runs}\")\n",
    "                \n",
    "                # Create evaluation dataset\n",
    "                valid_results = pd.DataFrame()\n",
    "                valid_results['user_input'] = test_dataset['question']\n",
    "                valid_results['reference'] = test_dataset['ground_truth']\n",
    "                valid_results['response'] = ''\n",
    "                valid_results['retrieved_contexts'] = [[]] * len(test_dataset)\n",
    "\n",
    "                print(f\"Processing {len(valid_results)} valid queries\")\n",
    "                \n",
    "                # Process each query\n",
    "                for idx, row in valid_results.iterrows():\n",
    "                    try:\n",
    "                        query = row['user_input']\n",
    "                        if pd.isna(query):\n",
    "                            continue\n",
    "                            \n",
    "                        # Retrieve documents with detailed logging\n",
    "                        print(f\"\\nProcessing query {idx + 1}/{len(valid_results)}\")\n",
    "                        print(f\"Query: {query[:100]}...\")\n",
    "                        \n",
    "                        if retriever_name == \"vector\":\n",
    "                            # For vector retriever, use get_relevant_documents\n",
    "                            retrieved_docs = retriever.get_relevant_documents(query)\n",
    "                        else:\n",
    "                            # For other retrievers, use invoke\n",
    "                            retrieved_docs = retriever.invoke(query)\n",
    "                            \n",
    "                        if retriever_name == \"bm25\":\n",
    "                            retrieved_docs = retriever.invoke(query)\n",
    "                            # 确保文档有正确的格式\n",
    "                            retrieved_docs = [\n",
    "                                Document(\n",
    "                                    page_content=doc.page_content,\n",
    "                                    metadata=doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "                                ) for doc in retrieved_docs\n",
    "                            ]\n",
    "                        else:\n",
    "                            retrieved_docs = retriever.invoke(query)\n",
    "                        \n",
    "                        print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "                        \n",
    "                        if not retrieved_docs:\n",
    "                            print(f\"Warning: No documents retrieved for query: {query[:50]}...\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Print first document details for debugging\n",
    "                        if retrieved_docs:\n",
    "                            print(\"First retrieved document:\")\n",
    "                            print(f\"Length: {len(retrieved_docs[0].page_content)}\")\n",
    "                            print(f\"Content preview: {retrieved_docs[0].page_content[:200]}...\")\n",
    "                            if hasattr(retrieved_docs[0], 'metadata'):\n",
    "                                print(f\"Metadata: {retrieved_docs[0].metadata}\")\n",
    "                        \n",
    "                        # Generate response\n",
    "                        response = self._generate_response(query, retrieved_docs)\n",
    "                        print(f\"Generated response length: {len(response)}\")\n",
    "                        \n",
    "                        # Update results\n",
    "                        valid_results.at[idx, 'response'] = response\n",
    "                        valid_results.at[idx, 'retrieved_contexts'] = [doc.page_content for doc in retrieved_docs]\n",
    "                        \n",
    "                        time.sleep(1)  # Avoid rate limits\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing query: {query[:50]}...\")\n",
    "                        print(f\"Error: {str(e)}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        continue\n",
    "\n",
    "                print(f\"Successfully processed {len(valid_results)} queries\")\n",
    "\n",
    "                # Create evaluation dataset\n",
    "                eval_dataset = EvaluationDataset.from_pandas(valid_results)\n",
    "\n",
    "                # Setup evaluators\n",
    "                evaluator_llm = LangchainLLMWrapper(self.eval_llm)\n",
    "                evaluator_embeddings = LangchainEmbeddingsWrapper(self.embeddings)\n",
    "\n",
    "                # Define metrics\n",
    "                metrics = [\n",
    "                    LLMContextRecall(llm=evaluator_llm),\n",
    "                    FactualCorrectnessReviseBeta(  # 使用 Beta 版本\n",
    "                        llm=evaluator_llm,\n",
    "                        mode=\"f1\",\n",
    "                        beta=1.0,\n",
    "                        atomicity=\"high\",  # 使用高原子性以获得更细粒度的评估\n",
    "                        coverage=\"high\"    # 使用高覆盖率以捕获所有细节\n",
    "                    ),\n",
    "                    Faithfulness(llm=evaluator_llm),\n",
    "                    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "                ]\n",
    "\n",
    "                # Run evaluation\n",
    "                print(\"\\nRunning evaluation metrics...\")\n",
    "                results = evaluate(\n",
    "                    dataset=eval_dataset,\n",
    "                    metrics=metrics\n",
    "                )\n",
    "\n",
    "                # Process scores for this run\n",
    "                run_scores = {}\n",
    "                if hasattr(results, 'scores'):\n",
    "                    if isinstance(results.scores, list) and results.scores:\n",
    "                        first_result = results.scores[0]\n",
    "                        for metric_name, score in first_result.items():\n",
    "                            if isinstance(score, (int, float)) and not pd.isna(score):\n",
    "                                run_scores[metric_name] = float(score)\n",
    "                \n",
    "                if run_scores:\n",
    "                    all_run_scores.append(run_scores)\n",
    "                    print(f\"Run {run + 1} scores:\", run_scores)\n",
    "                else:\n",
    "                    print(f\"Warning: No valid scores for run {run + 1}\")\n",
    "            \n",
    "            # Calculate average scores and confidence intervals\n",
    "            if not all_run_scores:\n",
    "                print(f\"No valid results for {retriever_name}\")\n",
    "                return None\n",
    "                \n",
    "            # Calculate mean scores across all runs\n",
    "            mean_scores = {}\n",
    "            confidence_intervals = {}\n",
    "            \n",
    "            for metric in all_run_scores[0].keys():\n",
    "                metric_values = [run[metric] for run in all_run_scores if metric in run]\n",
    "                if metric_values:\n",
    "                    mean_score = np.mean(metric_values)\n",
    "                    std_dev = np.std(metric_values) if len(metric_values) > 1 else 0.1\n",
    "                    \n",
    "                    mean_scores[metric] = mean_score\n",
    "                    confidence_intervals[metric] = (\n",
    "                        max(0.0, mean_score - std_dev),  # Lower bound\n",
    "                        min(1.0, mean_score + std_dev)   # Upper bound\n",
    "                    )\n",
    "            \n",
    "            print(f\"\\nFinal average scores for {retriever_name}:\")\n",
    "            for metric, score in mean_scores.items():\n",
    "                ci = confidence_intervals[metric]\n",
    "                print(f\"{metric}: {score:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])\")\n",
    "            \n",
    "            return {\n",
    "                \"mean_scores\": mean_scores,\n",
    "                \"confidence_intervals\": confidence_intervals,\n",
    "                \"all_run_scores\": all_run_scores,\n",
    "                \"raw_results\": results\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {retriever_name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_t_validation(results, save_path=None):\n",
    "    \"\"\"\n",
    "    为每个评估指标创建单独的T-validation图\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # 定义要展示的指标及其显示名称\n",
    "    metrics_mapping = {\n",
    "        'context_recall': 'Context Recall',\n",
    "        'factual_correctness': 'Factual Correctness',\n",
    "        'faithfulness': 'Faithfulness',\n",
    "        'semantic_similarity': 'Semantic Similarity'\n",
    "    }\n",
    "    \n",
    "    for metric_key, metric_name in metrics_mapping.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 收集该指标的数据\n",
    "        retrievers = []\n",
    "        means = []\n",
    "        errors = []\n",
    "        \n",
    "        for retriever_name, result in results.items():\n",
    "            if result and result.get(\"mean_scores\") and metric_key in result[\"mean_scores\"]:\n",
    "                retrievers.append(retriever_name)\n",
    "                means.append(result[\"mean_scores\"][metric_key])\n",
    "                ci = result[\"confidence_intervals\"][metric_key]\n",
    "                errors.append((ci[1] - ci[0]) / 2)  # 使用置信区间作为误差范围\n",
    "        \n",
    "        if not retrievers:\n",
    "            continue\n",
    "            \n",
    "        # 创建图表\n",
    "        x_pos = np.arange(len(retrievers))\n",
    "        \n",
    "        # 绘制均值点和误差条\n",
    "        plt.errorbar(x_pos, means, \n",
    "                    yerr=errors,\n",
    "                    fmt='o', capsize=5, capthick=1.5, \n",
    "                    markersize=8, elinewidth=1.5)\n",
    "        \n",
    "        # 添加数值标签\n",
    "        for i, mean in enumerate(means):\n",
    "            plt.text(x_pos[i], mean + errors[i], f'{mean:.2f}', \n",
    "                    horizontalalignment='center', verticalalignment='bottom')\n",
    "        \n",
    "        # 设置图表属性\n",
    "        plt.title(f'T-Validation: {metric_name}')\n",
    "        plt.xlabel('Retriever Type')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(x_pos, retrievers, rotation=45)\n",
    "        \n",
    "        # 设置y轴范围为0-1\n",
    "        plt.ylim(0, 1.1)  # 1.1留出空间显示标签\n",
    "        \n",
    "        # 添加网格\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # 调整布局\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图表\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            plt.savefig(f'{save_path}/t_validation_{metric_key}.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(data_dir: str = \"./data\", persist_dir: str = \"./chroma_db\", n_samples: int = 15, n_runs: int = 3):\n",
    "    \"\"\"Run evaluation process\"\"\"\n",
    "    print(\"Starting evaluation process...\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RetrievalEvaluator(data_dir, persist_dir)\n",
    "    \n",
    "    # Load test dataset\n",
    "    print(\"Loading test dataset...\")\n",
    "    test_dataset = pd.read_json('./data/eval_dataset_1.json')\n",
    "    if n_samples:\n",
    "        test_dataset = test_dataset.head(n_samples)\n",
    "    \n",
    "    # Evaluate all retrievers\n",
    "    results = {}\n",
    "    retrievers = [\"vector\", \"bm25\", \"hybrid\"]\n",
    "    \n",
    "    for retriever in tqdm(retrievers, desc=\"Evaluating retrievers\"):\n",
    "        print(f\"\\nEvaluating {retriever}...\")\n",
    "        result = evaluator.evaluate_retriever(retriever, test_dataset, n_runs=n_runs)\n",
    "        if result and result.get(\"mean_scores\"):\n",
    "            results[retriever] = result\n",
    "    \n",
    "    # Print results\n",
    "    # Print results with improved formatting\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics_order = ['context_recall', 'factual_correctness', 'faithfulness', 'semantic_similarity']\n",
    "    metrics_display = {\n",
    "        'context_recall': 'Context Recall',\n",
    "        'factual_correctness': 'Factual Correctness',\n",
    "        'faithfulness': 'Faithfulness',\n",
    "        'semantic_similarity': 'Semantic Similarity'\n",
    "    }\n",
    "    \n",
    "    for retriever_name in retrievers:\n",
    "        print(f\"\\nResults for {retriever_name}:\")\n",
    "        print(\"-\" * 40)\n",
    "        if retriever_name in results and results[retriever_name].get(\"mean_scores\"):\n",
    "            print(\"Mean scores across all runs:\")\n",
    "            for metric in metrics_order:\n",
    "                if metric in results[retriever_name][\"mean_scores\"]:\n",
    "                    score = results[retriever_name][\"mean_scores\"][metric]\n",
    "                    ci = results[retriever_name][\"confidence_intervals\"][metric]\n",
    "                    print(f\"{metrics_display[metric]:<20}: {score:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])\")\n",
    "        else:\n",
    "            print(\"No valid results available\")\n",
    "        print()\n",
    "    \n",
    "    # Create visualization for each metric\n",
    "    visualize_t_validation(results, save_path=\"./evaluation_plots\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行评估\n",
    "results = run_evaluation(n_samples=15, n_runs=3)\n",
    "\n",
    "# 查看结果\n",
    "for retriever_name, result in results.items():\n",
    "    if result and result.get(\"results\"):\n",
    "        print(f\"\\nResults for {retriever_name}:\")\n",
    "        df = result[\"results\"].to_pandas()\n",
    "        print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag6998",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
