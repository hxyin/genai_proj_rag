{"user_input":"How does instruction tuning affect the zero-shot performance on unseen tasks?","reference":"For larger models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks. However, for smaller models (8B and smaller), instruction tuning actually hurts performance on held-out tasks.","response":"Instruction tuning significantly improves zero-shot performance on unseen tasks compared to models only fine-tuned on the dataset name. This is because training with instructions is crucial for zero-shot performance. \n\nHowever, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning. \n","retrieved_contexts":["models finetuned on dataset name only, we report zero-shot performance for FLAN instructions as\nwell as using the dataset name. Figure 8 shows the results\u2014both ablation configurations performed\nsubstantially worse than FLAN, indicating that training with instructions is crucial for zero-shot\nperformance on unseen tasks.\n4.4\nINSTRUCTIONS WITH FEW-SHOT EXEMPLARS\nSo far, we have focused on instruction tuning in the zero-shot setting. Here, we study how instruction\ntuning can be used when few-shot exemplars are available at inference time. The format for the\nfew-shot setting builds on the zero-shot format. For some input x and output y, let instruct(x)\ndenote the zero-shot instructions. Then, given k few-shot exemplars (xi, yi)k\ni=1 and a new input\nx, the instruction format for the few-shot setting is \u201cinstruct(x1) \u2295y1 \u2295instruct(x2) \u2295y2 \u2295. . . \u2295\n7","outperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT.\n4\nABLATION STUDIES & FURTHER ANALYSIS\n4.1\nNUMBER OF INSTRUCTION TUNING CLUSTERS\nAs the core question of our paper asks how instruction tuning improves a model\u2019s zero-shot performance on unseen tasks, in this first ablation we examine how performance is affected by the number\nof clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and\ncommonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction\ntuning.3 We show results for one to seven instruction tuning clusters, where clusters are added in\ndecreasing order of number of tasks per cluster.\nFigure 6 shows these results. As expected, we observe that average performance across the three\nheld-out clusters improves as we add additional clusters and tasks to instruction tuning (with the","Figure 6 shows these results. As expected, we observe that average performance across the three\nheld-out clusters improves as we add additional clusters and tasks to instruction tuning (with the\nexception of the sentiment analysis cluster), confirming the benefits of our proposed instruction\ntuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for\nthe seven clusters we test, the performance does not appear to saturate, implying that performance\nmay further improve with even more clusters added to instruction tuning. Of note, this ablation does\nnot allow us to draw conclusions about which instruction tuning cluster contributes the most to each\nevaluation cluster, although we see minimal added value from the sentiment analysis cluster.\nPerformance (%) \non held-out cluster\nClusters used for instruction tuning\n50\n70\n90\n\u2013 Average\nHeld-out clusters\n30\n# clusters: \n1\n2\n3\n4\n5\n6\n7\n+ summarization\n+ translation\n+ read. comp.\n(# datasets): \n(11)\n(20)","instructions. Most recently, Mishra et al. (2021) finetune 140M parameter BART on instructions\nwith few-shot exemplars, and evaluate its few-shot abilities on unseen tasks\u2014this is similar to our\nfew-shot instruction tuning result from \u00a74.4. This promising result (as well as one from Ye et al.\n(2021), which does not emphasize instructions as much) suggests that finetuning on a collection of\ntasks improves few-shot performance on unseen tasks, even at a smaller model scale. Sanh et al.\n(2021) finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a\nmodel of 11B parameters. At a model scale similar to ours, OpenAI\u2019s InstructGPT models are trained\nvia both finetuning and reinforcement learning to produce outputs that are more preferred by human\nraters (Ouyang et al., 2022).\n6\nDISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does finetuning a model on a"]}
{"user_input":"What is the Zero-shot-CoT method and how does it elicit chain of thought from large language models?","reference":"Zero-shot-CoT is a zero-shot template-based prompting method that elicits chain of thought reasoning from large language models. It does not require step-by-step few-shot examples and instead uses a single fixed prompt to prompt the models. This method encourages the discovery of broad cognitive abilities in LLMs rather than narrow task-specific skills.","response":"I'm sorry, but the answer to your question cannot be found in the provided context. The context discusses the Zero-shot-CoT method, which is a way to elicit chain of thought from large language models, but it does not provide a definition of the method itself.","retrieved_contexts":["Large Language Models are Zero-Shot Reasoners\nTakeshi Kojima\nThe University of Tokyo\nt.kojima@weblab.t.u-tokyo.ac.jp\nShixiang Shane Gu\nGoogle Research, Brain Team\nMachel Reid\nGoogle Research\u2217\nYutaka Matsuo\nThe University of Tokyo\nYusuke Iwasawa\nThe University of Tokyo\nAbstract\nPretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT) prompting,\na recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics\nand symbolic reasoning, difficult system-2 tasks that do not follow the standard\nscaling laws for LLMs. While these successes are often attributed to LLMs\u2019\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\nby simply adding \u201cLet\u2019s think step by step\u201d before each answer. Experimental","confirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\n6\nConclusion\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach","Chain of thought prompting\nMulti-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved significant\nboosts in performance across these difficult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difficult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nas Few-shot-CoT in this work.\n3\nZero-shot Chain of Thought","Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-specific few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the"]}
{"user_input":"How does prompt tuning affect model performance in improving NLP tasks?","reference":"Prompt tuning improves model performance in improving NLP tasks, especially when using FLAN. In many cases, prompt tuning on FLAN achieves more than 10% improvement over prompt tuning on LaMDA-PT. This result exemplifies how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks.","response":"Prompt tuning improves model performance in NLP tasks by allowing the model to respond better to continuous inputs. This is because the model is able to learn how to use the prompts to generate more accurate and relevant responses. For example, in Figure 10, the instruction-tuned model achieves a higher performance than the untuned model on the SuperGLUE dev set. This suggests that prompt tuning is an effective way to improve the performance of NLP models.","retrieved_contexts":["prompt tuning\nInstruction-tuned model\nUntuned model\n63.8\n78.1\n79.1\n87.4\nFigure 10:\nInstruction-tuned\nmodels respond better to continuous inputs from prompt tuning.\nWhen prompt tuning on a given\ndataset, no tasks from the same\ncluster as that dataset were seen\nduring instruction tuning. Performance shown is the average on\nthe SuperGLUE dev set.\nAs we\u2019ve seen that instruction tuning improves the ability of\na model to respond to instructions, it follows that, if FLAN is\nindeed more amenable to performing NLP tasks, then it should\nalso achieve better performance when performing inference using\nsoft prompts, represented by prepended continuous variables\noptimized via prompt tuning (Li & Liang, 2021; Lester et al.,\n2021). As further analysis, we train continuous prompts for each\nof the SuperGLUE (Wang et al., 2019a) tasks in accordance with\nthe cluster splits from \u00a72.2 such that when prompt-tuning on task\nT , no tasks in the same cluster as T were seen during instruction","better understand the output format. In addition, for all task clusters, standard deviation among\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\nNLI\nRead. Comp. Closed-Book QA Commonsense\nCoreference\nTranslation\nZero-shot FLAN\nFew-shot FLAN\nPerformance\n20\n40\n60\n80\n54.7 59.3\n59.6 60.0\n53.7\nStruct to text\n57.2\n31.0 33.0\n80.0 80.8\n63.8 67.4\n39.2\n49.4\nTask Cluster:\n# datasets:\n7\n5\n3\n4\n2\n3\n4\nFigure 9:\nAdding few-shot exemplars to FLAN is a complementary method for improving the\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\ntemplates, averaged at the dataset level for each task cluster.\n4.5\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\n32 training \nexamples\nFull training \nset\n100\n0\n50\n75\n25\nPerformance after \nprompt tuning\nInstruction-tuned model\nUntuned model\n63.8\n78.1\n79.1\n87.4\nFigure 10:\nInstruction-tuned\nmodels respond better to continuous inputs from prompt tuning.\nWhen prompt tuning on a given","raters (Ouyang et al., 2022).\n6\nDISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does finetuning a model on a\ncollection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\nthis question via instruction tuning, a simple method that combines appealing aspects of both\nthe pretrain\u2013finetune and prompting paradigms. Our instruction-tuned model, FLAN, improves\nperformance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\nwe evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\nof instruction tuning task clusters, and, interestingly, that performance improvements from instruction\ntuning emerge only with sufficient model scale. Moreover, instruction tuning can be combined with\nother prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between","other prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between\nspecialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan\net al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might\nexpect labeled data to have the most natural role in improving specialist models, instruction tuning\ndemonstrates how labeled data can be used to help large language models perform many, unseen\ntasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\ntask-specific training is complementary to general language modeling and motivates further research\non generalist models.\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\nwe try to use accepted categorizations in the literature), and we only explore the use of relatively"]}
{"user_input":"What is the purpose of instruction tuning in language models and how does it enhance response to NLP instructions for unseen tasks?","reference":"The purpose of instruction tuning in language models is to improve their ability to respond to NLP instructions. It involves using supervision to teach the language model to perform tasks described via instructions. By instruction tuning, the language model learns to follow instructions and can do so even for unseen tasks. This enhances the model's response to NLP instructions for tasks it has not encountered before.","response":"Instruction tuning aims to enhance the ability of language models to respond to NLP instructions for unseen tasks. It works by training the model on a diverse set of tasks described via instructions, allowing it to learn to follow instructions and generalize to new tasks it hasn't encountered before. This approach has been shown to improve performance on unseen tasks compared to traditional pretrain-finetune and prompting methods. \n","retrieved_contexts":["LM\n(C) Instruction tuning (FLAN)\nInstruction-tune on \nmany tasks: \nB, C, D, \u2026\nInference \non task A\nInference on \nunseen task\nModel learns to perform \nmany tasks via natural \nlanguage instructions\nFigure 2: Comparing instruction tuning with pretrain\u2013finetune and prompting.\n2\nFLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING\nThe motivation of instruction tuning is to improve the ability of language models to respond to NLP\ninstructions. The idea is that by using supervision to teach an LM to perform tasks described via\ninstructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate\nperformance on unseen tasks, we group datasets into clusters by task type and hold out each task\ncluster for evaluation while instruction tuning on all remaining clusters.\n2","Published as a conference paper at ICLR 2022\net al., 2021). Compared with this prior work that finetunes and evaluates on the same downstream\ntask, our setup studies the effect of instruction tuning on ability to perform unseen tasks.\nD.5\nMULTI-TASK QUESTION ANSWERING\nThe instructions we use for instruction tuning are similar to QA-based task formulation research,\nwhich aims to unify NLP tasks by casting them as question-answering over a context. For instance,\nMcCann et al. (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated\nwith natural language prompts; they report transfer learning gains on finetuning tasks as well as\nzero-shot domain adaptation results on SNLI (Bowman et al., 2015) and Amazon\/Yelp Reviews\n(Kotzias et al., 2015). While McCann et al. (2018) does not leverage unsupervised pre-training and\nonly reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on","Published as a conference paper at ICLR 2022\nB\nFURTHER ABLATION STUDIES AND ANALYSIS\nB.1\nDATASETS PER TASK CLUSTER & TEMPLATES PER DATASET\nOur primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on\nunseen tasks. \u00a74.1 showed that adding more task clusters improves performance; here, we further\nexplore whether adding additional datasets improves performance when the number of task clusters\nis held constant. We use the same split as in \u00a74.1, where the NLI, commonsense reasoning, and\nclosed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning. For\nthese seven task clusters, we instruction tune models using just one dataset per task cluster and using\nfour datasets per task cluster (for task clusters that did not have four tasks, we just used all available\ntasks). In addition, we simultaneously explore the role of the number of instruction templates per","raters (Ouyang et al., 2022).\n6\nDISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does finetuning a model on a\ncollection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\nthis question via instruction tuning, a simple method that combines appealing aspects of both\nthe pretrain\u2013finetune and prompting paradigms. Our instruction-tuned model, FLAN, improves\nperformance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\nwe evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\nof instruction tuning task clusters, and, interestingly, that performance improvements from instruction\ntuning emerge only with sufficient model scale. Moreover, instruction tuning can be combined with\nother prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between"]}
{"user_input":"What distinguishes Zero-shot-CoT from Few-shot-CoT in terms of prompting methods for large language models?","reference":"Zero-shot-CoT differs from Few-shot-CoT in that it does not require step-by-step few-shot examples for prompting. It is a zero-shot template-based prompting method for chain of thought reasoning.","response":"Zero-shot-CoT uses the same prompt \"Let's think step by step\" across all tasks, while Few-shot-CoT uses step-by-step reasoning examples per task. Zero-shot-CoT does not require any examples, while Few-shot-CoT requires examples from different datasets. \n","retrieved_contexts":["Chain of thought prompting\nMulti-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved significant\nboosts in performance across these difficult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difficult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nas Few-shot-CoT in this work.\n3\nZero-shot Chain of Thought","templates, the performance does not improve. It remains an open question how to automatically\ncreate better templates for Zero-shot-CoT.\nHow does prompt selection affect Few-shot-CoT?\nTable 5 shows the performance of Fewshot-CoT when using examples from different datasets: CommonsenseQA to AQUA-RAT and\nCommonsenseQA to MultiArith. The domains are different in both cases, but the answer format\n8","Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-specific few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the","confirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\n6\nConclusion\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach"]}
{"user_input":"Which language models were used in the experiment 'Exploring Zero-Shot Learning in Neural Networks', and what were their parameters, libraries\/API names, and licenses?","reference":"The language models used in the experiment 'Exploring Zero-Shot Learning in Neural Networks' were PaLM with 540B parameters, Original GPT3 with 175B parameters (library\/API name: OpenAI API, model name: davinci), Original GPT3 with 6.7B parameters (library\/API name: OpenAI API, model name: curie), Original GPT3 with 1.3B parameters (library\/API name: OpenAI API, model name: babbage), Original GPT3 with 0.3B parameters (library\/API name: OpenAI API, model name: ada), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-davinci-002), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-davinci-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-curie-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-babbage-001), Instruct GPT3 with unknown parameters (library\/API name: OpenAI API, model name: text-ada-001), OPT with 13B parameters (library\/API name: Hugging Face Library, model name: opt-13b), T0 with 11B parameters (library\/API name: Hugging Face Library, model name: T0pp), GPT-J with 6B parameters (library\/API name: Hugging Face Library, model name: gptj), GPT-Neo with 2.7B parameters (library\/API name: Hugging Face Library, model name: gpt-neo), and GPT-2 with 1.5B parameters (library\/API name: Hugging Face Library, model name: gpt2-xl). The licenses for the models are unspecified for most of them, except for Original GPT3 and GPT-2 which have an Apache-2.0 license.","response":"I am sorry, but the information you are looking for is not available in the provided context. The context does not mention the specific language models used in the experiment, their parameters, libraries\/API names, or licenses. ","retrieved_contexts":["A\nDetails of Experimental Setup\nA.1\nCode\nCode is available at https:\/\/github.com\/kojima-takeshi188\/zero_shot_cot.\nA.2\nDatasets\nA.2.1\nDataset Description\nTable 7 summarizes the description of each dataset used in our experiment.\nTable 7: Dataset Description. Our experiments used publicly available datasets except for \u201cLast\nLetters\u201d and \u201cCoin Flip\u201d datasets. We created these two datasets. See Appendix A.2.2 for the details.\n(*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format.\n(*2) Average number of words in questions texts.\nDataset\nAnswer\nFormat\n(*1)\n# of\nsamples\nAvg #\nwords\n(*2)\nData split (filename)\nused for our experiment\nLicense\nSingleEq\nN\n508\n27.4\nquestions.json\nNo License\nAddSub\nN\n395\n31.5\nAddSub.json\nUnspecified\nMultiArith\nN\n600\n31.8\nMultiArith.json\nUnspecified\nGSM8K\nN\n1319\n46.9\ntest.jsonl\nMIT License\nAQUA-RAT\nM\n254\n51.9\ntest.jsonl\nApache-2.0\nSVAMP\nN\n1000\n31.8\nSVAMP.json\nMIT License\nCommonsenseQA\nM\n1221\n27.8\ndev_rand_split.jsonl","Published as a conference paper at ICLR 2022\nFLAN 137B\nGLaM\nLaMDA-PT\nGPT-3 175B\nzero-shot\nfew-shot\nRandom\nGuess\nSupervised\nModel\nzeroshot\noneshot\nzeroshot\nfewshot [k]\nzeroshot\nfewshot [k]\naverage\ntemplate\nbest dev\ntemplate\naverage\ntemplate\nbest dev\ntemplate [k] #t\nNLI\nANLI R1\n33.3\n57.4b\n40.9 42.4 39.6 39.0 [5]\n34.6 36.8 [50] 47.7\u00b11.4\n46.4 44.2\u00b12.3\n47.9\n[6]\n8\nANLI R2\n33.3\n48.3b\n38.2 40.0 39.9 37.5 [5]\n35.4 34.0 [50] 43.9\u00b11.3\n44.0 41.6\u00b11.4\n41.1\n[6]\n8\nANLI R3\n33.3\n43.5b\n40.9 40.8 39.3 40.7 [5]\n34.5 40.2 [50] 47.0\u00b11.3\n48.5 42.8\u00b12.2\n46.8\n[6]\n8\nCB\n33.3\n93.6a\n33.9 73.2 42.9 34.4 [5]\n46.4 82.1 [32] 64.1\u00b114.7\n83.9 82.6\u00b14.4\n82.1\n[7]\n10\nMNLI-m\n33.3\n92.2a\n\u2013\n\u2013\n35.7 43.7 [5]\n\u2013\n\u2013\n51.1\u00b16.2\n61.2 60.8\u00b13.7\n63.5\n[10] 10\nMNLI-mm\n33.3\n91.9a\n\u2013\n\u2013\n37.0 43.8 [5]\n\u2013\n\u2013\n51.0\u00b16.5\n62.4 61.0\u00b13.5\n63.5\n[10] 10\nQNLI\n50.0\n96.9a\n\u2013\n\u2013\n50.6 55.7 [5]\n\u2013\n\u2013\n59.6\u00b14.9\n66.4 62.0\u00b11.7\n63.3\n[12]\n9\nRTE\n50.0\n92.5a\n68.8 71.5 73.3 70.8 [5]\n63.5 72.9 [32] 78.3\u00b17.9\n84.1 79.9\u00b16.9\n84.5\n[8]\n10\nSNLI\n33.3\n91.3b\n\u2013\n\u2013\n33.3 54.7 [5]\n\u2013\n\u2013\n43.0\u00b17.4","Language Models are Few-Shot Learners\nTom B. Brown\u2217\nBenjamin Mann\u2217\nNick Ryder\u2217\nMelanie Subbiah\u2217\nJared Kaplan\u2020\nPrafulla Dhariwal\nArvind Neelakantan\nPranav Shyam\nGirish Sastry\nAmanda Askell\nSandhini Agarwal\nAriel Herbert-Voss\nGretchen Krueger\nTom Henighan\nRewon Child\nAditya Ramesh\nDaniel M. Ziegler\nJeffrey Wu\nClemens Winter\nChristopher Hesse\nMark Chen\nEric Sigler\nMateusz Litwin\nScott Gray\nBenjamin Chess\nJack Clark\nChristopher Berner\nSam McCandlish\nAlec Radford\nIlya Sutskever\nDario Amodei\nOpenAI\nAbstract\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions \u2013 something which current NLP systems still largely","Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\nvalidation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior\nobserved in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the\npredicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\nSetting\nPTB\nSOTA (Zero-Shot)\n35.8a\nGPT-3 Zero-Shot\n20.5\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets\nare omitted because they are derived from Wikipedia or other sources which are included in GPT-3\u2019s training data.\na[RWC+19]\n3.1\nLanguage Modeling, Cloze, and Completion Tasks\nIn this section we test GPT-3\u2019s performance on the traditional task of language modeling, as well as related tasks\nthat involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible"]}
{"user_input":"How does Zero-shot-CoT differ from previous few-shot approaches in eliciting chain of thought from large language models?","reference":"Zero-shot-CoT differs from previous few-shot approaches in eliciting chain of thought from large language models by being a single zero-shot prompt that elicits chain of thought across a variety of reasoning tasks, whereas previous few-shot approaches require hand-crafting few-shot examples per task.","response":"Zero-shot-CoT differs from previous few-shot approaches in eliciting chain of thought from large language models by not requiring any in-context examples. This means that Zero-shot-CoT can be used to elicit chain of thought from any large language model, regardless of its size or training data. Additionally, Zero-shot-CoT is more efficient than few-shot approaches, as it does not require any additional training data or fine-tuning.","retrieved_contexts":["Chain of thought prompting\nMulti-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved significant\nboosts in performance across these difficult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difficult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nas Few-shot-CoT in this work.\n3\nZero-shot Chain of Thought","confirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\n6\nConclusion\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach","Large Language Models are Zero-Shot Reasoners\nTakeshi Kojima\nThe University of Tokyo\nt.kojima@weblab.t.u-tokyo.ac.jp\nShixiang Shane Gu\nGoogle Research, Brain Team\nMachel Reid\nGoogle Research\u2217\nYutaka Matsuo\nThe University of Tokyo\nYusuke Iwasawa\nThe University of Tokyo\nAbstract\nPretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT) prompting,\na recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics\nand symbolic reasoning, difficult system-2 tasks that do not follow the standard\nscaling laws for LLMs. While these successes are often attributed to LLMs\u2019\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\nby simply adding \u201cLet\u2019s think step by step\u201d before each answer. Experimental","benchmarks (MultiArith and GSM8K) across Zero-shot-CoT and baselines. The large gap between\nstandard prompting (1st block) and chain of thought prompting (2nd block) suggests that these\ntasks are difficult without eliciting multi-step reasoning. Major improvements are confirmed on\nboth Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). While Zero-shot-CoT\nnaturally underperforms Few-shot-CoT, it substantially outperforms standard Few-shot prompting\nwith even 8 examples per task. For GSM8K, Zero-shot-CoT with Instruct GPT-3 (text-davinci-002)\nalso outperforms finetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B),\nreported in Wei et al. [2022] (3rd and 4th block). See App. D for more experiment results with PaLM.\nDoes model size matter for zero-shot reasoning?\nFigure 3 compares performance of various\nlanguage models on MultiArith \/ GSM8K. Without chain of thought reasoning, the performance"]}
{"user_input":"What are the stages in the Zero-shot-CoT method for reasoning and answer extraction, and how are they implemented?","reference":"The Zero-shot-CoT method for reasoning and answer extraction involves two stages: reasoning extraction and answer extraction. In the reasoning extraction stage, the input question is modified into a prompt using a template. The prompt is fed into a language model to generate a subsequent sentence. In the answer extraction stage, the generated sentence and the prompted sentence are used to extract the final answer from the language model. The prompt for this stage is self-augmented, as it contains the sentence generated by the same language model. The language model is then fed the prompted text to generate sentences and parse the final answer.","response":"The Zero-shot-CoT method for reasoning and answer extraction involves two stages:\n\n1. **Reasoning extraction:** The input question is modified into a prompt with a trigger sentence designed to elicit a chain of thought. The prompt is then fed into a language model to generate a subsequent sentence containing the reasoning steps.\n2. **Answer extraction:** The generated text is processed to extract the answer according to the specified answer format. This may involve identifying the first number or letter that satisfies the format, or selecting the first answer choice.","retrieved_contexts":["extract the answer in the correct format from the reasoning text.\nin Figure 1). In summary, Few-shot-CoT [Wei et al., 2022] requires careful human engineering of\na few prompt examples with specific answer formats per task, while Zero-shot-CoT requires less\nengineering but requires prompting LLMs twice.\n1st prompt: reasoning extraction\nIn this step we first modify the input question x into a prompt\nx\u2032 using a simple template \u201cQ: [X]. A: [T]\u201d, where [X] is an input slot for x and [T] is an slot\nfor hand-crafted trigger sentence t that would extract chain of though to answer the question x. For\nexample, if we use \u201cLet\u2019s think step by step\u201d as a trigger sentence, the prompt x\u2032 would be \u201cQ: [X].\nA: Let\u2019s think step by step.\u201d. See Table 4 for more trigger examples. Prompted text x\u2032 is then fed into\na language model and generate subsequent sentence z. We can use any decoding strategy, but we\nused greedy decoding throughout the paper for the simplicity.\n2nd prompt: answer extraction","of thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\nsampled outputs for each model).\nError Analysis\nTo better understand the behavior of Zero-shot-CoT, we manually investigated\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Appendix C for examples, where some of the observations include: (1) In commonsense reasoning\n(CommonsenseQA), Zero-shot-CoT often produces flexible and reasonable chain of thought even\nwhen the final prediction is not correct. Zero-shot-CoT often output multiple answer choices when\nthe model find it is difficult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\n7","reasoning (MultiArith), Zero-shot-CoT and Few-shot-CoT show substantial differences regarding\nthe error patterns. First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting\nthe correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also\nsometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT\ntend to fail when generated chain of thought include ternary operation, e.g. (3 + 2) \u22174.\nHow does prompt selection affect Zero-shot-CoT?\nWe validate the robustness of Zero-shot-CoT\nagainst input prompts. Table 4 summarizes performance using 16 different templates with three\ncategories. Specifically, following Webson and Pavlick [2022], the categories include instructive\n(encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong\nway), and irrelevant (nothing to do with reasoning). The results indicate that the performance is","deterministic. For the few-shot approaches, since the order of in-context examples could affect the\nresults [Lu et al., 2022], we run each experiment only once with a fixed seed across all methods and\ndatasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of\nexamples did not cause large variance in CoT experiments.\nAnswer cleansing\nAfter the model outputs a text by answer extraction (see \u00a7 3 and Figure 2), our\nmethod picks up only the part of the answer text that first satisfies the answer format. For example,\nif the answer prompting outputs \u201cprobably 375 and 376\u201d on arithmetic tasks, we extract the first\nnumber \u201c375\u201d and set it as the model prediction. In the case of multiple-choice, the first large letter\nwe encounter is set as the prediction. See Appendix A.6 for more detail. Standard Zero-shot method\nfollows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and"]}
{"user_input":"What are the main approaches for inducing LLMs to perform chain-of-thought reasoning, according to Wei et al. in 2022?","reference":"The main approaches for inducing LLMs to perform chain-of-thought reasoning, according to Wei et al. in 2022, are Zero-Shot-CoT and Manual-CoT. Zero-Shot-CoT adds a single prompt like \"Let's think step by step\" after the test question to facilitate reasoning chains in LLMs. Manual-CoT involves few-shot prompting with manual reasoning demonstrations one by one, where each demonstration consists of a question and a reasoning chain composed of intermediate steps and an expected answer.","response":"According to Wei et al. (2022), there are two main approaches for inducing LLMs to perform chain-of-thought reasoning: Zero-Shot-CoT and Manual-CoT. Zero-Shot-CoT leverages the self-generated rationales of LLMs to reflect CoT reasoning, while Manual-CoT uses manually written few-shot demonstrations to guide LLMs.","retrieved_contexts":["slow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\nsatisfies the scaling laws better and jumps up with the size of the language models. For example,\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\nprompting significantly increases the performance over standard few-shot prompting across several\nbenchmark reasoning tasks, e.g., GSM8K (17.9% \u219258.1%).\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-specific","AUTOMATIC CHAIN OF THOUGHT PROMPTING\nIN LARGE LANGUAGE MODELS\nZhuosheng Zhang\u2020\u2217, Aston Zhang\u2021, Mu Li\u2021, Alex Smola\u2021\n\u2020Shanghai Jiao Tong University, \u2021Amazon Web Services\nABSTRACT\nLarge language models (LLMs) can perform complex reasoning by generating intermediate reasoning\nsteps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting.\nCoT prompting has two major paradigms. One leverages a simple prompt like \u201cLet\u2019s think step by\nstep\u201d to facilitate step-by-step thinking before answering a question. The other uses a few manual\ndemonstrations one by one, each composed of a question and a reasoning chain that leads to an\nanswer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific\ndemonstrations one by one. We show that such manual efforts may be eliminated by leveraging\nLLMs with the \u201cLet\u2019s think step by step\u201d prompt to generate reasoning chains for demonstrations one","multi-step reasoning and in-context learning for inducing LLMs to learn from demonstrations.\n2.1\nChain-of-thought Prompting\nCoT prompting is a gradient-free technique of inducing LLMs to produce intermediate reasoning steps that lead to the\nfinal answer. Wei et al. [2022a] formally studied the topic of CoT prompting in language models. This technique elicits\nLLMs to generate a coherent series of intermediate reasoning steps that lead to the final answer to a question. Studies\nhave shown that LLMs can perform CoT reasoning with zero-shot prompting (Zero-Shot-CoT) [Kojima et al., 2022] or\nmanually written few-shot demonstrations (Manual-CoT) [Wei et al., 2022a].\nZero-Shot-CoT.\nKojima et al. [2022] showed that LLMs are decent zero-shot reasoners whose generated rationales\nhave already reflected the CoT reasoning. This finding inspires our work to leverage the self-generated rationales for","improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi\u00b4c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the final\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the final answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps\u2014interpretability is just a side effect. Marasovi\u00b4c et al. (2022) show that prompt-based finetuning\nwith NLE improves NLI and classification performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,"]}
{"user_input":"Which sorting method has the most impact on Auto-CoT's demonstration accuracy in question clustering?","reference":"The sorting method that has the most impact on Auto-CoT's demonstration accuracy in question clustering is the minimal distance to the cluster center (In-Cluster Min Dist).","response":"I am sorry, but the provided context does not contain information about the impact of different sorting methods on Auto-CoT's demonstration accuracy in question clustering. Therefore, I cannot answer your question. ","retrieved_contexts":["same demonstrations for multiple datasets (e.g., 5\/6 of the\narithmetic datasets). In contrast, Auto-CoT is more flexible and task-adaptive: every single dataset gets its own\ndemonstrations that are automatically constructed.\n5.3\nVisualization of Question Clustering\nFigure 5 visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist\ngeneric patterns, where different patterns may be characterized by questions from different clusters. We present the\nconstructed demonstrations of Auto-CoT in Appendix D.\n#5\nAddSub\nSingleEq\nCoin Flip \nGSM8K\nLast Letter Concatenation\nMultiArith\nStrategyQA\nAQUA\nCSQA\nSVAMP\nFigure 5: Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.\n8","5.4\nGeneral Effectiveness Using the Codex LLM\nTo evaluate the general effectiveness of Auto-CoT using different LLMs, here we change the LLM to the Codex model\n[Chen et al., 2021]. As in Table 4, the Codex LLM leads to performance improvement for Manual-CoT when compared\nwith Table 3 that uses the GPT-3 (text-davinci-002) LLM. Nonetheless, using the Codex LLM, the overall performance\nof Auto-CoT is still competitive compared to Manual-CoT, providing additional empirical evidence for the effectiveness\nof Auto-CoT.\n5.5\nEffect of Wrong Demonstrations\nRecall our discussions in Section 3.3 that there can be wrong demonstrations (whose answers are wrong). To see if\ndiversity mitigates this effect, we design an In-Cluster Sampling baseline that constructs demonstrations by randomly\nsampling questions from the same cluster that contains a test question. Figure 6 compares accuracy with varying","the effect of misleading by similarity (Section 3.1). On the other hand, if we took each demonstration as a kind of skill,\ndiverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists\na small portion (e.g., 1\/8) of mistakes in the demonstrations, the performance will not be negatively affected (to be\nshown in Figure 6).\nNevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such\nas from questions in the frequent-error cluster. As we will show later, some of these wrong demonstrations may be\neliminated with heuristics. For example, wrong demonstrations often come with long questions and long rationales.\nUsing simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps\nmitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix C.2).\n4\nAuto-CoT: Automatic Chain-of-Thought Prompting","sampling questions from the same cluster that contains a test question. Figure 6 compares accuracy with varying\namounts of wrong demonstrations on MultiArith. Compared with In-Cluster Sampling, Auto-CoT (using diversity-based\nclustering) is less affected by wrong demonstrations: its performance still does not degrade significantly even when\npresented with 50% wrong demonstrations.\n12.5%\n25.0%\n37.5%\n50.0%\n80\n85\n90\n95\n100\nPercentage of wrong demonstrations\nAccuracy (%)\nIn-Cluster Sampling\nAuto-CoT\nFigure 6: Effect of wrong demonstrations.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n60\n70\n80\n90\n100\nBatch\nAccuracy (%)\nZero-Shot-CoT\nManual-CoT\nAuto-CoT*\nFigure 7: Bootstraping for the streaming setting.\n5.6\nMore Challenging Streaming Setting\nCoT studies commonly assume that a full dataset with test questions is given [Wei et al., 2022a, Kojima et al., 2022].\nBased on the given dataset, Auto-CoT samples questions to construct the demonstrations. Nonetheless, now we consider"]}
{"user_input":"What are the pros and cons of prompting methods for large language models in terms of their cognitive abilities and task prompt sensitivity?","reference":"Our work is based on prompting methods for large language models. LLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset Details\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a method that looks to take advantage of the patterns captured by language models conducive to various tasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way to probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context learning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.","response":"I'm sorry, but the information you provided does not contain the answer to the question. The provided text discusses the role of prompt engineering in large language models and how different prompting methods can affect their performance. However, it does not mention the pros and cons of prompting methods in terms of their cognitive abilities and task prompt sensitivity.","retrieved_contexts":["Question\nQuestion\nQuestion\nFigure 10:\nExamples of semantic understanding and one-step missing errors that were fixed by\nscaling PaLM from 62B to 540B.\nA.2\nWhat is the role of prompt engineering?\nOne of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage\nof work showing that prompts affect language models in unexpected ways (Min et al., 2022). The\ngeneral way that we created chain of thought annotations was by taking eight exemplars from the\ntraining set and decomposing the reasoning process into multiple steps leading to the final answer.\nExamples of chain of thought annotations are provided in Figure 3, with full prompts given in\nAppendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed\nrobustness experiments with respect to various factors.\n\u2022 Different annotators. We first analyze robustness to three different annotators (Section 3.4 and","did not lead to significant gains (e.g., increasing from 8 to 16 exemplars did not improve the\nperformance of standard prompting enough to catch up with chain-of-thought prompting).\n\u2022 Different language models. Another interesting question is whether certain prompts that work\nbetter for one model work better for other large language models. We find that with the same\nprompts, chain-of-thought prompting improves performance across all three models (LaMDA,\nGPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4,\nTable 5). The fact that gains from chain of thought did not transfer perfectly among models is\na limitation; further work could investigate why how different pre-training datasets and model\narchitectures affect the performance gain from chain-of-thought prompting.\nPrompt engineering still matters, though. Although the results are relatively robust to the prompt","2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language\nmodels (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various\ntasks by simply conditioning the models on a few examples (few-shot) or instructions describing the\ntask (zero-shot). The method of conditioning the language model is called \u201cprompting\u201d [Liu et al.,\n2021b], and designing prompts either manually [Schick and Sch\u00fctze, 2021, Reynolds and McDonell,\n2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\n\u2217Work done while at The University of Tokyo.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2205.11916v4  [cs.CL]  29 Jan 2023","Standard prompting\n21.5 \u00b10.6\n59.5 \u00b13.0\n80.8 \u00b11.8\n5.8 \u00b10.6\n49.0 \u00b12.1\nChain of thought prompting\n26.8 \u00b12.1\n85.8 \u00b11.8\n91.7 \u00b11.4\n77.5 \u00b13.8\n99.6 \u00b10.3\nAblations\n\u00b7 variable compute only\n21.3 \u00b10.7\n61.6 \u00b12.2\n74.2 \u00b12.3\n7.2 \u00b11.6\n50.7 \u00b10.7\n\u00b7 reasoning after answer\n20.9 \u00b11.0\n63.0 \u00b12.0\n83.3 \u00b10.6\n0.0 \u00b10.0\n50.2 \u00b10.5\nRobustness\n\u00b7 different annotator (B)\n27.4 \u00b11.7\n75.4 \u00b12.7\n88.3 \u00b11.4\n76.0 \u00b11.9\n77.5 \u00b17.9\n\u00b7 different annotator (C)\n25.5 \u00b12.5\n81.1 \u00b13.6\n85.0 \u00b11.8\n68.1 \u00b12.2\n71.4 \u00b111.1\n23"]}
{"user_input":"What are the stages in Zero-shot-CoT for reasoning and answer extraction, and how is it different from Few-shot-CoT?","reference":"Zero-shot-CoT involves two stages: reasoning extraction and answer extraction. In the reasoning extraction stage, the input question is modified into a prompt using a template. The prompt is fed into a language model to generate a subsequent sentence. In the answer extraction stage, the generated sentence and the prompted sentence are used to extract the final answer from the language model. Zero-shot-CoT differs from Few-shot-CoT in that it does not require step-by-step few-shot examples.","response":"## Zero-Shot-CoT Stages and Differences from Few-Shot-CoT\n\nZero-Shot-CoT involves two stages:\n\n1. **Reasoning extraction:** The input question is modified with a prompt template to encourage the LLM to generate a chain of thought.\n2. **Answer extraction:** The generated chain of thought is used to extract the final answer.\n\nZero-Shot-CoT differs from Few-Shot-CoT in two key ways:\n\n* **Prompt engineering:** Zero-Shot-CoT requires less manual engineering of prompts, using a simple template instead of specific examples with answer formats.\n* **LLM prompting:** Zero-Shot-CoT requires prompting the LLM twice, once for reasoning extraction and once for answer extraction, while Few-Shot-CoT only requires one prompt. \n","retrieved_contexts":["extract the answer in the correct format from the reasoning text.\nin Figure 1). In summary, Few-shot-CoT [Wei et al., 2022] requires careful human engineering of\na few prompt examples with specific answer formats per task, while Zero-shot-CoT requires less\nengineering but requires prompting LLMs twice.\n1st prompt: reasoning extraction\nIn this step we first modify the input question x into a prompt\nx\u2032 using a simple template \u201cQ: [X]. A: [T]\u201d, where [X] is an input slot for x and [T] is an slot\nfor hand-crafted trigger sentence t that would extract chain of though to answer the question x. For\nexample, if we use \u201cLet\u2019s think step by step\u201d as a trigger sentence, the prompt x\u2032 would be \u201cQ: [X].\nA: Let\u2019s think step by step.\u201d. See Table 4 for more trigger examples. Prompted text x\u2032 is then fed into\na language model and generate subsequent sentence z. We can use any decoding strategy, but we\nused greedy decoding throughout the paper for the simplicity.\n2nd prompt: answer extraction","reasoning (MultiArith), Zero-shot-CoT and Few-shot-CoT show substantial differences regarding\nthe error patterns. First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting\nthe correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also\nsometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT\ntend to fail when generated chain of thought include ternary operation, e.g. (3 + 2) \u22174.\nHow does prompt selection affect Zero-shot-CoT?\nWe validate the robustness of Zero-shot-CoT\nagainst input prompts. Table 4 summarizes performance using 16 different templates with three\ncategories. Specifically, following Webson and Pavlick [2022], the categories include instructive\n(encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong\nway), and irrelevant (nothing to do with reasoning). The results indicate that the performance is","of thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\nsampled outputs for each model).\nError Analysis\nTo better understand the behavior of Zero-shot-CoT, we manually investigated\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Appendix C for examples, where some of the observations include: (1) In commonsense reasoning\n(CommonsenseQA), Zero-shot-CoT often produces flexible and reasonable chain of thought even\nwhen the final prediction is not correct. Zero-shot-CoT often output multiple answer choices when\nthe model find it is difficult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\n7","Table 1: Accuracy comparison of Zero-shot-CoT with Zero-shot on each tasks. The values on the left\nside of each task are the results of using answer extraction prompts depending on answer format as\ndescribed at \u00a7 3. The values on the right side are the result of additional experiment where standard\nanswer prompt \"The answer is\" is used for answer extraction. See Appendix A.5 for detail setups.\nArithmetic\nSingleEq\nAddSub\nMultiArith\nGSM8K\nAQUA\nSVAMP\nzero-shot\n74.6\/78.7\n72.2\/77.0\n17.7\/22.7\n10.4\/12.5\n22.4\/22.4\n58.8\/58.7\nzero-shot-cot\n78.0\/78.7\n69.6\/74.7\n78.7\/79.3\n40.7\/40.5\n33.5\/31.9\n62.1\/63.7\nCommon Sense\nOther Reasoning Tasks\nSymbolic Reasoning\nCommon\nStrategy\nDate\nShuffled\nLast Letter\nCoin Flip\nSenseQA\nQA\nUnderstand\nObjects\n(4 words)\n(4 times)\nzero-shot\n68.8\/72.6\n12.7\/54.3\n49.3\/33.6\n31.3\/29.7\n0.2\/12.8\/53.8\nzero-shot-cot\n64.6\/64.0\n54.8\/52.3\n67.5\/61.8\n52.4\/52.9\n57.6\/91.4\/87.8"]}
{"user_input":"How does the number of datasets and templates affect the performance of instruction tuning in the FLAN model?","reference":"Using more datasets per task cluster improves performance, while using more templates per dataset has a negligible effect on performance.","response":"The number of datasets and templates has a mixed effect on the performance of instruction tuning in the FLAN model. While using more datasets per cluster improves performance, using more templates per dataset has a negligible effect. This suggests that the model is not easily overfit to a single template.","retrieved_contexts":["better understand the output format. In addition, for all task clusters, standard deviation among\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\nNLI\nRead. Comp. Closed-Book QA Commonsense\nCoreference\nTranslation\nZero-shot FLAN\nFew-shot FLAN\nPerformance\n20\n40\n60\n80\n54.7 59.3\n59.6 60.0\n53.7\nStruct to text\n57.2\n31.0 33.0\n80.0 80.8\n63.8 67.4\n39.2\n49.4\nTask Cluster:\n# datasets:\n7\n5\n3\n4\n2\n3\n4\nFigure 9:\nAdding few-shot exemplars to FLAN is a complementary method for improving the\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\ntemplates, averaged at the dataset level for each task cluster.\n4.5\nINSTRUCTION TUNING FACILITATES PROMPT TUNING\n32 training \nexamples\nFull training \nset\n100\n0\n50\n75\n25\nPerformance after \nprompt tuning\nInstruction-tuned model\nUntuned model\n63.8\n78.1\n79.1\n87.4\nFigure 10:\nInstruction-tuned\nmodels respond better to continuous inputs from prompt tuning.\nWhen prompt tuning on a given","tasks). In addition, we simultaneously explore the role of the number of instruction templates per\ndataset; as mentioned in \u00a72.1, for each dataset we manually composed ten instructional templates for\ninstruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\nFigure 11 shows these results. Using more datasets per cluster improved performance by almost\n10% on average across the three held-out clusters. Using more templates per dataset, however,\nhad a comparatively negligible effect on performance when there was one task per cluster, which\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\noriginal motivation that composing ten templates per task would mitigate overfitting to any particular\ntemplate. This results serves to underscore, however, the unpredictability of finetuning large language\nmodels, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.","used in finetuning are 1024 and 256, respectively. We use packing (Raffel et al., 2020) to combine\nmultiple training examples into a single sequence, separating inputs from targets using a special EOS\ntoken. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations,\nwe report results on the final checkpoint trained for 30k steps.\n3\nRESULTS\nWe evaluate FLAN on natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, and struct-to-text. As described in \u00a72.2, we\nevaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for\nevaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses\na different checkpoint). For each dataset, we evaluate the mean of performance on all templates,\nwhich proxies the expected performance given a typical natural language instruction. As a dev set is","instruction tuning could result in positive task transfer among seen tasks, depending on the mixture\nof tasks (though we leave this for future work). In \u00a74.5, where we apply prompt tuning to the FLAN\ncheckpoint, we see promising results that indicate positive task transfer in a supervised setting.\nAre the ten unique templates per dataset or per task cluster?\nThe ten unique templates are for each dataset and not for a task cluster. This is because datasets in the\nsame task cluster often differed slightly (e.g., \u201cis this movie review positive\u201d vs \u201cis this yelp review\npositive\u201d).\nIn Figure 7A, why does the untuned LaMDA-PT model see worse performance with more\nparameters for reading comprehension and sentiment analysis?\nFor context, Figure 7A is a check of correctness for Figure 7B. Figure 7A confirms that scale improves\nperformance for tasks that were seen during instruction tuning, as expected. The untuned LaMDA-PT\nmodel performance in Figure 7A is shown just for completeness.\n30"]}
{"user_input":"What technique surpasses zero-shot large language models on diverse reasoning tasks without hand-crafted few-shot examples?","reference":"Chain of thought (CoT) prompting","response":"The \"Zero-shot-CoT\" technique surpasses zero-shot large language models on diverse reasoning tasks without hand-crafted few-shot examples. This technique involves adding the prompt \"Let's think step by step\" before each answer, which enables the model to perform complex multi-step reasoning. This approach has been shown to significantly improve accuracy on various benchmark reasoning tasks, including arithmetic, symbolic reasoning, and logical reasoning. \n","retrieved_contexts":["Large Language Models are Zero-Shot Reasoners\nTakeshi Kojima\nThe University of Tokyo\nt.kojima@weblab.t.u-tokyo.ac.jp\nShixiang Shane Gu\nGoogle Research, Brain Team\nMachel Reid\nGoogle Research\u2217\nYutaka Matsuo\nThe University of Tokyo\nYusuke Iwasawa\nThe University of Tokyo\nAbstract\nPretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT) prompting,\na recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics\nand symbolic reasoning, difficult system-2 tasks that do not follow the standard\nscaling laws for LLMs. While these successes are often attributed to LLMs\u2019\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\nby simply adding \u201cLet\u2019s think step by step\u201d before each answer. Experimental","of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and nonarithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\nconfirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.","ability for few-shot learning, we show that LLMs are decent zero-shot reasoners\nby simply adding \u201cLet\u2019s think step by step\u201d before each answer. Experimental\nresults demonstrate that our Zero-shot-CoT, using the same single prompt template,\nsignificantly outperforms zero-shot LLM performances on diverse benchmark\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP),\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\nUnderstanding, Tracking Shuffled Objects), without any hand-crafted few-shot\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci002), as well as similar magnitudes of improvements with another off-the-shelf\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\nvery diverse reasoning tasks hints at untapped and understudied fundamental","Language Models are Few-Shot Learners\nTom B. Brown\u2217\nBenjamin Mann\u2217\nNick Ryder\u2217\nMelanie Subbiah\u2217\nJared Kaplan\u2020\nPrafulla Dhariwal\nArvind Neelakantan\nPranav Shyam\nGirish Sastry\nAmanda Askell\nSandhini Agarwal\nAriel Herbert-Voss\nGretchen Krueger\nTom Henighan\nRewon Child\nAditya Ramesh\nDaniel M. Ziegler\nJeffrey Wu\nClemens Winter\nChristopher Hesse\nMark Chen\nEric Sigler\nMateusz Litwin\nScott Gray\nBenjamin Chess\nJack Clark\nChristopher Berner\nSam McCandlish\nAlec Radford\nIlya Sutskever\nDario Amodei\nOpenAI\nAbstract\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions \u2013 something which current NLP systems still largely"]}
{"user_input":"How does language model scale impact instruction tuning on zero-shot performance?","reference":"For larger language models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks. However, for smaller models (8B and smaller), instruction tuning actually hurts performance on held-out tasks. One potential explanation for this result is that for small-scale models, learning the tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse.","response":"The paper shows that instruction tuning helps large models generalize to new tasks, but for small models it actually hurts generalization. This is because all model capacity is used to learn the mixture of instruction tuning tasks. The paper also shows that the benefits of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, the paper evaluates the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters. Figure 7 shows these results. We see that the performance of instruction-tuned models increases with model size, but the gains diminish as the model size increases. This suggests that there is a point of diminishing returns for instruction tuning, and that the benefits of instruction tuning are most pronounced for smaller models.","retrieved_contexts":["Published as a conference paper at ICLR 2022\n4.2\nSCALING LAWS\n0.4B\n2B\n8B\n68B\n137B\nModel Size (# parameters)\n30\n40\n50\n60\n70\nInstruction tuning\nUntuned model\nAverage zero-shot accuracy \non 13 held-out tasks (%)\nPerformance on held-out tasks\nFigure 7:\nWhereas instruction tuning helps large\nmodels generalize to new tasks, for small models it\nactually hurts generalization to unseen tasks, potentially because all model capacity is used to learn the\nmixture of instruction tuning tasks.\nAs Brown et al. (2020) shows that zero and\nfew-shot capabilities of language models substantially improve for larger models, we next\nexplore how the benefits of instruction tuning\nare affected by model scale. Using the same\ncluster split as in the previous ablation study,\nwe evaluate the effect of instruction tuning\non models of size 422M, 2B, 8B, 68B, and\n137B parameters.\nFigure 7 shows these results. We see that","Published as a conference paper at ICLR 2022\nETHICAL CONSIDERATIONS\nThis work uses language models, for which the risks and potential harms are discussed in Bender &\nKoller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our\ncontribution in this paper is not a pretrained language model itself but rather an empirical study of\nhow instruction tuning affects the zero-shot performance of a language model on unseen tasks, we\nadditionally highlight two relevant ethical considerations. First, labeled datasets such as those we\nuse for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot\napplications of the model on downstream tasks. And second, instruction-tuned models can potentially\nrequire less data and expertise to use; such lower barriers to access could increase both the benefits\nand associated risks of such models.\nENVIRONMENTAL CONSIDERATIONS","Does model size matter for zero-shot reasoning?\nFigure 3 compares performance of various\nlanguage models on MultiArith \/ GSM8K. Without chain of thought reasoning, the performance\ndoes not increase or increases slowly as the model scale is increased, i.e., the curve is mostly flat. In\ncontrast, the performance drastically increases with chain of thought reasoning, as the model size\ngets bigger, for Original\/Instruct GPT-3 and PaLM. When the model size is smaller, chain of thought\nreasoning is not effective. This result aligns with the few-shot experiment results in Wei et al. [2022].\nAppendix E shows extensive experiment results using wider variety of language models, including\nGPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain\nof thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\nsampled outputs for each model).\nError Analysis","benchmarks (MultiArith and GSM8K) across Zero-shot-CoT and baselines. The large gap between\nstandard prompting (1st block) and chain of thought prompting (2nd block) suggests that these\ntasks are difficult without eliciting multi-step reasoning. Major improvements are confirmed on\nboth Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). While Zero-shot-CoT\nnaturally underperforms Few-shot-CoT, it substantially outperforms standard Few-shot prompting\nwith even 8 examples per task. For GSM8K, Zero-shot-CoT with Instruct GPT-3 (text-davinci-002)\nalso outperforms finetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B),\nreported in Wei et al. [2022] (3rd and 4th block). See App. D for more experiment results with PaLM.\nDoes model size matter for zero-shot reasoning?\nFigure 3 compares performance of various\nlanguage models on MultiArith \/ GSM8K. Without chain of thought reasoning, the performance"]}
{"user_input":"What's the advantage of using Zero-shot-CoT prompts over Few-shot-CoT prompts for multi-step reasoning in large language models?","reference":"Zero-shot-CoT prompts offer the advantage of not requiring step-by-step few-shot examples, making them more versatile and scalable for multi-step reasoning in large language models. They provide a stronger zero-shot baseline and encourage the discovery of broad cognitive abilities in LLMs.","response":"I'm sorry, but the provided context does not contain information about the advantages of using Zero-shot-CoT prompts over Few-shot-CoT prompts for multi-step reasoning in large language models. Therefore, I cannot answer your question. \n","retrieved_contexts":["confirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.\nLLMs have been trained on large corpora from various sources on the web (also see \u201cTraining Dataset\nDetails\u201d), and have shown to capture and amplify biases found in the training data. Prompting is a\nmethod that looks to take advantage of the patterns captured by language models conducive to various\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\n6\nConclusion\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach","Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\ntask, ours does not need any examples and just uses the same prompt \u201cLet\u2019s think step by step\u201d across\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\nand West, 2000] tasks with task-specific few-shot or zero-shot prompting [Liu et al., 2021b], even\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the","Chain of thought prompting\nMulti-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\nby modifying the answers in few-shot examples to step-by-step answers, and achieved significant\nboosts in performance across these difficult benchmarks, especially when combined with very large\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difficult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nas Few-shot-CoT in this work.\n3\nZero-shot Chain of Thought","of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and nonarithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\nconfirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models."]}
{"user_input":"What's the difference in unresolving rate between Retrieval-Q-CoT and Random-Q-CoT with extra demonstrations?","reference":"The unresolving rate of Retrieval-Q-CoT is 46.9% and the unresolving rate of Random-Q-CoT is 25.8%. Therefore, the difference in unresolving rate between Retrieval-Q-CoT and Random-Q-CoT with extra demonstrations is 21.1%.","response":"The unresolving rate of Retrieval-Q-CoT is 46.9%, while the unresolving rate of Random-Q-CoT is 25.8%. This means that Retrieval-Q-CoT is more likely to make mistakes like Zero-Shot-CoT, which is likely due to misleading by similarity.","retrieved_contexts":["whether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT.\nRetrieval-Q-CoT\nRandom-Q-CoT\n20\n30\n40\n50\nRate (%)\nFigure 2: Unresolving Rate.\nTo begin with, we invoke Zero-Shot-CoT on all the 600 questions\nfrom the MultiArith dataset. Among them, we collect those 128\nquestions (denoted as Q) where Zero-Shot-CoT generates wrong\nanswers (error rate: 21.3% = 128\/600). As we mentioned, with\nextra demonstrations, Retrieval-Q-CoT and Random-Q-CoT are\nexpected to perform more competitively than Zero-Shot-CoT. Among\nQ where Zero-Shot-CoT fails, we call those where Retrieval-Q-CoT\nor Random-Q-CoT still fail as their unresolved questions. We divide\nthe number of unresolved questions by 128 (number of questions in\nQ) to calculate the unresolving rate. A higher unresolving rate means\nthat a method more likely still makes mistakes like Zero-Shot-CoT.\nFigure 2 shows that the unresolving rate of Retrieval-Q-CoT (46.9%)","On a high level, both Retrieval-Q-CoT and Random-Q-CoT take the concatenation of qdemo\ni\n, cdemo\ni\npairs (i = 1, . . . , k)\nand qtest as input to predict the reasoning chain for qtest, which contains the answer in the end (like right of Figure 1).\nTable 1: Accuracy (%) of different sampling\nmethods. Symbol \u2020 indicates using training sets\nwith annotated reasoning chains.\nMethod\nMultiArith GSM8K AQuA\nZero-Shot-CoT\n78.7\n40.7\n33.5\nManual-CoT\n91.7\n46.9\n35.8\u2020\nRandom-Q-CoT\n86.2\n47.6\u2020\n36.2\u2020\nRetrieval-Q-CoT\n82.8\n48.0\u2020\n39.7\u2020\nTo our surprise, Retrieval-Q-CoT underperforms Random-Q-CoT\non the arithmetic dataset MultiArith [Roy and Roth, 2015] (Table\n1). Note that the retrieval methods were originally proposed in\ntasks with annotated labels [Rubin et al., 2022, Su et al., 2022],\nhowever, invoking Zero-Shot-CoT does not guarantee entirely correct\nreasoning chains. Thus, we hypothesize that the inferior performance","that a method more likely still makes mistakes like Zero-Shot-CoT.\nFigure 2 shows that the unresolving rate of Retrieval-Q-CoT (46.9%)\nis much higher than Random-Q-CoT (25.8%). It indicates that with similar questions being sampled for test questions,\nRetrieval-Q-CoT is negatively affected by misleading by similarity.\nTo show that unresolved questions of Retrieval-Q-CoT tend to be similar, we present a case study in Table 2. In the left\npart, the retrieved demonstration questions are similar to the test question and ask \u201chow long will it take him to cook the\nrest?\u201d The reasoning chains generated by Zero-Shot-CoT produce answers regarding \u201cthe total of\u201d instead of \u201cthe rest\u201d.\nFollowing the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of \u201cthe rest\u201d. In contrast,\nRandom-Q-CoT correctly understands \u201cthe rest\u201d better without making similar mistakes in the demonstrations, thanks\nto relatively more diverse (random) demonstrations.\n3.2","addressed. To design more effective Auto-CoT, we need to understand its challenge better.\n3.1\nRetrieval-Q-CoT Fails due to Misleading by Similarity\nSince Retrieval-Q-CoT uses a few prompting demonstrations like in Manual-CoT, Retrieval-Q-CoT is expected to\nperform competitively as well. However, reasoning chains (both rationales and answers) in Retrieval-Q-CoT are\ngenerated by Zero-Shot-CoT: they may have mistakes that lead to wrong answers. Let us simply call demonstrations\nwith wrong answers as wrong demonstrations. Intuitively, after similar questions to a test question are retrieved, wrong\ndemonstrations caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g.,\nreplicating mistakes) for the test question. We refer to this phenomenon as misleading by similarity. We will investigate\nwhether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT.\nRetrieval-Q-CoT\nRandom-Q-CoT\n20\n30\n40\n50\nRate (%)"]}
{"user_input":"What are the pros and cons of prompting methods for large language models in terms of bias and unbiased study?","reference":"Prompting methods for large language models have both pros and cons in terms of bias and unbiased study. On one hand, these methods take advantage of the patterns captured by language models, which can amplify biases found in the training data. This means that the prompts may inherit and perpetuate biases present in the training data. On the other hand, prompting methods provide a more direct way to probe complex reasoning inside pre-trained language models, removing the confounding factor of in-context learning in prior few-shot approaches. This can lead to a more unbiased study of biases in language models. Overall, while prompting methods have the potential for bias, they also offer opportunities for unbiased exploration of language models.","response":"I'm sorry, I can't answer that question. The provided context does not contain information about the pros and cons of prompting methods for large language models in terms of bias and unbiased study.","retrieved_contexts":["of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and nonarithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\nconfirm that it is not specially engineered for multi-step reasoning.\nLimitation and Social Impact\nOur work is based on prompting methods for large language models.","Question\nQuestion\nQuestion\nFigure 10:\nExamples of semantic understanding and one-step missing errors that were fixed by\nscaling PaLM from 62B to 540B.\nA.2\nWhat is the role of prompt engineering?\nOne of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage\nof work showing that prompts affect language models in unexpected ways (Min et al., 2022). The\ngeneral way that we created chain of thought annotations was by taking eight exemplars from the\ntraining set and decomposing the reasoning process into multiple steps leading to the final answer.\nExamples of chain of thought annotations are provided in Figure 3, with full prompts given in\nAppendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed\nrobustness experiments with respect to various factors.\n\u2022 Different annotators. We first analyze robustness to three different annotators (Section 3.4 and","6.2.4\nFuture Bias and Fairness Challenges\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\nand to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an\narea of continuous research for us and are excited to discuss different methodological approaches with the community.\nWe view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but\nwe recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18].\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this\nis also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions specific to large","Standard prompting\n21.5 \u00b10.6\n59.5 \u00b13.0\n80.8 \u00b11.8\n5.8 \u00b10.6\n49.0 \u00b12.1\nChain of thought prompting\n26.8 \u00b12.1\n85.8 \u00b11.8\n91.7 \u00b11.4\n77.5 \u00b13.8\n99.6 \u00b10.3\nAblations\n\u00b7 variable compute only\n21.3 \u00b10.7\n61.6 \u00b12.2\n74.2 \u00b12.3\n7.2 \u00b11.6\n50.7 \u00b10.7\n\u00b7 reasoning after answer\n20.9 \u00b11.0\n63.0 \u00b12.0\n83.3 \u00b10.6\n0.0 \u00b10.0\n50.2 \u00b10.5\nRobustness\n\u00b7 different annotator (B)\n27.4 \u00b11.7\n75.4 \u00b12.7\n88.3 \u00b11.4\n76.0 \u00b11.9\n77.5 \u00b17.9\n\u00b7 different annotator (C)\n25.5 \u00b12.5\n81.1 \u00b13.6\n85.0 \u00b11.8\n68.1 \u00b12.2\n71.4 \u00b111.1\n23"]}
{"user_input":"What are the stages and processes in the Auto-CoT method for constructing demonstrations?","reference":"The Auto-CoT method for constructing demonstrations consists of two main stages: question clustering and demonstration sampling. In the question clustering stage, questions are partitioned into clusters using the k-means clustering algorithm. In the demonstration sampling stage, reasoning chains are generated for sampled questions and demonstrations are constructed. The overall procedure is illustrated in Figure 4.","response":"I'm sorry, but the provided context does not contain information about the stages and processes in the Auto-CoT method for constructing demonstrations. Therefore, I cannot answer your question.","retrieved_contexts":["A\nExtended analysis for the challenge of Auto-CoT\nA.1\nImpact of demonstration elements.\nA demonstration is a triple composed by <question, rationale, answer> as shown in Figure 1. We shuffle either of\nthe demonstration components to see how the performance changes. According to the results in Table 5, shuffling\nquestions has the least performance reduction (91.7% \u219273.8%). A possible reason for the decent performance is that\nthe model may capture the rationale-answer mapping patterns. The pattern is expected to reflect how the intermediate\nreasoning steps can lead to the final answer. This finding shows that it is possible to leverage the generated rationales\nby Zero-Shot-CoT because those rationales are often logically correct even though they may lead to wrong answers\n[Kojima et al., 2022].\nTable 5: Impact of different components in the demonstrations.\nMethod\nAccuracy\nManual-CoT\n91.7\nShuffle Questions\n73.8\nShuffle Rationales\n43.8\nShuffle Answers\n17.0","Experimental results on ten public benchmark reasoning datasets showed that with GPT-3, Auto-CoT consistently\nmatches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations.\n9","the effect of misleading by similarity (Section 3.1). On the other hand, if we took each demonstration as a kind of skill,\ndiverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists\na small portion (e.g., 1\/8) of mistakes in the demonstrations, the performance will not be negatively affected (to be\nshown in Figure 6).\nNevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such\nas from questions in the frequent-error cluster. As we will show later, some of these wrong demonstrations may be\neliminated with heuristics. For example, wrong demonstrations often come with long questions and long rationales.\nUsing simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps\nmitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix C.2).\n4\nAuto-CoT: Automatic Chain-of-Thought Prompting","same demonstrations for multiple datasets (e.g., 5\/6 of the\narithmetic datasets). In contrast, Auto-CoT is more flexible and task-adaptive: every single dataset gets its own\ndemonstrations that are automatically constructed.\n5.3\nVisualization of Question Clustering\nFigure 5 visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist\ngeneric patterns, where different patterns may be characterized by questions from different clusters. We present the\nconstructed demonstrations of Auto-CoT in Appendix D.\n#5\nAddSub\nSingleEq\nCoin Flip \nGSM8K\nLast Letter Concatenation\nMultiArith\nStrategyQA\nAQUA\nCSQA\nSVAMP\nFigure 5: Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.\n8"]}
{"user_input":"How are genes passed from one generation to the next?","reference":"Genes are passed from parent to offspring.","response":"Genes are passed from one generation to the next through a process called reproduction. During reproduction, two parents contribute their genetic material to create a new individual. The genetic material from each parent is combined in the offspring, resulting in a unique combination of genes. This process ensures that the offspring inherits a mixture of traits from both parents.","retrieved_contexts":["Input: {question}\nKnowledge:\nTable 9: Prompt for knowledge generation on CSQA2. Demonstration examples are selected from the CSQA2\ntraining set; we use the annotated Google featured snippet as the knowledge.\nTask\nPrompt\nQASC\nGenerate some knowledge about the input. Examples:\nInput: What type of water formation is formed by clouds?\nKnowledge: Clouds are made of water vapor.\nInput: What can prevent food spoilage?\nKnowledge: Dehydrating food is used for preserving food.\nInput: The process by which genes are passed is\nKnowledge: Genes are passed from parent to offspring.\nInput: The stomach does what in the body?\nKnowledge: The stomach is part of the digestive system.\nInput: What can cause rocks to break down?\nKnowledge: Mechanical weathering is when rocks are broken down by mechanical means.\nInput: {question}\nKnowledge:\nTable 10: Prompt for knowledge generation on QASC. Demonstration examples are selected from the QASC\ntraining set; we use one of the gold separate facts as the knowledge.","Task\nNumerSense\nQASC\nPrompt\nGenerate some numerical facts about objects. Examples:\nGenerate some knowledge about the input. Examples:\nInput: penguins have <mask> wings.\nInput: What type of water formation is formed by clouds?\nKnowledge: Birds have two wings. Penguin is a kind of bird.\nKnowledge: Clouds are made of water vapor.\n...\n...\nInput: a typical human being has <mask> limbs.\nInput: The process by which genes are passed is\nKnowledge: Human has two arms and two legs.\nKnowledge: Genes are passed from parent to offspring.\nInput: {question}\nInput: {question}\nKnowledge:\nKnowledge:\nTable 2: Prompts for knowledge generation for two of our tasks, NumerSense and QASC. The prompt consists of\nan instruction, five demonstrations of question-knowledge pairs, and a new question placeholder. For full prompts\non all the tasks we evaluate on, see Appendix A.2.\nchallenges posed by the task (e.g. numerical commonsense, scientific commonsense). We pair each","Task\nPrompt\nCSQA\nGenerate some knowledge about the concepts in the input. Examples:\nInput: Google Maps and other highway and street GPS services have replaced what?\nKnowledge: Electronic maps are the modern version of paper atlas.\nInput: The fox walked from the city into the forest, what was it looking for?\nKnowledge: Natural habitats are usually away from cities.\nInput: You can share files with someone if you have a connection to a what?\nKnowledge: Files can be shared over the Internet.\nInput: Too many people want exotic snakes. The demand is driving what to carry them?\nKnowledge: Some people raise snakes as pets.\nInput: The body guard was good at his duties, he made the person who hired him what?\nKnowledge: The job of body guards is to ensure the safety and security of the employer.\nInput: {question}\nKnowledge:","Table 25: Few-shot exemplars for full chain of thought prompt for StrategyQA.\nPROMPT FOR STRATEGYQA\nQ: Do hamsters provide food for any animals?\nA: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So\nthe answer is yes.\nQ: Could Brooke Shields succeed at University of Pennsylvania?\nA: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\nUniversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the\nanswer is yes.\nQ: Yes or no: Hydrogen\u2019s atomic number squared exceeds number of Spice Girls?\nA: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen\u2019s atomic\nnumber squared is less than 5. So the answer is no.\nQ: Yes or no: Is it common to see frost during some college commencements?\nA: College commencement ceremonies can happen in December, May, and June. December is in the winter, so"]}
